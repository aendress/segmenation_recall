---
title: "Recall for segmentation"
author: "Ansgar Endress"
output:
  pdf_document:
    toc: true
    number_sections: true
    keep_tex: true
    fig_caption: true
  html_document:
    theme: spacelab      
    number_sections: yes
    df_print: paged
    toc: yes
    toc_float: yes
    fig_caption: true
  html_notebook:
    theme: spacelab      
    number_sections: yes
    toc: yes
    toc_float: yes
    fig_caption: true
---

# House keeping

## Set up knitr
```{r setup, echo = FALSE, include=FALSE}
rm (list=ls())

options (digits = 3)
knitr::opts_chunk$set(
    # Run the chunk
    eval = TRUE,
    # Don't include source code
    echo = FALSE, 
    # Print warnings to console rather than the output file
    warning = FALSE,  
    # Stop on errors
    error = FALSE,
    # Print message to console rather than the output file
    message = FALSE,
    # Include chunk output into output
    include = TRUE,
    # Don't reformat R code
    tidy = FALSE,
    # Center images
    fig.align = 'center',
    # Default image width
    out.width = '80%')

# other knits options are here:
# https://yihui.name/knitr/options/
```

## Set parameters 
```{r set-parameters, echo = FALSE, include=FALSE}
# Parameters used below

PRINT.INDIVIDUAL.PDFS <- TRUE

# Remove items that contain unattested syllables
# Set to FALSE as this would lead to problems with participants having no vocalization for one of the conditions.
FILTER.UNATTESTED.ITEMS <- FALSE

#ALLOW.CONCATENTATIONS.FOR.W.VS.PW.ANALYSIS <- TRUE

REMOVEBADSUBJ <- TRUE

```

In the analyses below, we use the following parameters: 
```{r list-parameters}
knitr::kable(
    do.call (rbind, 
             lapply (ls(),
                     function (X) 
                         data.frame(Name = X, Value = as.character(get (X)))
             )
    )
)
```

## Load various libraries
```{r load-libraries, include = FALSE, message = TRUE, warning = TRUE}

# check this
#http://www.ats.ucla.edu/stat/r/dae/melogit.htm
#http://www.sagepub.com/upm-data/38503_Chapter6.pdf
# Read in a random collection of custom functions
# Read in a random collection of custom functions
if (Sys.info()[["user"]] %in% c("ansgar", "endress")){
    source ("/Users/endress/R.ansgar/ansgarlib/R/tt.R")
    source ("/Users/endress/R.ansgar/ansgarlib/R/null.R")
    #source ("helper_functions.R")
} else {
    # Note that these will probably not be the latest versions
    source("http://endress.org/progs/tt.R")
    source("http://endress.org/progs/null.R")
}

library ("knitr")
```

## Define some helper functions
```{r helper-functions, include = FALSE}

count.sylls <- function (items){
    
    sapply (items,
            function (X) nchar (X) /2 )
    
}

get.unique.str.length <- function (items){
    
    item.len <- unique (stringr::str_length(items))
    
    if (length (item.len) > 1)
        stop ("Inconsistent item length")
    
    return (item.len)
}


get.substrings.of.length <- function (items, substr.len = 2, allow.overlap = FALSE, overlap.offset = NULL, simplify = TRUE){
    
    if (allow.overlap) {
        if (is.null (overlap.offset)){
            substr.offset <- substr.len   
        } else {
            substr.offset <- overlap.offset
        }
    } else {
        substr.offset <- substr.len
    }
    
    sapply (items,
            function (X) {
                if (simplify){
                    substring (X,
                               seq(1, nchar(X)-substr.len + 1, substr.offset),
                               seq(substr.len, nchar(X), substr.offset))
                } else{
                    ifelse (nchar (X) < substr.len,
                            list (NULL),
                            list (substring (X,
                                             seq(1, nchar(X)-substr.len + 1, substr.offset),
                                             seq(substr.len, nchar(X), substr.offset)))) %>%
                        unlist
                }
            },
            simplify = simplify)
}

is.item.type <- function (items, 
                          all.items.for.type){
    
    sapply (items, 
            function (X) X %in% all.items.for.type)
}

is.concatenation.of.item.type <- function (items,
                                           all.items.for.type){
    
    item.length <- sapply (all.items.for.type, nchar) %>% 
        unique
    
    if (length (item.length) > 1)
        stop ("Items do not have a consistent length")
    
    # Pad items to minimum length where required
    items <- ifelse (is.na (items) | 
                         (nchar (items) < item.length),
                     paste (rep ("x", item.length), collapse=""),
                     items)
    
    is.concatenation <- lapply (items,
                                get.substrings.of.length, item.length) %>%
        lapply (is.item.type,
                all.items.for.type) %>%
        lapply (all) %>% 
        unlist
    
    # Exclude single items
    is.concatenation[nchar (items) <= item.length] <- FALSE
    
    return (is.concatenation)
}

is.chunk.from.item.type <- function (items, 
                                     all.items.for.type,
                                     min.length = 4){
    
    is.chunk <- sapply (items, 
                        function (X) {
                            ifelse (nchar(X) < min.length,
                                    FALSE,
                                    grepl (X, all.items.for.type) %>% 
                                        any())
                        })
    
    is.chunk[is.na(items)] <- FALSE
    
    return (is.chunk)
}

has.correct.initial.syll<- function (items, 
                                     all.items.for.type){
    
    sapply (items, 
            function (X) {
                grepl (paste ("^",
                              substr(X, 1, 2),
                              sep =""), 
                       all.items.for.type) %>%
                    any()
                
            })
}

has.correct.final.syll <- function (items, 
                                    all.items.for.type){
    
    sapply (items, 
            function (X) {
                grepl (paste (substr(X, nchar(X)-1, nchar(X)),
                              "$",
                              sep =""), 
                       all.items.for.type) %>%
                    any()
                
            })
}

reverse.items <- function (items, syll.len = 2){
    
    items.as.vectors <- get.substrings.of.length(items,
                                                 substr.len = syll.len,
                                                 simplify = FALSE) 
    
    items.rev <- lapply (items.as.vectors,
                         function (X) paste (rev (X), collapse = "")) %>%
        unlist
    
    return (items.rev)
}

get.part.words <- function (words, parts.word1 = c(3), parts.word2 = c(1, 2), allow.repeats = FALSE){
    
    words.as.sylls <- get.substrings.of.length (words) %>%
        as.data.frame(stringsAsFactors = FALSE) %>%
        as.list
    
    part.words <- c()
    for (first.word.ind in 1:length(words)){
        
        second.word.inds <- 1:length(words)
        if (!allow.repeats)
            second.word.inds <- second.word.inds[-first.word.ind]    
        
        part.words.current <- lapply (second.word.inds,
                                      function (X) paste (
                                          paste (words.as.sylls[[first.word.ind]][parts.word1], 
                                                 collapse=""),
                                          paste (words.as.sylls[[X]][parts.word2],
                                                 collapse=""),
                                          sep = "")) %>%
            unlist 
        
        part.words <- c(part.words,
                        part.words.current)
    }
    return (part.words)
}

calculate.average.tps.from.chunks <- function (items,
                                               item.type.list,
                                               chunk.length = 4){
    mean.tps.in.items <- c()
    for (current.item in items){
        
        if (is.na (current.item) |
            is.null (current.item) |
            (nchar (current.item) < chunk.length)) {
            mean.tps.in.items <- c(mean.tps.in.items,
                                   NA)
            next
        }
        
        current.chunk.list <- get.substrings.of.length(current.item, 
                                                       chunk.length, 
                                                       allow.overlap = TRUE, 
                                                       overlap.offset = chunk.length/2,
                                                       simplify = FALSE)
        
        # Loop through the chunks for the current item
        tps.in.current.chunks <- c()
        for (current.chunk in unlist (current.chunk.list)){
            
            current.tp <- 0
            for (current.item.type in item.type.list){
                
                if (any (grepl (current.chunk, current.item.type$chunks))){
                    
                    current.tp <- current.tp + current.item.type$tp    
                    
                }
            }
            
            tps.in.current.chunks <- c(tps.in.current.chunks, 
                                       current.tp)
        }
        mean.tps.in.items <- c(mean.tps.in.items,
                               mean (tps.in.current.chunks))
    }
    
    return (mean.tps.in.items)
}


calculate.expected.tps.for.chunks <- function (items, words, high.tp = 1, low.tp = 1/3, syll.len = 2) {
    
    # use words to detect whether we start with an A, B or C syllable
    word.len <- get.unique.str.length(words)
    
    potential.starting.sylls <- lapply (    
        seq (1, word.len, syll.len),
        function (X) substr (words, X, X + syll.len-1))
    
    # Calculate expected TPs
    max.item.length <- max (nchar (items)) / syll.len
    expected.tps <- list (
        # Starting with an A syllable
        rep (c(high.tp, high.tp, low.tp), ceil (max.item.length / 3)),
        # Starting with a B syllable
        rep (c(high.tp, low.tp, high.tp), ceil (max.item.length / 3)),
        # Starting with a C syllable
        rep (c(low.tp, high.tp, high.tp), ceil (max.item.length / 3)))
    
    lapply (items,
            function (X) {
                
                if ((nchar(X) / syll.len) < 2)
                    return (NA)
                
                current.starting.pos <- which (
                    lapply (potential.starting.sylls,
                            function (PSS) any (gdata::startsWith (X, PSS))) %>%
                        unlist)
        
                if (length (current.starting.pos) == 0){
                    warning (paste("No starting position could be determined for item ", 
                                   X, 
                                   ". Picking random position instead.", sep =""))
                    current.starting.pos <- sample (length (expected.tps), 1)
                }
                
                                
                current.expected.tps <- expected.tps[[current.starting.pos]]

                current.expected.tps <- current.expected.tps[1:((nchar(X)/syll.len)-1)]
                
                return (mean (current.expected.tps))
            }) %>%
        unlist
    
}

wilcox.p <- function (x, mu = 0)
{
    x <- as.numeric (x)
    if ((all %.% is.na) (x)) {
        return (NA)
    } else {
        return (signif (
            wilcox.test (x, mu = mu)$p.value,
            getOption('digits')))
            
    }
}

t.test.p <- function (x, mu = 0)
{
    x <- as.numeric (x)
    if ((all %.% is.na) (x)) {
        return (NA)
    } else {
        return (signif (
            t.test (x, mu = mu)$p.value,
            getOption('digits')))
    }
}

replace_column_labels <- function (X)
{    
    # Uses pryr
    # Evalution from right to left
    compose (
        function (X) {gsub ("flanker.rt.d.median.split", 
                            "Flanker Group", X)},
        function (X) {gsub ("scs.median.split", 
                            "Self Control Group", X)},
        function (X) {gsub ("countCond", "*Secondary Task*", X)},    
        function (X) {gsub ("experimentID", "Experiment", X)},
        function (X) {gsub ("poolSize", "*Pool Size*", X)},
        function (X) {gsub ("nItems", "*Set Size*", X)},
        function (X) {gsub ("yPosCond", "*Sequential Position*", X)},
        function (X) {gsub ("locCond", "*Location Condition*", X)},
        
        function (X) {gsub ("countCond", "*Secondary Task*", X)},
        function (X) {gsub ("piCond", "*PI Condition*", X)},
        function (X) {gsub ("partial.eta.squared", "$\\\\eta_p^2$", X)},

        function (X) {gsub ("p<=.05", "$p \\\\leq .05$", X)},
        function (X) {gsub ("p.value", "$p$", X)},
        function (X) {gsub ("F.value", "*F*", X)},
        function (X) {gsub ("Cohen.d", "Cohen's *d*", X)},
        function (X) {gsub ("^CI$", "*CI*", X)},
        function (X) {gsub ("^P$", "$p$", X)},
        function (X) {gsub ("^p$", "$p$", X)},
        function (X) {gsub ("^t$", "$t$", X)},
        function (X) {gsub ("p.t.test", "$p_{t\\\\ test}$", X)},
        function (X) {gsub ("p.wilcox.test", "$p_{Wilcoxon}$", X)},
        function (X) {gsub ("^SE.log$", "*SE* (log)", X)},
        function (X) {gsub ("^SE$", "*SE*", X)},
        function (X) {gsub ("^SD.log$", "*SD* (log)", X)},
        function (X) {gsub ("^SD$", "*SD*", X)},
        function (X) {gsub ("^M.log$", "*M* (log)", X)},
        function (X) {gsub ("^M$", "*M*", X)},
        function (X) {gsub ("^effect$", "Effect", X)},
        function (X) {gsub ("^model.name$", "Experiment", X)},
        function (X) {gsub ("^Chisq$", "$\\\\chi^2$", X)},
        function (X) {gsub ("Chi Df", "Df", X)},
        function (X) {gsub ("Pr\\(>Chisq\\)", "$p$", X)},
        function (X) {gsub ("IV.removed", "Removed IV", X)}
    ) (X)
}


```

## Specifify languages
```{r specificy-languages, include = FALSE}
words.fw <- list (L1 = c("pAbiku", "tibudO", "dArOpi", "gOLAtu"),
                  L2 = c("bikuti", "pigOLA", "tudArO", "budOpA"))
words.fw <- lapply (words.fw,
                    tolower)

words.bw <- lapply (words.fw,
                    reverse.items)

part.words.fw <- rbind.data.frame(
    # BCA
    lapply (words.fw,
            get.part.words,
            parts.word1 = c(2:3), 
            parts.word2 = c(1)),
    # CAB
    lapply (words.fw,
            get.part.words,
            parts.word1 = c(3), 
            parts.word2 = c(1:2)),
    stringsAsFactors = FALSE) %>%
    as.list 

part.words.bw <- rbind.data.frame(
    # BCA
    lapply (words.bw,
            get.part.words,
            parts.word1 = c(2:3), 
            parts.word2 = c(1)),
    # CAB
    lapply (words.bw,
            get.part.words,
            parts.word1 = c(3), 
            parts.word2 = c(1:2)),
    stringsAsFactors = FALSE) %>%
    as.list 

class.words.fw <- rbind.data.frame(
    # AiBiCj
    lapply (words.fw,
            get.part.words,
            parts.word1 = c(1:2), 
            parts.word2 = c(3)),
    # AiBjCj
    lapply (words.fw,
            get.part.words,
            parts.word1 = c(1), 
            parts.word2 = c(2:3)),
    stringsAsFactors = FALSE) %>%
    as.list 

low.tp.chunk.fw <- lapply (words.fw,
                           get.part.words,
                           parts.word1 = c(3), 
                           parts.word2 = c(1))

low.tp.chunk.bw <- lapply (words.bw,
                           get.part.words,
                           parts.word1 = c(3), 
                           parts.word2 = c(1))
```

The two languages comprise the following words: 

```{r print-languages}
words.fw %>%
    data.frame %>%
    #knitr::kable("latex", booktabs = T, caption = '\\label{tab:languages}Words used in the recall experiment.') %>%
    knitr::kable (caption = "\\label{tab:languages}Words used in the recall experiment.") %>%
    kableExtra::kable_styling(bootstrap_options = "striped")

```

We also define part-words as well the corresponding backward items.

## Load data
```{r load-data, include = FALSE}

dat.recognition <- rbind(read.table ("data/recall.i.e3.cont.tab", 
                               header=T, sep="\t", comment.char = "%"),
                     read.table ("data/recall.i.e4.segm.tab",
                                 header=T, sep="\t", comment.char = "%")) %>% 
    mutate (subj = factor (tolower(as.character(subj))))



dat.recall <- gdata::read.xls("data/segmentation_recall_transcriptions.xlsx", 
                       sheet="Sheet2-ade",
                       stringsAsFactors = FALSE,
                       header=TRUE) %>%
    mutate (subj = paste (subjNum, subjInitials, sep = ".")) %>%
    mutate (closest_match = tolower(closest_match)) %>%
    # dplyr::distinct (.keep_all = TRUE)
    dplyr::distinct (subjNum, subjInitials, streamType, lang, closest_match, .keep_all = TRUE) %>%
    filter (!is.na (subjNum))

```

```{r display-averages-recognition}
dat.recognition.m <- 
    aggregate(correct~condDir+language+subj, dat.recognition, mean)

dat.recognition.m %>% 
    #filter (condDir == "recall.i.e3.cont") %>%
    group_by(condDir, language) %>%
    summarize (M = mean (correct),
               SD = sd (correct)) %>% 
    kable (caption="Recognition test performance")

```

```{r find-and-remove-bad-subjects}

bad.subj <- dat.recall %>% 
    filter (streamType == "continuous") %>% 
    distinct(subj, subjNum, subjInitials, correct_segm) %>%
    filter (correct_segm < .5) %>%
    pull ("subj")

if (REMOVEBADSUBJ){
    
    dat.recall <- dat.recall %>%
        remove.bad.subj(bad.subj,
                        subj.var = "subj")
        
}

```

## Change columns to categorize transcriptions
```{r categorize-transcriptions, include = FALSE}
dat.recall <- lapply (c(L1 = "L1", L2 = "L2"),
               function (current.lang) {
                   dat.recall %>% 
                       filter (lang == current.lang) %>%
                       mutate (n_syllables = count.sylls (closest_match)) %>%
                       mutate (is_word = is.item.type (closest_match, 
                                                       words.fw[[current.lang]])) %>%
                       mutate (is_multiple_words = is.concatenation.of.item.type (closest_match,
                                                                                  words.fw[[current.lang]])) %>%
                       mutate (is_single_or_multiple_words = is_word | is_multiple_words) %>% 
                       mutate (is_part_word = is.item.type (closest_match, 
                                                            part.words.fw[[current.lang]])) %>%
                       mutate (is_multiple_part_words = is.concatenation.of.item.type (closest_match,
                                                                                       part.words.fw[[current.lang]])) %>%
                       mutate (is_single_or_multiple_part_words = is_part_word | is_multiple_part_words) %>%
                       mutate (is_class_word = is.item.type(closest_match,
                                                            class.words.fw[[current.lang]])) %>%
                       mutate (is_high_tp_chunk = is.chunk.from.item.type (closest_match,
                                                                           words.fw[[current.lang]])) %>%
                       mutate (is_low_tp_chunk = is.chunk.from.item.type (closest_match,
                                                                          low.tp.chunk.fw[[current.lang]])) %>%
                       mutate (has_correct_initial_syllable = has.correct.initial.syll (closest_match,
                                                                                        words.fw[[current.lang]])) %>%
                       mutate (has_correct_final_syllable = has.correct.final.syll (closest_match,
                                                                                    words.fw[[current.lang]])) %>%
                       mutate (is_part_of_stream = ifelse (n_syllables == 1,
                                                           closest_match %in% 
                                                               get.substrings.of.length(words.fw[[current.lang]], 2) %>% 
                                                               paste,
                                                           is_word |
                                                               is_multiple_words |
                                                               is_part_word |
                                                               is_multiple_part_words |
                                                               is_high_tp_chunk | 
                                                               is_low_tp_chunk)) %>%
                       mutate (is_part_of_stream = ifelse (is.na (is_part_of_stream),
                                                           FALSE,
                                                           is_part_of_stream)) %>% 
                       mutate (is_part_of_stream = as.logical(is_part_of_stream)) %>%
                       mutate (is_bw_word = is.item.type (closest_match, 
                                                          words.bw[[current.lang]])) %>%
                       mutate (is_multiple_bw_words = is.concatenation.of.item.type (closest_match,
                                                                                     words.bw[[current.lang]])) %>%
                       mutate (is_single_or_multiple_bw_words = is_bw_word | is_multiple_bw_words) %>%
                       mutate (is_bw_part_word = is.item.type (closest_match, 
                                                               part.words.bw[[current.lang]])) %>%
                       mutate (is_multiple_bw_part_words = is.concatenation.of.item.type (closest_match,
                                                                                          part.words.bw[[current.lang]])) %>%
                       mutate (is_single_or_multiple_bw_part_words = is_bw_part_word | is_multiple_bw_part_words) %>%
                       mutate (is_high_tp_bw_chunk = is.chunk.from.item.type (closest_match,
                                                                              words.bw[[current.lang]])) %>%
                       mutate (is_low_tp_bw_chunk = is.chunk.from.item.type (closest_match,
                                                                             low.tp.chunk.bw[[current.lang]])) %>%
                       mutate (average_fw_tp = calculate.average.tps.from.chunks (closest_match,
                                                                                  list (list (chunks = words.fw[[current.lang]],
                                                                                              tp = 1),
                                                                                        list (chunks = low.tp.chunk.fw[[current.lang]],
                                                                                              tp = 1/3)),
                                                                                  chunk.length = 4)) %>% 
                       mutate (expected_fw_tp = calculate.expected.tps.for.chunks (closest_match, words.fw[[current.lang]])) %>%
                       mutate (average_bw_tp = calculate.average.tps.from.chunks (reverse.items (closest_match),
                                                                                  list (list (chunks = words.bw[[current.lang]],
                                                                                              tp = 1),
                                                                                        list (chunks = low.tp.chunk.bw[[current.lang]],
                                                                                              tp = 1/3)),
                                                                                  chunk.length = 4)) 
               }) %>% 
    do.call (rbind, .) %>% 
    arrange(subjNum, subjInitials, streamType)
```

```{r filter-unattested-items, include = FALSE}
if (FILTER.UNATTESTED.ITEMS){
    dat.recall <- dat.recall %>%
        filter (is_part_of_stream)
}
```

```{r check-that-all-participants-have-both-stream-types}

 subj.with.one.streamType <- dat.recall %>%
     distinct(subj, streamType) %>%
#    xtabs(formula = ~ subj + streamType) %>%
        xtabs(formula = ~ subj ) %>%
    as.data.frame() %>% 
    filter (Freq != 2)

    if (nrow(subj.with.one.streamType))
        stop ("Some participants have productions in only one stream type, exiting.")
```

## Demographics and missing subjects
```{r final-demographics-calculate}

recall.demographics <- dat.recall %>% 
    filter (streamType == "continuous") %>%
     distinct (subj, Gender, Age, lang) %>%
    mutate (Gender = tolower (Gender)) %>%
    mutate (Gender = ifelse (startsWith(Gender, "f"), "female", "male")) %>%
    group_by(lang) %>%
    summarize (N = n(), 
               females = sum (gdata::startsWith(Gender, "f", ignore.case = TRUE)),
               males = sum (gdata::startsWith(Gender, "m", ignore.case = TRUE)),
               age.m = round (mean (Age), 1),
               age.range = paste (range(Age), collapse = "-"))
 
```

The final demographic information is given in Table \ref{tab:recall_demographics}. 

```{r final-demographics-print}

recall.demographics %>%
     setNames (replace_column_labels(names(.))) %>%
     knitr::kable(caption = '\\label{tab:recall_demographics}Demographics of the final sample')

```



## Save categorized data 
```{r save-data, include = FALSE, eval = FALSE}
# save.data.frame(dat, row.names = FALSE)
xlsx::write.xlsx (dat.recall,
                  file="recall.populated.xlsx",
                  row.names=FALSE,
                  sheetName="Sheet1",
                  append=FALSE)
```

# Analyze data

In the analyses below, we do not removed all items that contained syllables not attested in the speech stream as this would lead to an unbalanced data set.

Analyses to be performed

* Number of items produced 
* Average length of items produced
* Number and proportion of
    a. Words and Part-Words
    b. a and multiple vs. multiple part-words
    c. c and high TP vs low TP chunks
* Check if there are interesting differences in backward items
* Check for close matches in terms of positions
    a. Items with correct initial chunks but a final syllable from another word
    b. Items with correct initial chunks but a final syllable that is not a final syllable in another word
    c. Items with correct final chunks but an inital syllable from another word
    d. Items with correct final chunks but an initial syllable that is not an intial syllable in another word
* Check for close matches in terms of TPs
    a. Compute average TPs by length of items. 
        - The expected TPs for items of at least 2 syllables starting on an initial syllable are c(1, 1/3, 1, 1, 1/3, 1, 1, 1/3, ...)
        - The expected TPs for a random concatenation are the TPs in a random bigram. For an A or a B syllable, the random TP is 1 $\times$ 1 / 12, as there is only 1 (out of 12) non-zero TP continuations. For a C syllable, the random TP is 3 $\times$ 1/3 / 12, as there are 3 possible concatenations. On average, the random TP is thus $(1/12 + 1/12 + 1/12)/ 3 = 1/12 \approx .083$. Compare this TP to the average TPs in chunks.
    b. Check if all sub-chunks of an item are attested
* Test whether the number of recalled words is significantly different from zero.

## Make overall averages

```{r averages-across-items-calculate, include = FALSE}

dat.recall.m <- dat.recall %>% 
    group_by(subj, subjNum, subjInitials, Age, Gender, streamType, correct_segm) %>%
    summarize_at (vars(n_syllables:average_bw_tp), mean, na.rm = TRUE)

dat.recall.s <- dat.recall %>% 
    group_by(subj, subjNum, subjInitials, Age, Gender, streamType, correct_segm) %>%
    summarize_at (vars(n_syllables:average_bw_tp), sum, na.rm = TRUE)
    #summarize_at (vars(n_syllables:average_bw_tp), funs(n(), sum))

# These are counts that are significantly above zero
dat.recall.s.selected.vars.by.wilcox.df <- 
    dat.recall.s %>% 
    group_by(streamType) %>%
    summarize_at (vars(starts_with("is_"),starts_with("has_")), function (X) wilcox.test (X, alternative = "greater")$p.value) %>% 
    remove_rownames %>% 
    column_to_rownames("streamType") %>% 
    t %>% 
as.data.frame (row.names = row.names(.)) %>% 
    rownames_to_column("var") %>% 
     mutate (use = (continuous <= .05) | (segmented <= .05)) 

dat.recall.s.selected.vars.by.wilcox <- dat.recall.s.selected.vars.by.wilcox.df %>% 
    filter (use) %>% 
    pull ("var")

```



```{r wilcox-across-stream-types-calculate}
dat.recall.m.wilcox.by.streamType <-
    dat.recall.m %>%
    ungroup () %>%
    mutate (streamType = factor (streamType)) %>%
    summarize_at (vars(n_syllables:average_bw_tp),
                  function (X) {
                      wilcox.test(X ~ get ("streamType", .), 
                                  ., 
                                  paired = FALSE)$p.value
                  })


dat.recall.s.wilcox.by.streamType <-
    dat.recall.s %>%
    ungroup () %>%
    mutate (streamType = factor (streamType)) %>%
    summarize_at (vars(n_syllables:average_bw_tp),
                  function (X) {
                      wilcox.test(X ~ get ("streamType", .), 
                                  ., 
                                  paired = FALSE)$p.value
                  })
```

```{r averages-across-subjects-calculate, include = FALSE}
dat.recall.m2 <- dat.recall.m %>% 
    group_by(streamType) %>%
    summarize_at (vars(n_syllables:average_bw_tp), mean, na.rm = TRUE)

dat.recall.s2 <- dat.recall.s %>% 
    group_by(streamType) %>%
    summarize_at (vars(n_syllables:average_bw_tp), mean, na.rm = TRUE)

```

As shown in Table \ref{tab:recall_all_averages}, participants produced on average `r dat.recall.s[dat.recall.s$streamType == "segmented",]$is_word %>% mean` words in the segmented condition, and `r dat.recall.s[dat.recall.s$streamType == "continuous",]$is_word %>% mean` in the cotinuous condition.

```{r averages-print}
dat.recall.m2 %>% 
        rbind (., 
           cbind(streamType = "$p_{Wilcoxon}$", dat.recall.m.wilcox.by.streamType)) %>% 
    column_to_rownames("streamType") %>%
    t %>% 
    #knitr::kable("latex", booktabs = T, caption = '\\label{tab:recall_all_averages}All averages. The *p* value has been calculated from a paired Wilcoxon test across the familiarization conditions.') %>%
    knitr::kable (caption = "\\label{tab:recall_all_averages}All averages. The *p* value has been calculated from a paired Wilcoxon test across the familiarization conditions.") %>%
    kableExtra::kable_styling(bootstrap_options = "striped")





```

In Table \ref{tab:recall_count_var}, we consider only those count variables where the counts were significantly greater than zero by a one tailed Wilcoxon test.

```{r counts-print}

dat.recall.s2.for.print <- dat.recall.s2[,c("streamType",
          dat.recall.s.selected.vars.by.wilcox)] %>%
    rbind (., 
           cbind(streamType = "$p_{Wilcoxon}$", dat.recall.s.wilcox.by.streamType[,dat.recall.s.selected.vars.by.wilcox])) %>% 
    column_to_rownames("streamType") %>%
    t %>%
    data.frame

count.vars.sign.stars <- lapply (rownames (dat.recall.s2.for.print)[-3], 
        function (X) (dat.recall.s.selected.vars.by.wilcox.df %>% 
                           filter (var == X))[,c("continuous", "segmented")]) %>% 
    do.call (rbind, .) %>%
    mutate_all (function (X) ifelse (X <= .05, "*", ""))

dat.recall.s2.for.print$continuous <- 
    paste (round (dat.recall.s2.for.print$continuous, 3),
           count.vars.sign.stars$continuous)
dat.recall.s2.for.print$segmented <- 
    paste (round (dat.recall.s2.for.print$segmented, 3),
           count.vars.sign.stars$segmented)

dat.recall.s2.for.print %>%
     setNames (c("continuous", "segmented", "$p_{Wilcoxon}$")) %>%
    #knitr::kable("latex", booktabs = T, caption = '\\label{tab:recall_count_var}Count variables that are significantly great than zero by a Wilcoxon test. The *p* value has been calculated from a paired Wilcoxon test across the familiarization conditions.') %>%
    knitr::kable (caption = "\\label{tab:recall_count_var}Count variables that are significantly great than zero by a Wilcoxon test. The *p* value has been calculated from a paired Wilcoxon test across the familiarization conditions.") %>%
    kableExtra::kable_styling(bootstrap_options = "striped")
```

```{r averages-plot, fig.cap="\\label{fig:recall_w_vs_pw}. Counts of words and part-words produced by the participants. The counts reflect only words and part-words, but not concatenations thereof."}
current.plot.name <- "recall_numbers"
prepare.graphics

dat.recall.s %>% 
    ungroup %>%
    data.table::setDT(.) %>% 
    data.table::dcast (subjNum + subjInitials ~ streamType, 
                       value.var = c("is_word", "is_part_word")) %>%
    data.table::setDF(.) %>%
    dplyr::select (c(is_word_continuous, is_part_word_continuous, is_word_segmented, is_part_word_segmented)) %>%
    strip4c(.,
            main="",
            ylim=c(-0.5,4),  pch=21, mean.pch=17, x=c(1, 2, 4, 5),
            offset = .5,
            ylab="Number of items recalled",
            xlab_big=c("Continuous", "Segmented"), xlab_big_at=c(1.5, 4.5), xlab_big_line=1,
            xlab_exp=rep(c("W", "PWs"), 2), xlab_exp_at=c(1:2, 4:5), xlab_exp_line=-.5,
            margins=c(3.5,6.5,2.5,2.5), write.percent=FALSE, forced.digits=2,
            ref.line = NULL)

show.graphics

```

## Word vs. part-word analysis
```{r word-vs-pw-analysis-calculate, include = FALSE}
w.vs.pw <- dat.recall %>% 
    group_by(subjNum, subjInitials, streamType, correct_segm) %>%
    summarize_at (vars (is_single_or_multiple_words, is_single_or_multiple_part_words), sum, na.rm = TRUE) %>%
    mutate (p_word_vs_part_word = ifelse ((is_single_or_multiple_words == 0 ) &
                                              (is_single_or_multiple_part_words == 0),
                                          .5,
                                          is_single_or_multiple_words / (is_single_or_multiple_words + is_single_or_multiple_part_words)))

w.vs.pw.wide <- w.vs.pw %>%
    data.table::setDT(.) %>% 
    data.table::dcast (subjNum + subjInitials ~ streamType, 
                       value.var = c("correct_segm",
                                     "is_single_or_multiple_words",
                                     "is_single_or_multiple_part_words",
                                     "p_word_vs_part_word")) %>%
    data.table::setDF(.) %>% 
    mutate (d_segm = correct_segm_segmented - correct_segm_continuous) %>%
    mutate (d_p_word_vs_part_word = p_word_vs_part_word_segmented - p_word_vs_part_word_continuous) %>%
    mutate (d_segm_p_word = d_segm - d_p_word_vs_part_word)

w.vs.pw.long <- w.vs.pw %>% 
    gather (testType, 
            p.cor, 
            c(correct_segm, p_word_vs_part_word),
            factor_key = TRUE) 

# Long version of data frame for differences 
w.vs.pw.d.long <- w.vs.pw.wide %>% 
    gather (testType, 
            d, 
            c(d_segm, d_p_word_vs_part_word),
            factor_key = TRUE) 


```

```{r word-vs-pw-analysis-print}
w.vs.pw.wide %>%
    dplyr::select(starts_with("p_word_vs_part_word")) %>%
    summarize_all (funs(n(), mean(., na.rm = TRUE))) %>%
    t %>%
    kable
```

```{r word-vs-pw-analysis-plot, fig.cap="\\label{fig:recall_w_vs_pw}. Perentage of words among words and part-words. The percentage counted both words and part-words and concatenations thereof. The below-chance performance in the continuous condition is expected if participants start items on a random syllable, because they are twice as likely to produce an item starting with the second or the third syllable of a word than to start with a word-initial syllable."}
current.plot.name <- "recall_w_vs_pw"
prepare.graphics

w.vs.pw.wide %>%
    dplyr::select(c(starts_with("correct_segm"),
        starts_with("p_word_vs_part_word"))) %>%
    mutate_all (function (X) 100 * X) %>%
    strip4c(.,
            main="Words vs. Part-Words",
            ylim=c(0,100),  pch=21, mean.pch=17, x=c(1, 2, 4, 5),
            offset = .4,
            ylab=TeX("$100 \\times \\frac{Words}{Words + Part-Words}$"),
                        xlab_sma=rep(c("Cont.", "Segm."), 2), xlab_sma_at=c(1, 2, 4, 5), xlab_sma_line=.2,
            xlab_big=c("Recognition", "Recall"), xlab_big_at=c(1.5, 4.5), xlab_big_line=2,
            margins=c(4.5,6.5,2.5,2.5), write.percent=TRUE, forced.digits=2,
            ref.line = 50)

show.graphics
```

```{r sw-acc-calculate, include = FALSE}
w.vs.pw.sw <- 
    w.vs.pw.long %>% 
    calculate.shapiro.wilk.test.for.cells(.,
                                          c("testType",
                                            "streamType"),
                                          "p.cor",
                                          .return.msg = FALSE)

```


```{r sw-acc-print}
 
if (any (w.vs.pw.sw$p.value <= .05)) {
    w.vs.pw.sw %>%
        filter (p.value <= .05) %>%
        setNames(replace_column_labels(names (.))) %>%
        #dplyr::select (-c(locCond)) %>%
        arrange (-row_number()) %>%
        knitr::kable (caption = "\\label{tab:sw_acc}Cells across experiments where a violation of normality was detected by a Shapiro-Wilk test when performance was measured in terms of accuracy.")
}

```


```{r sw-d-calculate, include = FALSE}
w.vs.pw.d.sw <- 
    w.vs.pw.d.long %>% 
    calculate.shapiro.wilk.test.for.cells(.,
                                          c("testType"),
                                          "d",
                                          .return.msg = FALSE)
```


```{r sw-d-print}
 
if (any (w.vs.pw.d.sw$p.value <= .05)) {
    w.vs.pw.d.sw %>%
        filter (p.value <= .05) %>%
        setNames(replace_column_labels(names (.))) %>%
        #dplyr::select (-c(locCond)) %>%
        arrange (-row_number()) %>%
        knitr::kable (caption = "\\label{tab:sw_acc}Cells across experiments where a violation of normality was detected by a Shapiro-Wilk test when performance was measured in terms of accuracy.")
}

```


```{r will-be-anova}
lapply (grep ("^d_", names (w.vs.pw.wide), value = TRUE),
        function (X){
          cbind (d = X,   
            P = w.vs.pw.wide %>%
                pull (X) %>% 
            wilcox.p(.))
        }) %>%
    do.call (rbind, .) %>%
    kable (caption = "\\label{tab:wilcox_d}Wilcoxon tests for various differences. No of them is normally distributed.")

```

```{r wmc.within.circle.12_20-anova-calculate}
# wmc.within.circle.12_20.aov <- dat.circle.combined.within.m %>%
#     filter (experimentID == "wmc.within.circle.12_20") %>%
#     mutate_at (c("subj", "piCond", "nItems"), factor) %>%
#     aov (cor ~ 
#              piCond * nItems + 
#              Error (subj/(piCond * nItems)),
#          data = .) 
# 
# wmc.within.circle.12_20.aov.ez <- dat.circle.combined.within.m %>%
#     filter (experimentID == "wmc.within.circle.12_20") %>%
#     ezANOVA(data = .,
#             dv = .(cor),
#             wid = .(subj),
#             within = .(piCond, nItems),
#             between = NULL,
#             detailed = TRUE,
#             type = 3)
# 
# wmc.within.circle.12_20.aov.ez.sphericity <- 
#     extract.sphericity.from.ezANOVA(wmc.within.circle.12_20.aov.ez)
# 
# ```
# 
# The results of overall ANOVA in Experiment 3 are given in the table below
# 
# ```{r nItems-anova-show}
# 
# if (!is.null (wmc.within.circle.12_20.aov.ez.sphericity)){
#     warning ("There were sphericity corrections available that you ignored, do something about them.")    
# }
# 
# 
# wmc.within.circle.12_20.aov.ez$ANOVA %>%
#     mutate (Effect = replace_condition_labels(Effect)) %>%
#     setNames(replace_column_labels(names (.))) %>%
#     knitr::kable (caption = "\\label{tab:anova_acc_12_20_ez}ANOVA for Experiments 3")
# 
# report.aov (wmc.within.circle.12_20.aov,
#        .correct.for.repeated.tests = FALSE,
#                         .return.df = TRUE,
#        .print.results = FALSE) %>%
#     dplyr::select(-c("model.name")) %>%
#     mutate (effect = replace_condition_labels(effect)) %>%
#     setNames(replace_column_labels(names (.))) %>%
#     knitr::kable (caption = "\\label{tab:anova_acc_12_20}ANOVA for Experimen 3.")
# 

```

```{r save-data-for-alexandra}
xlsx::write.xlsx (dat.recall,
                  file="segmentation_recall_transcriptions_output.xlsx",
                  row.names=FALSE,
                  sheetName="complete",
                  append=FALSE)


dat.recall.m %>% 
    ungroup %>%
    data.table::setDT(.) %>% 
    data.table::dcast (subj  + subjNum + subjInitials + Age + Gender ~ streamType, 
                       value.var = names(.)[7:31]) %>%
    data.table::setDF(.) %>%
    xlsx::write.xlsx (.,
                      file="segmentation_recall_transcriptions_output.xlsx",
                      row.names=FALSE,
                      sheetName="means",
                      append=TRUE)

dat.recall.s %>% 
    ungroup %>%
    data.table::setDT(.) %>% 
    data.table::dcast (subj  + subjNum + subjInitials + Age + Gender ~ streamType, 
                       value.var = names(.)[7:31]) %>%
    data.table::setDF(.) %>%
    xlsx::write.xlsx (.,
                      file="segmentation_recall_transcriptions_output.xlsx",
                      row.names=FALSE,
                      sheetName="sums",
                      append=TRUE)



dat.recall.s %>% 
    ungroup %>%
    data.table::setDT(.) %>% 
    data.table::dcast (subj  + subjNum + subjInitials + Age + Gender ~ streamType, 
                       value.var = dat.recall.s.selected.vars.by.wilcox) %>%
    data.table::setDF(.) %>%
    xlsx::write.xlsx (.,
                      file="segmentation_recall_transcriptions_output.xlsx",
                      row.names=FALSE,
                      sheetName="sums (>0)",
                      append=TRUE)

w.vs.pw.wide %>%
    xlsx::write.xlsx (.,
                      file="segmentation_recall_transcriptions_output.xlsx",
                      row.names=FALSE,
                      sheetName="w_vs_pw",
                      append=TRUE)

```