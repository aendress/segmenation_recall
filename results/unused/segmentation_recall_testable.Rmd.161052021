---
title: "Recall for segmentation"
author: "Ansgar Endress"
output:
  pdf_document:
    toc: true
    number_sections: true
    keep_tex: true
    fig_caption: true
  html_document:
    theme: spacelab      
    number_sections: yes
    df_print: paged
    toc: yes
    toc_float: yes
    fig_caption: true
  html_notebook:
    theme: spacelab      
    number_sections: yes
    toc: yes
    toc_float: yes
    fig_caption: true
---

# House keeping

```{r recall-setup, echo = FALSE, include=FALSE}
rm (list=ls())

options (digits = 3)
knitr::opts_chunk$set(
    # Run the chunk
    eval = TRUE,
    # Don't include source code
    echo = FALSE, 
    # Print warnings to console rather than the output file
    warning = FALSE,  
    # Stop on errors
    error = FALSE,
    # Print message to console rather than the output file
    message = FALSE,
    # Include chunk output into output
    include = TRUE,
    # Don't reformat R code
    tidy = FALSE,
    # Center images
    fig.align = 'center',
    # Default image width
    out.width = '80%')

# other knits options are here:
# https://yihui.name/knitr/options/
```

```{r recall-set-parameters, echo = FALSE, include=FALSE}
# Parameters used below

PRINT.INDIVIDUAL.PDFS <- TRUE

RESEGMENT.RESPONSES <- FALSE

# Remove items that contain unattested syllables
# Set to FALSE as this would lead to problems with participants having no vocalization for one of the conditions.
FILTER.UNATTESTED.ITEMS <- FALSE

#ALLOW.CONCATENTATIONS.FOR.W.VS.PW.ANALYSIS <- TRUE

REMOVEBADSUBJ <- TRUE

ANALYZED.DATA.SETS <- c(CITY = FALSE,
                        TESTABLE = TRUE)

```

In the analyses below, we use the following parameters: 
```{r recall-list-parameters}
knitr::kable(
    do.call (rbind, 
             lapply (ls(),
                     function (X) 
                         data.frame(Name = X, Value = as.character(get (X)))
             )
    )
)
```

```{r recall-load-libraries, include = FALSE, message = TRUE, warning = TRUE}

# check this
#http://www.ats.ucla.edu/stat/r/dae/melogit.htm
#http://www.sagepub.com/upm-data/38503_Chapter6.pdf
# Read in a random collection of custom functions
# Read in a random collection of custom functions
if (Sys.info()[["user"]] %in% c("ansgar", "endress")){
    source ("/Users/endress/R.ansgar/ansgarlib/R/tt.R")
    source ("/Users/endress/R.ansgar/ansgarlib/R/null.R")
    #source ("helper_functions.R")
} else {
    # Note that these will probably not be the latest versions
    source("http://endress.org/progs/tt.R")
    source("http://endress.org/progs/null.R")
}

library ("knitr")
library (kableExtra)
library (stringr)
library (rlang)
```

```{r recall-helper-functions-string-manipulation, include = FALSE}

count.sylls <- function (items){
    
    sapply (items,
            function (X) nchar (X) /2 )
    
}

get.unique.str.length <- function (items){
    
    item.len <- unique (stringr::str_length(items))
    
    if (length (item.len) > 1)
        stop ("Inconsistent item length")
    
    return (item.len)
}


get.substrings.of.length <- function (items, substr.len = 2, allow.overlap = FALSE, overlap.offset = NULL, simplify = TRUE){
    
    if (allow.overlap) {
        if (is.null (overlap.offset)){
            substr.offset <- substr.len   
        } else {
            substr.offset <- overlap.offset
        }
    } else {
        substr.offset <- substr.len
    }
    
    sapply (items,
            function (X) {
                if (simplify){
                    substring (X,
                               seq(1, nchar(X)-substr.len + 1, substr.offset),
                               seq(substr.len, nchar(X), substr.offset))
                } else{
                    ifelse (nchar (X) < substr.len,
                            list (NULL),
                            list (substring (X,
                                             seq(1, nchar(X)-substr.len + 1, substr.offset),
                                             seq(substr.len, nchar(X), substr.offset)))) %>%
                        unlist
                }
            },
            simplify = simplify)
}

reverse.items <- function (items, syll.len = 2){
    
    items.as.vectors <- get.substrings.of.length(items,
                                                 substr.len = syll.len,
                                                 simplify = FALSE) 
    
    items.rev <- lapply (items.as.vectors,
                         function (X) paste (rev (X), collapse = "")) %>%
        unlist
    
    return (items.rev)
}

```
```{r recall-helper-functions-segmentation, include = FALSE}

new_candidate <- function (candidate, n.changes = 0, surface = candidate){
    
    if (is.null (candidate))
        return (NULL)
    
    structure (list(underlying = candidate,
                    surface = surface,
                    n.changes = n.changes,
                    closest.match = NA,
                    closest.match.length = NA,
                    closest.match.pos = NA,
                    stream = NA),
               class = "candidate")
}


replace_phoneme <- function (candidate.list = ., phoneme1, phoneme2){
    
    if (class (candidate.list) == "candidate")
        candidate.list <- list (candidate.list)
    
    
    if (class (candidate.list) != "list")
        stop ("candidate.list must be of type candidate or list.\n")
    
    new.candidate.list <- list ()
    for (candidate in candidate.list){
        
        new.candidate.list <- c (new.candidate.list,
                                 list (candidate))
        
        if (!grepl (phoneme1, candidate$underlying, ignore.case = TRUE))
            next
        
        for (ppos in stringi::stri_locate_all(candidate$underlying, 
                                              regex = phoneme1)[[1]][,"start"]){
            # Replace each of the matches.
            # Then recursively replace the other matches 
            new.candidate <- new_candidate (candidate$underlying, 
                                            candidate$n.changes + 1,
                                            candidate$surface)
            substr (new.candidate$underlying, 
                    ppos, ppos) <- phoneme2
            
            new.candidate.list <- c (new.candidate.list,
                                     replace_phoneme (new.candidate,
                                                      phoneme1, phoneme2))
            
        }
        
    }
    
    return (new.candidate.list)
}

remove.geminates <- function (items){
    
    # https://stackoverflow.com/questions/29438282/find-repeated-pattern-in-a-string-of-characters-using-r
    geminate.regexp <- "(\\S+?)\\1(\\S)"
    
    if (is.list (items)){
        new.items <- list ()
    } else {
        new.items <- c()
    }
    
    for (current.item in items){
        
        while (grepl (geminate.regexp, current.item, perl = TRUE)){
            current.item <- gsub (geminate.regexp, "\\1\\2", 
                                  current.item, perl = TRUE)
        }
        
        new.items <- c(new.items, current.item)
        
    }
    
    new.items
}

get.syllables.from.words <- function (words = ., sort.sylls = TRUE){
    
    get.substrings.of.length (words, simplify = FALSE) %>%
        unlist %>% 
        unname %>% 
        {if (sort.sylls) sort (.) else .} %>% 
        unique
}

find_syllable_match <- function (candidate.list = ., syllable.list){
    
    if (class (candidate.list) == "candidate")
        candidate.list <- list (candidate.list)
    
    new.candidate.list <- list ()
    
    for (candidate in candidate.list){
        closest.match <- candidate$underlying
        
        closest.match.sylls <- c(get.substrings.of.length(closest.match))
        
        closest.match.sylls[!(closest.match.sylls %in% syllable.list)] <- "XX"
        
        closest.match <- paste (closest.match.sylls, collapse="")
        
        candidate$closest.match <- closest.match
        
        new.candidate.list <- c (new.candidate.list,
                                 list (candidate))    
    }
    
    new.candidate.list    
    
}

segment.extra.spaces <- function (utterance){
    
    # If any of the segmented items contains just a single contigent vowel
    #   Remove the spaces and keep the resulting item
    # else
    #   Keep separate entries of each item enclosed by a space
    # end							
    
    
    if (!grepl ("\\s", utterance))
        return (utterance)
    
    # Temporarily split utterance
    utterance.split <- strsplit(utterance, "\\s+")[[1]]
    
    # Count the number of vowels    
    n.vowels <- sapply (utterance.split,
                        str_count, "[aeiou]+")
    
    if (any (n.vowels == 1)){
        # One of the items is a single syllable, remove the spaces
        return (list (gsub ("\\s+", "", utterance)))
    } else {
        return (utterance.split)   
    }
}

segment.utterance <- function (utterance){
    if (grepl ("[;,]", utterance)){
        
        # First pass segmentation based on characters
        utterance.split <- strsplit(utterance, "[;,]+")[[1]]
        
        if (grepl("\\s", utterance)){
            # Deal with additional spaces
            
            utterance.split <- lapply (utterance.split,
                                       segment.extra.spaces) %>%
                unlist
        }
        
    } else {
        
        utterance.split <- strsplit(utterance, "\\s+")
    }
    
    return (utterance.split)
}


get.non.repeating.word.sequences <- function (words, max_length, return.df = FALSE){
    
    if ((length (words) == 1) & is.numeric(words))
        words <- 1:words
    
    word.repetition.filter <-  paste (
        paste ("(Var", 1:(max_length-1), sep =""),
        paste ("Var", 2:max_length, ")", sep =""),
        sep = "!=",
        collapse = "&")
    
    word.seq <- expand.grid(lapply (1:max_length, 
                                    function (X) words)) %>%
        filter (!!!parse_exprs(word.repetition.filter)) 
    
    if (return.df)
        return (word.seq)
    
    word.seq <- word.seq %>%
        apply (1, paste, collapse = "")
    
    return (word.seq)
}

find.longest.match <- function (target, lang, word.sequences) {
    
    # Find a match in any of the word.sequences with the full length 
    # of the word
    #
    # If no match is found, generate all subsequence of the target 
    # with on syllable less and recursively call the function again
    # until a match is found (or return NULL)
    
    if (nchar (target) < 2 )
        return (NULL)
    
    # Needs to be multiplied by 2 as the matches are segment based
    # The maximal distance is 0, except if one of the segments is XX, in which
    # case it's ignored
    max.dist <- 2 * str_count (target, "XX")
    
    for (ws in 1:length(word.sequences)){
        
        # 1. Search from the word onset
        current.word.list <- substr(word.sequences[[ws]][[lang]], 1, 
                                    nchar (target)) %>% 
            unique
        current.dist <- stringdist::stringdist (target,
                                                current.word.list,
                                                "hamming")
        
        if (any (current.dist == max.dist)) {
            
            return (list (match = target,
                          length = nchar (target),
                          pos = 1,
                          stream = names(word.sequences)[ws]))
        }
        
        # 2. Search from the word offset
        current.word.list.word.length <- get.unique.str.length (
            word.sequences[[ws]][[lang]])
        current.word.list <- substr(word.sequences[[ws]][[lang]], 
                                    current.word.list.word.length - nchar (target) + 1,
                                    current.word.list.word.length) %>% 
            unique
        
        current.dist <- stringdist::stringdist (target,
                                                current.word.list,
                                                "hamming")
        
        if (any (current.dist == max.dist)) {
            
            return (list (match = target,
                          length = nchar (target),
                          pos = 1,
                          stream = names(word.sequences)[ws]))
        }
        
        
        
    }
    
    # We haven't found a match yet
    target.fragement.length <- nchar(target)-2
    target.fragments <- get.substrings.of.length (target, 
                                                  substr.len = target.fragement.length, 
                                                  allow.overlap = TRUE, 
                                                  overlap.offset = 2)
    
    fragment.match.list <- list()
    for (tf in 1:length(target.fragments)){
        
        current.match <- find.longest.match (target.fragments[tf],
                                             lang,
                                             word.sequences)
        
        if (!is.null (current.match)) {
            current.match$pos <- (tf - 1) + current.match$pos  
            
            if (current.match$length == target.fragement.length){
                return (current.match)
            } else {
                fragment.match.list <- c(fragment.match.list, 
                                         list (current.match))
            }
            #return (current.match)
        }
        
    }
    
    if (length (fragment.match.list) > 0){
        
        longest.match.ind <- which.max (
            map_dbl (fragment.match.list, "length")
        )
        
        return (fragment.match.list[[longest.match.ind]])
    }
    
    return (NULL)
}

find.longest.matches <- function (targets, lang, word.sequences){
    
    lapply (targets, 
            find.longest.match,
            lang,
            word.sequences)
    
}

find.unique.candidates <- function (candidates = .){
    
    underlying.derivation.length <- lapply (candidates,
                                            function (X) {
                                                cbind.data.frame (underlying = X$underlying,
                                                                  n.changes = X$n.changes) 
                                            }) %>% 
        do.call (rbind, .) %>%  
        mutate (underlying = as.character (underlying)) %>% 
        group_by (underlying) %>% 
        summarize (n.changes = min(n.changes)) %>%
        column_to_rownames("underlying")
    
    
    keep <- sapply (candidates, 
                    function (X) {
                        X$n.changes == 
                            underlying.derivation.length[X$underlying,"n.changes"]    
                    }) 
    
    candidates  <- candidates[keep]
    candidates <- candidates[!(sapply (candidates, 
                                       function (X) X$underlying) %>%  
                                   duplicated)]
    
    candidates
}

select.candidates.by.surface.form <- function (candidates, lang, syllables){
    
    # Remove candidates for which no match has been found 
    candidates.df <- candidates %>% 
        do.call (rbind.data.frame, .) %>% 
        filter (!is.na(closest.match)) %>% 
        mutate (n.attested.sylls = ifelse (
            is.na (underlying), 
            0,
            sapply (as.character (underlying),
                    function (U){
                        get.syllables.from.words (U) %>%
                            is.item.type (syllables[[lang]]) %>%
                            unlist %>%
                            sum (na.rm = TRUE)
                    }
            ))) %>%
        # Calculate by surface form, and filter 
        # underlying forms in this anking 
        # (1) maximum number of attested sylls.
        # (2) maximum length
        # (3) Number of changes
        group_by (surface, .drop = FALSE) %>% 
        filter (n.attested.sylls == max (n.attested.sylls)) %>% 
        filter (closest.match.length == max (closest.match.length)) %>% 
        filter (n.changes == min (n.changes)) %>% 
        ungroup
    if (nrow (candidates.df) == 0){
        candidates.df[1,names(candidates.df)] <- rep (NA, 
                                                      ncol (candidates.df)) %>% 
            t
    }
    
    
    return (candidates.df)
}

add.other.syllables.to.match <- function (underlying, lang, closest_match_just_match, ...) {
    # Take the closest match and add the remaining 
    # syllables in the underlying form
    
    if (is.na (closest_match_just_match))
        return (closest_match_just_match)
    
    if (underlying == closest_match_just_match)
        return (closest_match_just_match)
    
    closest.match.pos <- stringi::stri_locate_first(
        underlying, 
        fixed = closest_match_just_match)
    
    # Split underlying form into syllables
    underlying.sylls <- get.syllables.from.words(
        underlying, 
        sort.sylls = FALSE)
    
    # Replace unattested syllables with XX
    underlying.sylls[!is.item.type (
        underlying.sylls, 
        syllables[[lang]])] <- "XX"
    
    closest.match2 <- paste(underlying.sylls, 
                            collapse = "")
    # Sanity check
    if (closest_match_just_match != substr (closest.match2,
                                            closest.match.pos[1,"start"],
                                            closest.match.pos[1,"end"]))
        warning ("Closest match ", closest_match_just_match, " does not match the underlying form ", underlying)
    
    return (closest.match2)
    
}

process.utterance <- function (utterance, lang, word.sequences, syllables){
    
    # 1. Apply pre-segmentation substitutions 
    
    utterance <- apply.substitution.rules.pre.segmentation(utterance)
    
    # 2. Segment into candidates
    
    utterance.split <- segment.utterance (utterance)
    
    # 3. Apply post-segmentation substitutions and make items unique
    
    utterance.split <- lapply (utterance.split,
                               remove.geminates) %>% 
        unlist %>% 
        unique 
    
    candidates <- lapply (unlist (utterance.split),
                          # Create candidate data structures for all candidates 
                          new_candidate) %>% 
        # And apply substitutions
        apply.substitution.rules.post.segmentation %>%
        find.unique.candidates
    
    # 4. Find longest match
    
    for (cand in 1:length(candidates)){
        # Create matches between the underlying forms 
        # of the candidate and the syllables in a language
        
        current.match <- find.longest.matches(
            candidates[[cand]]$underlying, 
            lang, 
            word.sequences) %>% 
            unlist (recursive = FALSE)
        
        if (!is.null (current.match)){
            
            candidates[[cand]]$closest.match <- current.match$match
            candidates[[cand]]$closest.match.length <- current.match$length
            candidates[[cand]]$closest.match.pos <- current.match$pos
            candidates[[cand]]$stream <- current.match$stream
            
        } else {
            
            candidates[[cand]]$closest.match <- NA
            candidates[[cand]]$closest.match.length <- 0
            candidates[[cand]]$closest.match.pos <- -1
        }
        
    }
    
    # 5. select longest match for each surface form
    
    candidates <- select.candidates.by.surface.form (candidates, lang, syllables)
    
    return (candidates)
}

```

```{r recall-helper-functions-string-analysis, include = FALSE}
is.item.type <- function (items = ., 
                          all.items.for.type){
    
    sapply (items, 
            function (X) X %in% all.items.for.type)
}

is.concatenation.of.item.type <- function (items,
                                           all.items.for.type){
    
    item.length <- sapply (all.items.for.type, nchar) %>% 
        unique
    
    if (length (item.length) > 1)
        stop ("Items do not have a consistent length")
    
    # Pad items to minimum length where required
    # Changed July 28th, 2020
    items <- ifelse (is.na (items),
                     rep ("x", item.length),
                     items)
    items <- sapply (items,
                     function (current.item) {
                         if (nchar (current.item) < item.length) {
                             paste (current.item,
                                    rep ("x", 
                                         item.length - nchar (current.item)), 
                                    collapse="") 
                         } else {
                             current.item
                         }
                     })
    # End change
    
    is.concatenation <- lapply (items,
                                get.substrings.of.length, item.length) %>%
        lapply (is.item.type,
                all.items.for.type) %>%
        lapply (all) %>% 
        unlist
    
    # Exclude single items
    is.concatenation[nchar (items) <= item.length] <- FALSE
    
    return (is.concatenation)
}

is.chunk.from.item.type <- function (items, 
                                     all.items.for.type,
                                     min.length = 4){
    
    # July 28th, 2020
    # Strip leading and trailing unattested syllables
    items <- gsub ("^x+", "", 
                   items, ignore.case = TRUE)
    items <- gsub ("x+$", "", 
                   items, ignore.case = TRUE)
    
    is.chunk <- sapply (items, 
                        function (X) {
                            ifelse (nchar(X) < min.length,
                                    FALSE,
                                    grepl (X, all.items.for.type) %>% 
                                        any())
                        })
    
    is.chunk[is.na(items)] <- FALSE
    
    return (is.chunk)
}

has.correct.initial.syll<- function (items, 
                                     all.items.for.type){
    
    sapply (items, 
            function (X) {
                grepl (paste ("^",
                              substr(X, 1, 2),
                              sep =""), 
                       all.items.for.type) %>%
                    any()
                
            })
}

has.correct.final.syll <- function (items, 
                                    all.items.for.type){
    
    sapply (items, 
            function (X) {
                grepl (paste (substr(X, nchar(X)-1, nchar(X)),
                              "$",
                              sep =""), 
                       all.items.for.type) %>%
                    any()
                
            })
}


get.part.words <- function (words, parts.word1 = c(3), parts.word2 = c(1, 2), allow.repeats = FALSE){
    
    words.as.sylls <- get.substrings.of.length (words) %>%
        as.data.frame(stringsAsFactors = FALSE) %>%
        as.list
    
    part.words <- c()
    for (first.word.ind in 1:length(words)){
        
        second.word.inds <- 1:length(words)
        if (!allow.repeats)
            second.word.inds <- second.word.inds[-first.word.ind]    
        
        part.words.current <- lapply (second.word.inds,
                                      function (X) paste (
                                          paste (words.as.sylls[[first.word.ind]][parts.word1], 
                                                 collapse=""),
                                          paste (words.as.sylls[[X]][parts.word2],
                                                 collapse=""),
                                          sep = "")) %>%
            unlist 
        
        part.words <- c(part.words,
                        part.words.current)
    }
    return (part.words)
}

#' calculate.average.tps.from.chunks
#'
#' @description
#' Calculate the average TPs in a set of items based 
#'   on the TPs in its constituent chunks. 
#'
#' @param items A vector of items whose TPs should be calculated
#' @param item.type.list List of lists with element chunks and tp
#'   containing chunks with their TPs 
#' @param chunk.length Length of the chunks (in characters) whose 
#'   TPs should be calculated
#'
#' @return Vector of the average TPs in \code{items}
#'
#' @examples 
#' calculate.average.tps.from.chunks (items, list (list(chunks=chunkVector, tp=3)), 4)
#' 
#' @details 
#' For each item in \code{items}, the function generates all substrings
#' of length \code{chunk.length}. For each substring, it loops through 
#' \code{item.type.list}, checks whether the substring is contained in 
#' the chunk elements, records the corresponding TP and averages the TP
#' across chunks
calculate.average.tps.from.chunks <- function (items,
                                               item.type.list,
                                               chunk.length = 4){
    mean.tps.in.items <- c()
    for (current.item in items){
        
        if (is.na (current.item) |
            is.null (current.item) |
            (nchar (current.item) < chunk.length)) {
            mean.tps.in.items <- c(mean.tps.in.items,
                                   NA)
            next
        }
        
        current.chunk.list <- get.substrings.of.length(current.item, 
                                                       chunk.length, 
                                                       allow.overlap = TRUE, 
                                                       overlap.offset = chunk.length/2,
                                                       simplify = FALSE)
        
        # Loop through the chunks for the current item
        tps.in.current.chunks <- c()
        for (current.chunk in unlist (current.chunk.list)){
            
            current.tp <- 0
            for (current.item.type in item.type.list){
                
                if (any (grepl (current.chunk, current.item.type$chunks))){
                    
                    current.tp <- current.tp + current.item.type$tp    
                    
                }
            }
            
            tps.in.current.chunks <- c(tps.in.current.chunks, 
                                       current.tp)
        }
        mean.tps.in.items <- c(mean.tps.in.items,
                               mean (tps.in.current.chunks))
    }
    
    return (mean.tps.in.items)
}

#' calculate.expected.tps.for.chunks
#'
#' @description
#' Calculate the expected TPs in a set of items if the items
#'   correctly reproduce the speech stream
#'
#' @param items A vector of items whose TPs should be calculated
#' @param words Vector of words used to determine the expected TP
#' @param high.tp Within-word TPs (Default: 1)
#' @param low.tp Across-word TPs (Default: 1/3)
#' @param syll.len Length (in characters) of syllables (Default: 2)
#'
#' @return Vector of the expected TPs in \code{items}
#'
#' @details
#' The function first generates two lists:
#'   * A list of syllables that can occur in each position of a word. 
#'   * A list of vectors of TPs expected for each syllable. For example, 
#'     an item starting on a word-final syllable has the expected TPs
#'     \code{c(low.tp, high.tp, low.tp, ...)}
#' 
#' For each item in \code{items}, then determines the starting position 
#' (or picks a random position if none can be determined), retrieves the 
#' vector of TPs expected for this starting position, trims the vector 
#' to the length expected by the length of the item and returns the 
#' average expected TP.
calculate.expected.tps.for.chunks <- function (items, words, high.tp = 1, low.tp = 1/3, syll.len = 2) {
    
    # use words to detect whether we start with an A, B or C syllable
    word.len <- get.unique.str.length(words)
    
    # Generate list of the list of syllables that can 
    # occur in each position 
    potential.starting.sylls <- lapply (    
        seq (1, word.len, syll.len),
        function (X) substr (words, X, X + syll.len-1))
    
    # Calculate expected TPs
    max.item.length <- max (nchar (items)) / syll.len
    expected.tps <- list (
        # Starting with an A syllable
        rep (c(high.tp, high.tp, low.tp), ceil (max.item.length / 3)),
        # Starting with a B syllable
        rep (c(high.tp, low.tp, high.tp), ceil (max.item.length / 3)),
        # Starting with a C syllable
        rep (c(low.tp, high.tp, high.tp), ceil (max.item.length / 3)))
    
    lapply (items,
            function (X) {
                
                if ((nchar(X) / syll.len) < 2)
                    return (NA)
                
                # Find current starting position within a word 
                # or pick a random position if none can be determined
                current.starting.pos <- which (
                    lapply (potential.starting.sylls,
                            function (PSS) any (gdata::startsWith (X, PSS))) %>%
                        unlist)
                
                if (length (current.starting.pos) == 0){
                    warning (paste("No starting position could be determined for item ", 
                                   X, 
                                   ". Picking random position instead.", sep =""))
                    current.starting.pos <- sample (length (expected.tps), 1)
                }
                
                # Retrieve vector of expected TPs based on 
                # this starting position
                current.expected.tps <- expected.tps[[current.starting.pos]]
                
                # Trim the vector of expected TPs 
                current.expected.tps <- current.expected.tps[1:((nchar(X)/syll.len)-1)]
                
                # Average the TPs
                return (mean (current.expected.tps))
            }) %>%
        unlist
    
}
```

```{r recall-helper-functions-general, include = FALSE}
wilcox.p <- function (x, mu = 0)
{
    x <- as.numeric (x)
    if ((all %.% is.na) (x)) {
        return (NA)
    } else {
        return (signif (
            wilcox.test (x, mu = mu)$p.value,
            getOption('digits')))
        
    }
}

t.test.p <- function (x, mu = 0)
{
    x <- as.numeric (x)
    if ((all %.% is.na) (x)) {
        return (NA)
    } else {
        return (signif (
            t.test (x, mu = mu)$p.value,
            getOption('digits')))
    }
}

replace_column_labels <- function (X)
{    
    # Uses pryr
    # Evalution from right to left
    compose (
        function (X) {gsub ("flanker.rt.d.median.split", 
                            "Flanker Group", X)},
        function (X) {gsub ("scs.median.split", 
                            "Self Control Group", X)},
        function (X) {gsub ("countCond", "*Secondary Task*", X)},    
        function (X) {gsub ("experimentID", "Experiment", X)},
        function (X) {gsub ("poolSize", "*Pool Size*", X)},
        function (X) {gsub ("nItems", "*Set Size*", X)},
        function (X) {gsub ("yPosCond", "*Sequential Position*", X)},
        function (X) {gsub ("locCond", "*Location Condition*", X)},
        
        function (X) {gsub ("countCond", "*Secondary Task*", X)},
        function (X) {gsub ("piCond", "*PI Condition*", X)},
        function (X) {gsub ("partial.eta.squared", "$\\\\eta_p^2$", X)},
        
        function (X) {gsub ("p<=.05", "$p \\\\leq .05$", X)},
        function (X) {gsub ("p.value", "$p$", X)},
        function (X) {gsub ("F.value", "*F*", X)},
        function (X) {gsub ("Cohen.d", "Cohen's *d*", X)},
        function (X) {gsub ("^CI$", "*CI*", X)},
        function (X) {gsub ("^P$", "$p$", X)},
        function (X) {gsub ("^p$", "$p$", X)},
        function (X) {gsub ("^t$", "$t$", X)},
        function (X) {gsub ("p.t.test", "$p_{t\\\\ test}$", X)},
        function (X) {gsub ("p.wilcox.test", "$p_{Wilcoxon}$", X)},
        function (X) {gsub ("^SE.log$", "*SE* (log)", X)},
        function (X) {gsub ("^SE$", "*SE*", X)},
        function (X) {gsub ("^SD.log$", "*SD* (log)", X)},
        function (X) {gsub ("^SD$", "*SD*", X)},
        function (X) {gsub ("^M.log$", "*M* (log)", X)},
        function (X) {gsub ("^M$", "*M*", X)},
        function (X) {gsub ("^effect$", "Effect", X)},
        function (X) {gsub ("^model.name$", "Experiment", X)},
        function (X) {gsub ("^Chisq$", "$\\\\chi^2$", X)},
        function (X) {gsub ("Chi Df", "Df", X)},
        function (X) {gsub ("Pr\\(>Chisq\\)", "$p$", X)},
        function (X) {gsub ("IV.removed", "Removed IV", X)}
    ) (X)
}

```

# Methods
## Materials
We resynthesized the languages used in @Saffran-Science Experiment 2. The four words in each language are given in Table \ref{tab:recall-languages}. Stimuli were synthesized using the us3 (male American English) voice of the mbrola @mbrola synthesizer, at with a constant F0 of 120 at a rate of 216 ms per syllable (108 ms per phoneme). 

```{r recall-recall-specificy-languages, include = FALSE}
words.fw <- list (L1 = c("pAbiku", "tibudO", "dArOpi", "gOLAtu"),
                  L2 = c("bikuti", "pigOLA", "tudArO", "budOpA"))
words.fw <- lapply (words.fw,
                    tolower)

words.bw <- lapply (words.fw,
                    reverse.items)

part.words.fw <- rbind.data.frame(
    # BCA
    lapply (words.fw,
            get.part.words,
            parts.word1 = c(2:3), 
            parts.word2 = c(1)),
    # CAB
    lapply (words.fw,
            get.part.words,
            parts.word1 = c(3), 
            parts.word2 = c(1:2)),
    stringsAsFactors = FALSE) %>%
    as.list 

part.words.bw <- rbind.data.frame(
    # BCA
    lapply (words.bw,
            get.part.words,
            parts.word1 = c(2:3), 
            parts.word2 = c(1)),
    # CAB
    lapply (words.bw,
            get.part.words,
            parts.word1 = c(3), 
            parts.word2 = c(1:2)),
    stringsAsFactors = FALSE) %>%
    as.list 

class.words.fw <- rbind.data.frame(
    # AiBiCj
    lapply (words.fw,
            get.part.words,
            parts.word1 = c(1:2), 
            parts.word2 = c(3)),
    # AiBjCj
    lapply (words.fw,
            get.part.words,
            parts.word1 = c(1), 
            parts.word2 = c(2:3)),
    stringsAsFactors = FALSE) %>%
    as.list 

low.tp.chunk.fw <- lapply (words.fw,
                           get.part.words,
                           parts.word1 = c(3), 
                           parts.word2 = c(1))

low.tp.chunk.bw <- lapply (words.bw,
                           get.part.words,
                           parts.word1 = c(3), 
                           parts.word2 = c(1))


syllables <- lapply (words.fw,
                     get.syllables.from.words)

# Generate list of concatenated words, bca part-words and cab part-words
word.sequences <- list (abc = lapply (words.fw,
                                      get.non.repeating.word.sequences,
                                      # Hopefully there will be no utterance longer than 
                                      # 10 * 3 = 30 syllables
                                      10)) 
word.sequences$bca <- lapply (word.sequences$abc,
                              substring, 3)
word.sequences$cab <- lapply (word.sequences$abc,
                              substring, 5)
```

```{r recall-print-languages}
words.fw %>%
    data.frame %>%
    #knitr::kable("latex", booktabs = T, caption = '\\label{tab:languages}Words used in the recall experiment.') %>%
    knitr::kable (caption = "\\label{tab:recall-languages}Languages used in the recall experiment.", booktabs = TRUE) %>%
    kable_styling(bootstrap_options = "striped")

```


During familiarization, words were presented 45 times each. For each participant, we generated a random concatenation of 45 repetitions of the 4 words, with the constraint that a words could not occur in immediate reptition. Each randomization was then (i) synthesized into a continuous speech stream using mbrola and then converted to mp3 using ffmpeg (https://ffmpeg.org/) (ii) used to concatenate words that had been synthesized in isolation, separated by silences of 222 ms into a segmented speech stream, which was then converted to mp3. Streams were faded in and out for 5 s using sox (http://sox.sourceforge.net/). For continuous streams, this yielded a stream duration of 1 min 57 s; for segmented streams, the duration was 2 min 37.


## Procedure
### Familiarization
Participants were informed that they would be listening to an unknown language and that they should try to learn the words from that language. Following, the familiarization stream was presented twice, leading to a total familiarization duration of 3 min 53 for the continuous streams and 5 min 13 for the segmented streams. They could proceed to the next presentation of the stream by pressing a button. 

For the online experiments, participants watched video with no clear objects during the familiarization (panning of the Carina nebula, obtained from https://esahubble.org/videos/heic0707g/). The video was combined with the speech stream using the the muxmovie utility.

Following the familiarization, the was a 30 s retention interval. Participants were instructed to count backwards from 99 in time with a metronome beat at 3s / beat. Performance was not monitored. 

(Note to self: This was the case for both psyscope and testable.)

### Recall test
Following the retention interval, participants completed the recall test. During the lab-based experiments, participants had 45 s to repeat back the words they remembered; their vocalizations were recorded using ffmpeg and saved in mp3 format. During the web-based experiments, participants had 60 s to type their answer into a comment field, during which they viewed a progress bar. 

### Recognition test
Following the recall test, participant completed a recognition test during which we pitted words against part-words. The (correct) test words for Language 1 (and part-words for Language 2) were /pAbiku/ and /tibudO/; the (correct) test words for Language 2 (and part-words for Language 1) were /tudArO/ and /pigOlA/.These items were combined into 4 test pairs

# Analysis
## Recognition test PUT LATER

* Use glmer 
* Test if intercept is > 0 
* This is equivalent to P > .5
* Since logit (x) = 1 / (1 + exp(-x))


## Recall test for the lab-based experiment

## Substitution rules compensating for potential misperceptions

* O might be perceived as A (but probably not vice versa)
* Voiced and unvoiced consonants can be confused:
  	 - g and k
	 - d and t
	 - b and p

* b might be perceived as v

In some cases, these rules give you several possible matches. For example, in line 64, rapidala might be rOpidAlA or rOpidOlA

In such case, we apply the following criteria to decide which match to choose (in this order).

1.  If one option provides more or longer existing chunks, choose it. For example, rOpidAlA has the chunk rOpi (pidA isn't possible in the stream), while rOpidOlA contains rOpi, so in this case the rule doesn't discrminate between the two :)

2. If one option requires fewer changes with respect to the original transcription, choose that.

I would apply the rules in this order, but I can see why you might want to use the opposite order as well.



## Categories of responses in the recall phase
We count the number of responses in each of the categories below

* Items with attested transitions across syllables
  * HIgh TP items
    - Words (3 syllables)
    - Backward word (3 syllables)
    - Combination of words (multiples of 3 syllables)
    - Combinations of backward words
  * Low TP items
    - Part-words (3 syllables)
    - Back part-words
    - Combinations of part-words (multiples of 3 syllables)
    - Combinations of backward part-words
* Items with unattested transitions
  - Items sharing the initial syllable with words
  - Items sharing the final syllable with words

## Statistical properties of the responses
* Forward TPs
* Backward TPs






#########################

## Define substitution rules



```{r specify-substitution-rules-pre-segmentation}

# Pre-segmentation subsitution rules
# These rules are not taken into consideration for the transformation count

substitution.rules.pre.segmentation <- list (
    list ("-", "", FALSE),
    list ("2", "tu", FALSE),
    list ("two", "tu", FALSE),
    # Some participants perceived "rock"
    list ("([aeou])ck", "\\1k", FALSE),
    list ("ar([,\\s+])", "a\\1", TRUE),
    # Some participant perceived "dollar"
    list ("ar$", "a", TRUE),
    # The next one most likely reflects a typo
    list ("tyu", "tu", FALSE),
    list ("ph", "f", FALSE),
    list ("th", "t", FALSE),
    list ("qu", "k", FALSE),
    list ("ea", "i", FALSE),
    list ("ou", "u", FALSE),
    list ("aw", "a", FALSE),
    list ("ai", "a", FALSE),
    list ("ie", "i", FALSE),
    list ("ee", "i", FALSE),
    list ("oo", "u", FALSE),
    list ("e", "i", FALSE),
    list ("c", "k", FALSE),
    list ("w", "v", FALSE),
    list ("y", "i", FALSE),
    list ("h", "", FALSE)) %>%
    rename_list_items (c("pattern", "replacement", "perl"))


apply.substitution.rules.pre.segmentation <- function (utterance = .){
    
    for (s.rule in substitution.rules.pre.segmentation){
        
        utterance <- gsub (s.rule$pattern,
                           s.rule$replacement, 
                           utterance, 
                           perl = s.rule$perl)
    }
    
    return (utterance)
    
    # July 27, 2020: We now loop through substitution
    # rules so they can be printed more easily
    
    # compose (
    #     function (X) {gsub ("h", "", X)},
    #     function (X) {gsub ("y", "i", X)},
    #     function (X) {gsub ("w", "v", X)},
    #     function (X) {gsub ("c", "k", X)},
    #     function (X) {gsub ("e", "i", X)},
    #     function (X) {gsub ("oo", "u", X)},
    #     function (X) {gsub ("ee", "i", X)},
    #     function (X) {gsub ("ie", "i", X)},
    #     function (X) {gsub ("ai", "a", X)},
    #     function (X) {gsub ("aw", "a", X)},
    #     function (X) {gsub ("ou", "u", X)},
    #     function (X) {gsub ("ea", "i", X)},
    #     function (X) {gsub ("qu", "k", X)},
    #     function (X) {gsub ("th", "t", X)},
    #     function (X) {gsub ("ph", "f", X)},
    #     
    #     # The next one most likely reflects a typo
    #     function (X) {gsub ("tyu", "tu", X)},
    #     # Some participant perceived "dollar"
    #     function (X) {gsub ("ar$", "a", X, 
    #                         perl = TRUE)},
    #     function (X) {gsub ("ar([,\\s+])", "a\\1", X,
    #                         perl = TRUE)},
    #     # Some participants perceived "rock"
    #     function (X) {gsub ("([aeou])ck", "\\1k", X)},
    #     function (X) {gsub ("two", "tu", X)},
    #     function (X) {gsub ("2", "tu", X)},
    #     function (X) {gsub ("-", "", X)}
    #     
    # ) (utterance)
    # 
}
```

```{r recall-specify-substitution-rules-post-segmentation}
# Post-segmentation subsitution rules
# These rules are  taken into consideration for the transformation count


substitution.rules.post.segmentation <- list (
    list ("u", "o"),
    list ("v", "b"),
    list ("p", "b"),
    list ("b", "p"),
    list ("t", "d"),
    list ("d", "t"),
    list ("k", "g"),
    list ("g", "k"),
    list ("a", "o")
) %>% 
    rename_list_items (c("pattern", "replacement"))

apply.substitution.rules.post.segmentation <- function (candidate = .){
    
    for (s.rule in substitution.rules.post.segmentation){
        
        candidate <- replace_phoneme (
            candidate,
            s.rule$pattern,
            s.rule$replacement)
    }
    
    return (candidate)
    
    # July 27, 2020: We now loop through substitution
    # rules so they can be printed more easily
    
    # compose (
    #     function (X) {replace_phoneme (X, "a", "o")},
    #     function (X) {replace_phoneme (X, "g", "k")},
    #     function (X) {replace_phoneme (X, "k", "g")},
    #     function (X) {replace_phoneme (X, "d", "t")},
    #     function (X) {replace_phoneme (X, "t", "d")},
    #     function (X) {replace_phoneme (X, "b", "p")},
    #     function (X) {replace_phoneme (X, "p", "b")},
    #     function (X) {replace_phoneme (X, "v", "b")},
    #     function (X) {replace_phoneme (X, "u", "o")}
    # ) (candidate)
}
```

```{r recall-print-substitution-rules}
bind_rows (
    substitution.rules.pre.segmentation %>% 
        lapply (., unlist) %>% 
        do.call (rbind, .) %>% 
        as.data.frame (stringsAsFactors = FALSE)%>% 
        select (-c("perl")),
    substitution.rules.post.segmentation %>% 
        lapply (., unlist) %>% 
        do.call (rbind, .) %>% 
        as.data.frame (stringsAsFactors = FALSE)
) %>% 
    kable (caption = "\\label{tab:substitution_rules}Substitution rules") %>%
    #     kable_styling() %>%
    pack_rows("Before segmentation",
                          1,
                          length (substitution.rules.pre.segmentation)) %>%
    pack_rows("After segmentation",
                          1+length (substitution.rules.pre.segmentation),
                          length (substitution.rules.pre.segmentation) + length (substitution.rules.post.segmentation))

```


```

``` 

## Load data
```{r recall-load-data, include = FALSE}

# Data from BSc at City
if (ANALYZED.DATA.SETS["CITY"]){
    dat.recall.city <- gdata::read.xls(
        "data/segmentation_recall_transcriptions.xlsx", 
                                       sheet="Sheet2-ade",
                                       stringsAsFactors = FALSE,
                                       header=TRUE) %>%
        mutate (subj = paste (subjNum, subjInitials, sep = ".")) %>%
        mutate (closest_match = tolower(closest_match)) %>%
        # dplyr::distinct (.keep_all = TRUE)
        dplyr::distinct (subjNum, subjInitials, streamType, lang, closest_match, .keep_all = TRUE) %>%
        filter (!is.na (subjNum))
}

if (ANALYZED.DATA.SETS["TESTABLE"]){
    # Data from testable
    dat.recall.tstbl <-
        read.testable.results("data/399612_results_20200424_0326", 
                              comment.char = "",
                              stringsAsFactors = FALSE)  %>%
        filter (myPhase != "sound_test") %>% 
        setNames (gsub ("myLang", "lang", names (.)))
    
    
}
```

```{r recall-find-and-remove-bad-subjects}
if (ANALYZED.DATA.SETS["CITY"]){
    bad.subj.city <- dat.recall.city %>% 
        filter (streamType == "continuous") %>% 
        distinct(subj, subjNum, subjInitials, correct_segm) %>%
        filter (correct_segm < .5) %>%
        pull ("subj")
    
    if (REMOVEBADSUBJ){
        
        dat.recall.city <- dat.recall.city %>%
            remove.bad.subj(bad.subj.city,
                            subj.var = "subj")
        
    }
}

if (ANALYZED.DATA.SETS["TESTABLE"]){
    bad.subj.tstbl <- dat.recall.tstbl %>% 
        filter (myPhase == "test_recognition") %>%
        distinct(filename, correct) %>%
        group_by (filename) %>% 
        summarize(correct_segm = mean (correct)) %>% 
        filter (correct_segm < .5) %>%
        pull ("filename")
    
    if (REMOVEBADSUBJ){
        
        dat.recall.tstbl <- dat.recall.tstbl %>%
            remove.bad.subj(bad.subj.tstbl,
                            subj.var = "filename")
        
    }
    
}
```

## Identify closest matches (for testable)
```{r recall-extract-recognition-performance}
if (ANALYZED.DATA.SETS["TESTABLE"]){
    dat.recall.tstbl.recognition.m <-dat.recall.tstbl %>% 
        filter (myPhase == "test_recognition") %>%
        group_by(filename, lang, mySegmentationCond) %>%
        summarize (N = n(),
                   correct_segm = mean (correct))
    
}
```
```{r recall-extract-recall-items}
if (ANALYZED.DATA.SETS["TESTABLE"]){
    if (RESEGMENT.RESPONSES){
        dat.all.recall.items.tstbl <- dat.recall.tstbl %>% 
            filter (myPhase == "test_recall") %>%
            filter (responseType == "comment") %>%
            dplyr::select(filename, subjectGroup, age, sex, Native.language.s., 
                          mySegmentationCond, lang, response) %>% 
            mutate (response = gsub (";1$", "", response)) %>%
            mutate (response = gsub (";timeout$", "", response)) %>%
            mutate (response = sub ("^\\s+", "", response)) %>%
            mutate (response = gsub ("\\s+$", "", response)) %>% 
            mutate (response = gsub ("\\\\n", ",", response)) %>%
            mutate (response = gsub (", ", ",", response)) %>%
            mutate (response = gsub (",,", ",", response)) %>%
            mutate (response = gsub (",$", "", response)) %>%
            mutate (response = gsub ("\\d\\.\\s*", "", response)) %>%
            mutate (response = tolower(response)) %>%
            arrange (desc(mySegmentationCond))
        
        # Add recognition performance
        dat.all.recall.items.tstbl <- left_join(
            dat.all.recall.items.tstbl,
            dat.recall.tstbl.recognition.m %>% 
                ungroup %>% 
                select (filename, correct_segm),
            by = "filename")
        
        
        # save.data.frame(all.recall.items,
        #                 row.names = FALSE)    
        
        # dat <- read.delim ("all.recall.items.txt", 
        #                    sep = "\t", 
        #                    stringsAsFactors = FALSE)
        
        dat.all.recall.items.tstbl <- dat.all.recall.items.tstbl %>% 
            # Filter participants for whom the vocalization 
            # cannot be analyzed (computer ran for several 
            # days), but whose production (dalonigtbdophophi 
            # dalobdakabdarobigopachu) did not ressemble 
            # the words anyhow
            filter (filename != "399612_200413_124119_M056106.csv")
        
    }
}
```

```{r recall-find-closest-matches-to-recall-items}

if (ANALYZED.DATA.SETS["TESTABLE"]){
    if (RESEGMENT.RESPONSES){        
        i <- 1
        dat.all.recall.items.tstbl.with.candidates <- dat.all.recall.items.tstbl %>% 
            slice (i) %>% 
            #group_by_all (.drop = FALSE) %>%
            group_by_all %>%
            do (process.utterance(.$response,
                                  .$lang,
                                  word.sequences,
                                  syllables)) 
        
        for (i in 2:nrow(dat.all.recall.items.tstbl)){
            print (sprintf ("Processing row %d of %d", i, nrow(dat.all.recall.items.tstbl)))
            
            dat.all.recall.items.tstbl.with.candidates <- dat.all.recall.items.tstbl.with.candidates %>% 
                bind_rows(dat.all.recall.items.tstbl %>% 
                              slice (i) %>% 
                              #                          group_by_all (.drop = FALSE) %>%
                              group_by_all%>%
                              do (process.utterance(.$response,
                                                    .$lang,
                                                    word.sequences,
                                                    syllables)))
        }
        
        
        # candidates <- process.utterance(
        #     utterance = dat.all.recall.items.tstbl$response[38],
        #     lang = dat.all.recall.items.tstbl$lang[38],
        #     word.sequences)
        
        dat.all.recall.items.tstbl.with.candidates  <- 
            dat.all.recall.items.tstbl.with.candidates %>%
            as.data.frame %>%
            filter ((closest.match.length %% 2) == 0) %>% 
            setNames(gsub ("closest.match$", "closest_match_just_match", names (.))) %>% 
            mutate (closest_match =
                        pmap_chr (., add.other.syllables.to.match))
        
        
        save.data.frame(dat.all.recall.items.tstbl.with.candidates,
                        row.names = FALSE)
        
    } else {
        
        dat.all.recall.items.tstbl.with.candidates <- read.delim("dat.all.recall.items.tstbl.with.candidates.txt", sep = "\t", header = TRUE, stringsAsFactors = FALSE)
    }
}
```

Each recall response was analyzed in five steps. First, we applied pre-segmentation substitution rules to make the transcriptions more consistent (see Table \ref{tab:substitution_rules}). For example, *ea* (presumably as in *tea*) was replaced with *i*. 

Second, responses were segmented into their underlying units. If a response contained a semicolon (;) or comma character (,), these were used to delineate units. For each of the resulting units, we verified if they contained additional spaces. If they did, these spaces were removed if further subdividing resulted in any single-syllable response (operationalized as a string with a single vowel); otherwise, the units were further sub-divided based on the spaces. The rationale for this algorithm is that responses such as *bee coo tee,two da ra,bout too pa* were like to reflect the words *bikuti*, *tudaro* and *budopa*.

Finally, if the response did not contain any commata or semicolons, it was segmented based on its spaces (if any).

Third, we removed geminate consonants and applied another set of substitution rules that to take into account possible misperceptions (see \ref{tab:substitution_rules})). For example, we treated the voiced and unvoiced variety of stop consonants as interchangeable. Specifically, for each "surface" form produced by the participants, we generated candidate "underlying" forms by recursively applying all substitutions rules and keeping track of the number of substitution rules that were applied to derive an underlying form from a surface form. For each unique candidate underlying form, we kept the shortest derivation. 

Fourth, for each candidate underlying form, we identified the longest matching string in the familiarization stream. The algorithm first verified if a form was contained in a speech stream starting with an *A*, *B* or *C* syllable; if the underlying form contained unattested syllable, one syllable change was allowed with respect to the speech streams. If no matches were found, two substrings were created by clipping the first or the last syllable from the underlying form, and the search was repeated recursively for each of these substrings until a match was found. We then selected the longest match for all substrings. 

Fifth, for each surface form, we selected the underlying form using the criteria (in this order) that, among the candidate underlying form of each surface form, the selected underlying form had (i)  had the maximal number of attested syllables, (ii) the maximal length, and (iii) the shortest derivation. 

## Change columns to categorize transcriptions
```{r recall-categorize-transcriptions-define, include = FALSE}
#categorize.matches %<a-% {
categorize.matches <- function (dat = ., current.lang) {
    dat %>% 
        mutate (n_syllables = count.sylls (closest_match)) %>%
        mutate (is_word = is.item.type (closest_match, 
                                        words.fw[[current.lang]])) %>%
        mutate (is_multiple_words = is.concatenation.of.item.type (closest_match,
                                                                   words.fw[[current.lang]])) %>%
        mutate (is_single_or_multiple_words = is_word | is_multiple_words) %>% 
        mutate (is_part_word = is.item.type (closest_match, 
                                             part.words.fw[[current.lang]])) %>%
        mutate (is_multiple_part_words = is.concatenation.of.item.type (closest_match,
                                                                        part.words.fw[[current.lang]])) %>%
        mutate (is_single_or_multiple_part_words = is_part_word | is_multiple_part_words) %>%
        mutate (is_class_word = is.item.type(closest_match,
                                             class.words.fw[[current.lang]])) %>%
        mutate (is_high_tp_chunk = is.chunk.from.item.type (closest_match,
                                                            words.fw[[current.lang]])) %>%
        mutate (is_low_tp_chunk = is.chunk.from.item.type (closest_match,
                                                           low.tp.chunk.fw[[current.lang]])) %>%
        mutate (has_correct_initial_syllable = has.correct.initial.syll (closest_match,
                                                                         words.fw[[current.lang]])) %>%
        mutate (has_correct_final_syllable = has.correct.final.syll (closest_match,
                                                                     words.fw[[current.lang]])) %>%
        mutate (is_part_of_stream = ifelse (n_syllables == 1,
                                            closest_match %in% 
                                                get.substrings.of.length(words.fw[[current.lang]], 2) %>% 
                                                paste,
                                            is_word |
                                                is_multiple_words |
                                                is_part_word |
                                                is_multiple_part_words |
                                                is_high_tp_chunk | 
                                                is_low_tp_chunk)) %>%
        mutate (is_part_of_stream = ifelse (is.na (is_part_of_stream),
                                            FALSE,
                                            is_part_of_stream)) %>% 
        mutate (is_part_of_stream = as.logical(is_part_of_stream)) %>%
        mutate (is_bw_word = is.item.type (closest_match, 
                                           words.bw[[current.lang]])) %>%
        mutate (is_multiple_bw_words = is.concatenation.of.item.type (closest_match,
                                                                      words.bw[[current.lang]])) %>%
        mutate (is_single_or_multiple_bw_words = is_bw_word | is_multiple_bw_words) %>%
        mutate (is_bw_part_word = is.item.type (closest_match, 
                                                part.words.bw[[current.lang]])) %>%
        mutate (is_multiple_bw_part_words = is.concatenation.of.item.type (closest_match,
                                                                           part.words.bw[[current.lang]])) %>%
        mutate (is_single_or_multiple_bw_part_words = is_bw_part_word | is_multiple_bw_part_words) %>%
        mutate (is_high_tp_bw_chunk = is.chunk.from.item.type (closest_match,
                                                               words.bw[[current.lang]])) %>%
        mutate (is_low_tp_bw_chunk = is.chunk.from.item.type (closest_match,
                                                              low.tp.chunk.bw[[current.lang]])) %>%
        mutate (average_fw_tp = calculate.average.tps.from.chunks (closest_match,
                                                                   list (list (chunks = words.fw[[current.lang]],
                                                                               tp = 1),
                                                                         list (chunks = low.tp.chunk.fw[[current.lang]],
                                                                               tp = 1/3)),
                                                                   chunk.length = 4)) %>% 
        mutate (expected_fw_tp = calculate.expected.tps.for.chunks (closest_match, words.fw[[current.lang]])) %>%
        mutate (average_bw_tp = calculate.average.tps.from.chunks (reverse.items (closest_match),
                                                                   list (list (chunks = words.bw[[current.lang]],
                                                                               tp = 1),
                                                                         list (chunks = low.tp.chunk.bw[[current.lang]],
                                                                               tp = 1/3)),
                                                                   chunk.length = 4)) 
    
}
```


```{r recall-categorize-transcriptions-do, include = FALSE}

if (ANALYZED.DATA.SETS["CITY"]){
    dat.recall.city <- lapply (c(L1 = "L1", L2 = "L2"),
                               function (current.lang) {
                                   dat.recall.city %>% 
                                       filter (lang == current.lang) %>%
                                       categorize.matches (current.lang)
                               }) %>% 
        do.call (rbind, .) %>% 
        arrange(subjNum, subjInitials, streamType)
}

if (ANALYZED.DATA.SETS["TESTABLE"]){
    dat.all.recall.items.tstbl.with.candidates <- lapply (c(L1 = "L1", L2 = "L2"),
                                                          function (current.lang) {
                                                              dat.all.recall.items.tstbl.with.candidates %>%
                                                                  filter (lang == current.lang) %>%
                                                                  categorize.matches (current.lang)
                                                          }) %>% 
        do.call (rbind, .) %>% 
        arrange(filename, mySegmentationCond)
    
}
```

We then computed various properties for each underlying form, given the "target" language the participant had been exposed to. Specifically, we calculated (1) the number of syllables, (2) whether it was a word from the target language, (3) whether it was a concatentation of words from the target language, (4) whether it was a single word or a concatenation of words from the target language (i.e., the disjunction of (2) and (3)), (5) whether it was a part-words from the target language, (6) whether it was a *complete* concatenation of part-words from the target language (i.e., the number of syllables of the item had to be a multiple of three, without any unattested syllables), (7) whether it was a single part-word or a concatenation of part-words from the target language, (8) whether it was a "class-word" with the two initial syllables coming from one word and the final syllables from another word or vice versa (i.e., class-words had the form *A\_iB\_iC\_j* or *A\_iB\_jC\_j*, (9) whether it was high-TP chunk (i.e., a word or a word with the first or the last syllable missing, after removing any leading or trailing unattested syllables), (10) whether it was a low-TP chunk (i.e., a chunk of the form *C\_iA\_j*, after removing lead or trailing unattested syllables, (11) whether it had a "correct" initial syllable, (12) whether it had a "correct" final syllable, (13) whether it is part of the speech stream (i.e., the disjunction of being an attested syllable, being a word or a concatenation thereof, being a part-word or a concatenation thereof, being a high-TP chunk or a low-TP chunk), (14) whether it was a backward word from the target language (i.e., a word with the syllable order reversed), (15) whether it was a concatenation of backward words, (16) whether it was a backward word or a concatenation thereof (i.e., the disjunction of (14) and (15)), (17) whether it was a backward part-word, (18) whether it was a concatenation of backward part-words, (19) whether it was a backward word or a concatenation thereof (i.e., the disjunction of (17) and (18)), (19) whether it was high-*backward*-TP chunk (i.e., a backward word or a backward word with the first or the last syllable missing, after removing any leading or trailing unattested syllables), (20) whether it was a low-*backward*-TP chunk (i.e., a chunk of the form *A\_iC\_j*, after removing lead or trailing unattested syllables, (21) the average forward TP of the transitions in the form, (22) the *expected* forward TP of the form if form is attested in the speech stream (see below for the calculation), and (23) the average backward TP of the transitions in the form.

### Expected TPs 
For items that are *correctly* reproduced from the speech stream, the expected TPs depend on the starting position. For example, the expected TPs for items of at least 2 syllables starting on an initial syllable are c(1, 1, 1/3, 1, 1, 1/3, 1, 1, 1/3, ...); if the item starts on a word-medial syllable, these TPs are c(1, 1/3, 1, 1, 1/3, 1, 1, 1/3, 1, ...).

In contrast, the expected TPs for a random concatenation of syllables are the TPs in a random bigram. For an *A* or a *B* syllable, the random TP is 1 $\times$ 1 / 12, as there is only 1 (out of 12) non-zero TP continuations. For a C syllable, the random TP is 3 $\times$ 1/3 / 12, as there are 3 possible concatenations. On average, the random TP is thus $(1/12 + 1/12 + 1/12)/ 3 = 1/12 \approx .083$. 


```{r recall-print-number-of-unattested-items}

dat.recall.unattested.m <- list ()

if (ANALYZED.DATA.SETS["CITY"]){
    dat.recall.city <- dat.recall.city %>%
        filter (is_part_of_stream)
    
 #   XXXUPDATE
}

if (ANALYZED.DATA.SETS["TESTABLE"]){
    dat.recall.unattested.m.city <- dat.all.recall.items.tstbl.with.candidates %>%
        group_by (filename, mySegmentationCond) %>% 
        summarize (N.total = n (), 
                   N.unattested = sum (!is_part_of_stream)) %>% 
        group_by (mySegmentationCond) %>% 
        summarize (N.total.M = mean (N.total),
                   N.total.min = min (N.total),
                   N.total.max = max (N.total),
                   N.unattested.M = mean (N.unattested),
                   N.unattested.min = min (N.unattested),
                   N.unattested.max = max (N.unattested)) %>% 
        add_column (
            data.set = "testable", 
            .before = 1)
    
    dat.recall.unattested.m <- c (dat.recall.unattested.m,
                                  list (dat.recall.unattested.m.city))
}

bind_rows(dat.recall.unattested.m) %>% 
    kable(caption="\\label{tab:unattested_items} Number of unattested items") %>%
kable_styling(latex_options =
                              c("hold_position", 
                                "scale_down"))

```

As shown in Table \ref{tab:unattested_items}, there was a considerable number of recall responses containing unattested syllables. However, it is unclear if these unattested syllables reflect misperceptions not caught by our substitution rules, typos, memory failures or creative responses. This makes it difficult to analyze these responses. For example, the TPs from and to an unattested syllable are zero. However, if the unattested syllable reflects a misperception or a typo, the true TP would be positive, and our estimates would underestimate the participant's statistical learning ability. 

We thus decided to restrict ourselves to responses that can be clearly interpreted and removed all items containing unattested syllables.


```{r recall-filter-unattested-items, include = FALSE}
if (FILTER.UNATTESTED.ITEMS){
    
    if (ANALYZED.DATA.SETS["CITY"]){
        dat.recall.city <- dat.recall.city %>%
            filter (is_part_of_stream)
    }
    
    if (ANALYZED.DATA.SETS["TESTABLE"]){
        dat.all.recall.items.tstbl.with.candidates <- dat.all.recall.items.tstbl.with.candidates %>%
            filter (is_part_of_stream)
    }
}
```

We also decided to remove single syllable responses, as it is not clear if participants volunteered such responses because they thought that individual syllables reflected the underlying units in the speech streams or because they misunderstood what they were ask to do.

```{r recall-filter-single-syllable-responses, include = FALSE}
if (FILTER.UNATTESTED.ITEMS){
    
    if (ANALYZED.DATA.SETS["CITY"]){
        dat.recall.city <- dat.recall.city %>%
            filter (n_syllables > 1)
    }
    
    if (ANALYZED.DATA.SETS["TESTABLE"]){
        dat.all.recall.items.tstbl.with.candidates <- dat.all.recall.items.tstbl.with.candidates %>%
            filter (n_syllables > 1)
    }
}
```



```{r recall-check-that-all-participants-have-both-stream-types}

if (ANALYZED.DATA.SETS["CITY"]){
    subj.with.one.streamType.city <- dat.recall.city %>%
        distinct(subj, streamType) %>%
        #    xtabs(formula = ~ subj + streamType) %>%
        xtabs(formula = ~ subj ) %>%
        as.data.frame() %>% 
        filter (Freq != 2)
    
    if (nrow(subj.with.one.streamType))
        stop ("Some participants have productions in only one stream type, exiting.")
}

# Testable subjects have just one stream anyhow
```


## Demographics and missing subjects
```{r recall-final-demographics-calculate}

if (ANALYZED.DATA.SETS["CITY"]){
    recall.demographics.city <- dat.recall.city %>% 
        filter (streamType == "continuous") %>%
        distinct (subj, Gender, Age, lang) %>%
        mutate (Gender = tolower (Gender)) %>%
        mutate (Gender = ifelse (startsWith(Gender, "f"), "female", "male")) %>%
        group_by(lang) %>%
        summarize (N = n(), 
                   females = sum (gdata::startsWith(Gender, "f", ignore.case = TRUE)),
                   males = sum (gdata::startsWith(Gender, "m", ignore.case = TRUE)),
                   age.m = round (mean (Age), 1),
                   age.range = paste (range(Age), collapse = "-"))
    
} 

if (ANALYZED.DATA.SETS["TESTABLE"]){
    recall.demographics.tstbl <- 
    dat.all.recall.items.tstbl.with.candidates %>%
        distinct (filename, sex, age,
                  mySegmentationCond, lang) %>% 
        rename(subj = filename, Gender = sex, Age = age,
               streamType = mySegmentationCond) %>% 
        group_by(streamType, lang) %>% 
        summarize (N = n(),
                   Females = sum (Gender == "female"),
                   Males = sum (Gender == "male"),
                                      Age.m = round (mean (Age), 1),
                   Age.range = paste (range(Age), collapse = "-")) 

}

```

The final demographic information is given in Table \ref{tab:recall_demographics}. 

```{r recall-final-demographics-print}
if (ANALYZED.DATA.SETS["CITY"]){
    recall.demographics.city %>%
        setNames (replace_column_labels(names(.))) %>%
        knitr::kable(caption = '\\label{tab:recall_demographics}Demographics of the final sample (City).')
    
        XXXCOMBINE
} 

if (ANALYZED.DATA.SETS["TESTABLE"]){
    recall.demographics.tstbl  %>% 
kable(caption="\\label{tab:demographics_tstbl} Demographics of the final sample (testable).") %>%
kable_styling(latex_options =
                              c("scale_down"))        
}

```

## Save categorized data 
```{r recall-save-data, include = FALSE}
# save.data.frame(dat, row.names = FALSE)

if (ANALYZED.DATA.SETS["CITY"]){
    xlsx::write.xlsx (dat.recall.city,
                      file="recall.city.populated.xlsx",
                      row.names=FALSE,
                      sheetName="Sheet1",
                      append=FALSE)
} 

if (ANALYZED.DATA.SETS["TESTABLE"]){
    xlsx::write.xlsx (dat.all.recall.items.tstbl.with.candidates,
                      file="recall.tstbl.populated.xlsx",
                      row.names=FALSE,
                      sheetName="Sheet1",
                      append=FALSE)
}

```

# Analyze data

In the analyses below, we removed all items that contained syllables not attested in the speech stream as it is unclear how these items should be analyzed. As a result, we also removed participants who did not produce any items that contained attested syllables only. 

## Analysis plan
Note to self: some analyses below rely on within-participant averages [A], within-participant sums [S] or other measures [O]. 

* [O] Performance in the two alternative forced-choice test. To be compared across segmentation conditions.
* [S] Number of items produced. To be compared across segmentation conditions, and against zero. 
* [A] Average length of items produced. To be compared across segmentation conditions.
* [S,A] Number and proportion (among productions) of words (and concatenations thereof)
* [S,A] Number and proportion (among productions) of part-words (and concatenations thereof)
* [A] Average forward TP in items
    - Compare across segmentation conditions
    - Compare to expected TPs for correctly reproduced items. The expected TPs for items of at least 2 syllables starting on an initial syllable are c(1, 1/3, 1, 1, 1/3, 1, 1, 1/3, ...). The difference between the actual and the expected TP needs to be compared to zero, as the expected TP differs across items.
    - Compare to expected TPs for a random string. The expected TPs for a random concatenation are the TPs in a random bigram. For an A or a B syllable, the random TP is 1 $\times$ 1 / 12, as there is only 1 (out of 12) non-zero TP continuations. For a C syllable, the random TP is 3 $\times$ 1/3 / 12, as there are 3 possible concatenations. On average, the random TP is thus $(1/12 + 1/12 + 1/12)/ 3 = 1/12 \approx .083$. 
* [A] Average backward TP in items
* [A] Proportion of items with syllables in correct postions
    a. Items with correct initial syllables
    b. Items with correct final syllables
    c. Add disjunction? 
    d. Ignore is_class_word based analyses
* [O] Proportion of
    a. Words among Words and Part-Words (or multiples thereof); this can be compared to the two-alternative forced choice test
    b. high-TP chunks among high and low-TP chunks

## Make overall averages

```{r recall-averages-across-items-calculate, include = FALSE}

if (ANALYZED.DATA.SETS["CITY"]){
    dat.recall.city.m <- dat.recall.city %>% 
        group_by(subj, subjNum, subjInitials, Age, Gender, streamType, correct_segm) %>%
        summarize_at (vars(n_syllables:average_bw_tp), mean, na.rm = TRUE)
    
    dat.recall.city.s <- dat.recall.city %>% 
        group_by(subj, subjNum, subjInitials, Age, Gender, streamType, correct_segm) %>%
        summarize_at (vars(n_syllables:average_bw_tp), sum, na.rm = TRUE)
    #summarize_at (vars(n_syllables:average_bw_tp), funs(n(), sum))
    
    # These are counts that are significantly above zero
    dat.recall.city.s.selected.vars.by.wilcox.df <- 
        dat.recall.city.s %>% 
        group_by(streamType) %>%
        summarize_at (vars(starts_with("is_"),starts_with("has_")), function (X) wilcox.test (X, alternative = "greater")$p.value) %>% 
        remove_rownames %>% 
        column_to_rownames("streamType") %>% 
        t %>% 
        as.data.frame (row.names = row.names(.)) %>% 
        rownames_to_column("var") %>% 
        mutate (use = (continuous <= .05) | (segmented <= .05)) 
    
    dat.recall.city.s.selected.vars.by.wilcox <- dat.recall.city.s.selected.vars.by.wilcox.df %>% 
        filter (use) %>% 
        pull ("var")
    
    #XXXUPDATE
} 

if (ANALYZED.DATA.SETS["TESTABLE"]){
    dat.recall.tstbl.m <- 
        dat.all.recall.items.tstbl.with.candidates %>% 
        mutate_at (vars(starts_with("is_"),starts_with("has_")), as.logical) %>% 
        # For compatibility with the city data set
        rename (subj = filename,
                Age = age,
                Gender = sex,
                streamType = mySegmentationCond,
                correct_segm = correct) %>% 
        # We have the following column in the city data set
        mutate (subjNum = NA, .after = "subj") %>% 
        mutate (subjInitials = NA, .after = "subjNum") %>%
        group_by(subj, Age, Gender, lang, streamType, correct_segm) %>%
#        summarize_at (vars(n_syllables:average_bw_tp), mean, na.rm = TRUE)
        summarize (
            n.items = n(),
            n.syll = mean (n_syllables),
            
            # Number and proportion (among all responses) of words
            n.words = sum (is_word),
            p.words = mean (is_word),
            n.words.or.multiple = sum (is_word | is_multiple_words),
            p.words.or.multiple = mean (is_word | is_multiple_words),

            # Number and proportion (among all responses) of part-words
            n.part.words = sum (is_part_word),
            p.part.words = mean (is_part_word),
            n.part.words.or.multiple = sum (is_part_word | is_multiple_part_words),
            p.part.words.or.multiple = mean (is_part_word | is_multiple_part_words),
            
            # Proportion of Words among Words and Part-Words (or multiples thereof)
            p.words.part.words = sum (is_word) / sum (is_word | is_part_word),
            p.words.part.words.or.multiple = sum (is_word | is_multiple_words) / 
                sum (is_word | is_multiple_words | is_part_word | is_multiple_part_words),

            # Number and proportion (among all responses) of high and low TP chunk
            n.high.tp.chunk = sum (is_high_tp_chunk),
            p.high.tp.chunk = mean (is_high_tp_chunk),
            
            n.low.tp.chunk = sum (is_low_tp_chunk),
            p.low.tp.chunk = mean (is_low_tp_chunk),
                        
            # Proportion of high-TP chunks among high and low-TP chunks
            p.high.tp.chunk.low.tp.chunk = sum (is_high_tp_chunk) / 
                sum (is_high_tp_chunk | is_low_tp_chunk),

            # Average forward TPs and difference from expected TP
            average_fw_tp = mean (average_fw_tp, na.rm = TRUE),
            average_fw_tp_d_actual_expected = mean (average_fw_tp - expected_fw_tp, na.rm = TRUE),
            
            average_bw_tp = mean (average_bw_tp, na.rm = TRUE),
            
            # Proportion of items with syllables in correct postions
            p.correct.initial.syll = mean (has_correct_initial_syllable),
            p.correct.final.syll = mean (has_correct_final_syllable),
            p.correct.initial.or.final.syll = mean (has_correct_initial_syllable | has_correct_final_syllable)
        )
            
    # These are counts that are significantly above zero
    dat.recall.tstbl.m.selected.vars.by.wilcox.df <- 
        dat.recall.tstbl.m %>% 
        group_by(streamType) %>%
        summarize_at (vars(starts_with("n.")), function (X) wilcox.test (X, alternative = "greater")$p.value) %>% 
        remove_rownames %>% 
        column_to_rownames("streamType") %>% 
        t %>% 
        as.data.frame (row.names = row.names(.)) %>% 
        rownames_to_column("var") %>% 
        mutate (use = (continuous <= .05) | (segmented <= .05)) 
    
    dat.recall.tstbl.m.selected.vars.by.wilcox <- dat.recall.tstbl.m.selected.vars.by.wilcox.df %>% 
        filter (use) %>% 
        pull ("var")    
    
}

```


```{r recall-combine-averages-across-data-sets}
dat.recall.combined.m <- list ()
dat.recall.combined.m.selected.vars.by.wilcox <- list ()

if (ANALYZED.DATA.SETS["CITY"]){
    dat.recall.combined.m <- c(dat.recall.combined.m,
                               list (dat.recall.city.m %>% 
                                         add_column (
                                             data.set = "city", 
                                             .before = 1)))    
    
    dat.recall.combined.m.selected.vars.by.wilcox <- c(
        list (city = dat.recall.city.s.selected.vars.by.wilcox)
    )
}

if (ANALYZED.DATA.SETS["TESTABLE"]){
    dat.recall.combined.m <- c(dat.recall.combined.m,
                               list (dat.recall.tstbl.m %>% 
                                         add_column(
                                             data.set = "testable",
                                             .before  = 1)))
    
    dat.recall.combined.m.selected.vars.by.wilcox <- c(
        list (testable = dat.recall.tstbl.m.selected.vars.by.wilcox))
    
}

dat.recall.combined.m <- bind_rows (dat.recall.combined.m)
```

After computing these counts and averages, we asked which counts were significantly different from zero in a one-tailed Wilcoxon test, either for the continuous or the segmented condition. These counts were `r toString (dat.recall.tstbl.m.selected.vars.by.wilcox)`. (Note: These counts are currently restricted to the testable data set.)


```{r recall-wilcox-across-stream-types-calculate}
dat.recall.combined.m.wilcox.by.streamType <-
    dat.recall.combined.m %>%
    group_by (data.set) %>%
    mutate (streamType = factor (streamType)) %>%
    summarize_at (vars(correct_segm:p.correct.initial.or.final.syll),

                  function (X) {
                      
                      # First check whether a variable has enough 
                      # data 
                      n.finite.in.x <- tapply (
                          X, 
                          get ("streamType", .), 
                          is.finite) %>% 
                          sapply (sum) 
                      
                      if (all (n.finite.in.x > 2)) {
                          wilcox.test(X ~ get ("streamType", .),
                                      .,
                                      paired = FALSE)$p.value
                      } else {
                          NA
                      }
                  })


```


```{r recall-averages-across-subjects-calculate, include = FALSE}
dat.recall.combined.m2 <- dat.recall.combined.m %>% 
    group_by(data.set, streamType) %>%
    summarize_at (vars(correct_segm:p.correct.initial.or.final.syll),
    mean, na.rm = TRUE)

```

As shown in Table \ref{tab:recall_all_averages}, participants produced on average `r dat.recall.combined.m[dat.recall.combined.m$streamType == "segmented",]$n.words` words in the segmented condition, and `r dat.recall.combined.m[dat.recall.combined.m$streamType == "continuous",]$n.words` in the continuous condition.

```{r recall-averages-print}
dat.recall.combined.m2 %>% 
    data.frame %>%
    rbind (., 
           cbind(streamType = "$p_{Wilcoxon}$", dat.recall.combined.m.wilcox.by.streamType)) %>% 
    remove_rownames %>% 
    column_to_rownames("streamType") %>%
    t %>% 
    #knitr::kable("latex", booktabs = T, caption = '\\label{tab:recall_all_averages}All averages. The *p* value has been calculated from a paired Wilcoxon test across the familiarization conditions.') %>%
    knitr::kable (caption = "\\label{tab:recall_all_averages}All averages. The *p* value has been calculated from a paired Wilcoxon test across the familiarization conditions.") %>%
    kable_styling(bootstrap_options = "striped")





```

# IN THE COUNTS, SELECT THOSE THAT ARE GREAT THAN ZERO ONLY

```{r recall-UPTOHERE}
knit_exit()
```


```{r recall-averages-plot, fig.cap="\\label{fig:recall_w_vs_pw}. Counts of words and part-words produced by the participants. The counts reflect only words and part-words, but not concatenations thereof."}

if (ANALYZED.DATA.SETS["CITY"]){
current.plot.name <- "recall_numbers"
prepare.graphics

dat.recall.city.s %>% 
    ungroup %>%
    data.table::setDT(.) %>% 
    data.table::dcast (subjNum + subjInitials ~ streamType, 
                       value.var = c("is_word", "is_part_word")) %>%
    data.table::setDF(.) %>%
    dplyr::select (c(is_word_continuous, is_part_word_continuous, is_word_segmented, is_part_word_segmented)) %>%
    strip4c(.,
            main="",
            ylim=c(-0.5,4),  pch=21, mean.pch=17, x=c(1, 2, 4, 5),
            offset = .5,
            ylab="Number of items recalled",
            xlab_big=c("Continuous", "Segmented"), xlab_big_at=c(1.5, 4.5), xlab_big_line=1,
            xlab_exp=rep(c("W", "PWs"), 2), xlab_exp_at=c(1:2, 4:5), xlab_exp_line=-.5,
            margins=c(3.5,6.5,2.5,2.5), write.percent=FALSE, forced.digits=2,
            ref.line = NULL)

show.graphics
}
```

## Word vs. part-word analysis
```{r recall-word-vs-pw-analysis-calculate, include = FALSE}
w.vs.pw <- dat.recall.city %>% 
    group_by(subjNum, subjInitials, streamType, correct_segm) %>%
    summarize_at (vars (is_single_or_multiple_words, is_single_or_multiple_part_words), sum, na.rm = TRUE) %>%
    mutate (p_word_vs_part_word = ifelse ((is_single_or_multiple_words == 0 ) &
                                              (is_single_or_multiple_part_words == 0),
                                          .5,
                                          is_single_or_multiple_words / (is_single_or_multiple_words + is_single_or_multiple_part_words)))

w.vs.pw.wide <- w.vs.pw %>%
    data.table::setDT(.) %>% 
    data.table::dcast (subjNum + subjInitials ~ streamType, 
                       value.var = c("correct_segm",
                                     "is_single_or_multiple_words",
                                     "is_single_or_multiple_part_words",
                                     "p_word_vs_part_word")) %>%
    data.table::setDF(.) %>% 
    mutate (d_segm = correct_segm_segmented - correct_segm_continuous) %>%
    mutate (d_p_word_vs_part_word = p_word_vs_part_word_segmented - p_word_vs_part_word_continuous) %>%
    mutate (d_segm_p_word = d_segm - d_p_word_vs_part_word)

w.vs.pw.long <- w.vs.pw %>% 
    gather (testType, 
            p.cor, 
            c(correct_segm, p_word_vs_part_word),
            factor_key = TRUE) 

# Long version of data frame for differences 
w.vs.pw.d.long <- w.vs.pw.wide %>% 
    gather (testType, 
            d, 
            c(d_segm, d_p_word_vs_part_word),
            factor_key = TRUE) 


```

```{r recall-word-vs-pw-analysis-print}
w.vs.pw.wide %>%
    dplyr::select(starts_with("p_word_vs_part_word")) %>%
    summarize_all (funs(n(), mean(., na.rm = TRUE))) %>%
    t %>%
    kable
```

```{r recall-word-vs-pw-analysis-plot, fig.cap="\\label{fig:recall_w_vs_pw}. Perentage of words among words and part-words. The percentage counted both words and part-words and concatenations thereof. The below-chance performance in the continuous condition is expected if participants start items on a random syllable, because they are twice as likely to produce an item starting with the second or the third syllable of a word than to start with a word-initial syllable."}
current.plot.name <- "recall_w_vs_pw"
prepare.graphics

w.vs.pw.wide %>%
    dplyr::select(c(starts_with("correct_segm"),
                    starts_with("p_word_vs_part_word"))) %>%
    mutate_all (function (X) 100 * X) %>%
    strip4c(.,
            main="Words vs. Part-Words",
            ylim=c(0,100),  pch=21, mean.pch=17, x=c(1, 2, 4, 5),
            offset = .4,
            ylab=TeX("$100 \\times \\frac{Words}{Words + Part-Words}$"),
            xlab_sma=rep(c("Cont.", "Segm."), 2), xlab_sma_at=c(1, 2, 4, 5), xlab_sma_line=.2,
            xlab_big=c("Recognition", "Recall"), xlab_big_at=c(1.5, 4.5), xlab_big_line=2,
            margins=c(4.5,6.5,2.5,2.5), write.percent=TRUE, forced.digits=2,
            ref.line = 50)

show.graphics
```

```{r recall-sw-acc-calculate, include = FALSE}
w.vs.pw.sw <- 
    w.vs.pw.long %>% 
    calculate.shapiro.wilk.test.for.cells(.,
                                          c("testType",
                                            "streamType"),
                                          "p.cor",
                                          .return.msg = FALSE)

```


```{r recall-sw-acc-print}

if (any (w.vs.pw.sw$p.value <= .05)) {
    w.vs.pw.sw %>%
        filter (p.value <= .05) %>%
        setNames(replace_column_labels(names (.))) %>%
        #dplyr::select (-c(locCond)) %>%
        arrange (-row_number()) %>%
        knitr::kable (caption = "\\label{tab:sw_acc}Cells across experiments where a violation of normality was detected by a Shapiro-Wilk test when performance was measured in terms of accuracy.")
}

```


```{r recall-sw-d-calculate, include = FALSE}
w.vs.pw.d.sw <- 
    w.vs.pw.d.long %>% 
    calculate.shapiro.wilk.test.for.cells(.,
                                          c("testType"),
                                          "d",
                                          .return.msg = FALSE)
```


```{r recall-sw-d-print}

if (any (w.vs.pw.d.sw$p.value <= .05)) {
    w.vs.pw.d.sw %>%
        filter (p.value <= .05) %>%
        setNames(replace_column_labels(names (.))) %>%
        #dplyr::select (-c(locCond)) %>%
        arrange (-row_number()) %>%
        knitr::kable (caption = "\\label{tab:sw_acc}Cells across experiments where a violation of normality was detected by a Shapiro-Wilk test when performance was measured in terms of accuracy.")
}

```


```{r recall-will-be-anova}
lapply (grep ("^d_", names (w.vs.pw.wide), value = TRUE),
        function (X){
            cbind (d = X,   
                   P = w.vs.pw.wide %>%
                       pull (X) %>% 
                       wilcox.p(.))
        }) %>%
    do.call (rbind, .) %>%
    kable (caption = "\\label{tab:wilcox_d}Wilcoxon tests for various differences. No of them is normally distributed.")

```

```{r recall-wmc.within.circle.12_20-anova-calculate}
# wmc.within.circle.12_20.aov <- dat.circle.combined.within.m %>%
#     filter (experimentID == "wmc.within.circle.12_20") %>%
#     mutate_at (c("subj", "piCond", "nItems"), factor) %>%
#     aov (cor ~ 
#              piCond * nItems + 
#              Error (subj/(piCond * nItems)),
#          data = .) 
# 
# wmc.within.circle.12_20.aov.ez <- dat.circle.combined.within.m %>%
#     filter (experimentID == "wmc.within.circle.12_20") %>%
#     ezANOVA(data = .,
#             dv = .(cor),
#             wid = .(subj),
#             within = .(piCond, nItems),
#             between = NULL,
#             detailed = TRUE,
#             type = 3)
# 
# wmc.within.circle.12_20.aov.ez.sphericity <- 
#     extract.sphericity.from.ezANOVA(wmc.within.circle.12_20.aov.ez)
# 
# ```
# 
# The results of overall ANOVA in Experiment 3 are given in the table below
# 
# ```{r recall-nItems-anova-show}
# 
# if (!is.null (wmc.within.circle.12_20.aov.ez.sphericity)){
#     warning ("There were sphericity corrections available that you ignored, do something about them.")    
# }
# 
# 
# wmc.within.circle.12_20.aov.ez$ANOVA %>%
#     mutate (Effect = replace_condition_labels(Effect)) %>%
#     setNames(replace_column_labels(names (.))) %>%
#     knitr::kable (caption = "\\label{tab:anova_acc_12_20_ez}ANOVA for Experiments 3")
# 
# report.aov (wmc.within.circle.12_20.aov,
#        .correct.for.repeated.tests = FALSE,
#                         .return.df = TRUE,
#        .print.results = FALSE) %>%
#     dplyr::select(-c("model.name")) %>%
#     mutate (effect = replace_condition_labels(effect)) %>%
#     setNames(replace_column_labels(names (.))) %>%
#     knitr::kable (caption = "\\label{tab:anova_acc_12_20}ANOVA for Experimen 3.")
# 

```

```{r recall-save-data-for-alexandra}
xlsx::write.xlsx (dat.recall.city,
                  file="data.alexandra.xlsx",
                  row.names=FALSE,
                  sheetName="complete",
                  append=FALSE)


dat.recall.city.m %>% 
    ungroup %>%
    data.table::setDT(.) %>% 
    data.table::dcast (subj  + subjNum + subjInitials + Age + Gender ~ streamType, 
                       value.var = names(.)[7:31]) %>%
    data.table::setDF(.) %>%
    xlsx::write.xlsx (.,
                      file="data.alexandra.xlsx",
                      row.names=FALSE,
                      sheetName="means",
                      append=TRUE)

dat.recall.city.s %>% 
    ungroup %>%
    data.table::setDT(.) %>% 
    data.table::dcast (subj  + subjNum + subjInitials + Age + Gender ~ streamType, 
                       value.var = names(.)[7:31]) %>%
    data.table::setDF(.) %>%
    xlsx::write.xlsx (.,
                      file="data.alexandra.xlsx",
                      row.names=FALSE,
                      sheetName="sums",
                      append=TRUE)



dat.recall.city.s %>% 
    ungroup %>%
    data.table::setDT(.) %>% 
    data.table::dcast (subj  + subjNum + subjInitials + Age + Gender ~ streamType, 
                       value.var = dat.recall.city.s.selected.vars.by.wilcox) %>%
    data.table::setDF(.) %>%
    xlsx::write.xlsx (.,
                      file="data.alexandra.xlsx",
                      row.names=FALSE,
                      sheetName="sums (>0)",
                      append=TRUE)

w.vs.pw.wide %>%
    xlsx::write.xlsx (.,
                      file="data.alexandra.xlsx",
                      row.names=FALSE,
                      sheetName="w_vs_pw",
                      append=TRUE)

```