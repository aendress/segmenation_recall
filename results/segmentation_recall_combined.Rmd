---
title: "The specificity of statistical learning"
author: "Ansgar Endress"
bibliography:
- /Users/endress/ansgar.bib
- /Users/endress/ansgar.own.bib
csl: /Users/endress/csl_files/pnas.csl
output:
  pdf_document:
    latex_engine: xelatex
    citation_package: natbib
    toc: true
    toc_depth: 5
    number_sections: true
    keep_tex: true
    fig_caption: true
    includes:
        in_header: /Users/endress/src/latex/ansgar.sty    
  html_document:
    theme: spacelab      
    number_sections: yes
    df_print: paged
    toc: yes
    toc_float: yes
    fig_caption: true
  html_notebook:
    theme: spacelab      
    number_sections: yes
    toc: yes
    toc_float: yes
    fig_caption: true
keywords: Keywords
abstract: "Statistical Learning exists in many domains and species. It might be particularly crucial for the earliest stages of word learning, for example for recovering word-like chunks from fluent speech. However, other forms of associative learning are remarkably tuned to the ecological constraints of the learning situation. Here, we show that Statistical Learning is similarly constrained. It predominantly operates in continuous speech sequences similar to those used in prior experiments, but not in discrete chunk sequences similar to those likely encountered during language acquisition (due to the prosodic organization of language). Conversely, when exposed to continuous sequence in a memory recall experiment, participants tend to produce low-probability sequences because, to the extent that they remember any items at all, they initiate their productions at random positions in the sequence rather than at the onsets of statistically defined chunks. In contrast, familiarization with discrete sequences produces reliable memories of actual, high-probability forms. This dissociation between Statistical Learning and memory suggests that Statistical Learning might have a specialized role when distributional information can be accumulated (e.g., for predictive processing), and that it is separable from the (declarative) memory mechanisms needed to acquire words."
---

```{r extract-R-file, eval = FALSE}
    
# Extract R file to accelarate segmentation

knitr::purl ('segmentation_recall_combined.Rmd', 
      'segmentation_recall_combined.R')
```

# House keeping

```{r recall-setup, echo = FALSE, include=FALSE}
rm (list=ls())

start.time <- Sys.time()

options (digits = 3)
knitr::opts_chunk$set(
    # Run the chunk
    eval = TRUE,
    # Don't include source code
    echo = FALSE, 
    # Print warnings to console rather than the output file
    warning = FALSE,  
    # Stop on errors
    error = FALSE,
    # Print message to console rather than the output file
    message = FALSE,
    # Include chunk output into output
    include = TRUE,
    # Don't reformat R code
    tidy = FALSE,
    # Center images
    fig.align = 'center',
    # Default image width
    out.width = '80%')

# other knits options are here:
# https://yihui.name/knitr/options/
```

```{r recall-set-parameters, echo = FALSE, include=FALSE}


PRINT.INDIVIDUAL.PDFS <- TRUE

# Set to FALSE unless you want to wait for several hours (6 for 100 subjects)
RESEGMENT.RESPONSES <- FALSE


# Columns to be ignored as they have different names 
# in different version of testable and are not used anyhow.
IGNORE.COL.PREFIXES <- c("ITI_", "presTime_", "ISI_")

PRINT.INDIVIDUAL.FIGURES <- FALSE
PRINT.INDIVIDUAL.TABLES <- FALSE

# Remove items that contain unattested syllables
# Set to FALSE as this would lead to problems with participants having no vocalization for one of the conditions.
FILTER.UNATTESTED.ITEMS <- FALSE
FILTER.SINGLE.SYLLABLES <- FALSE

#ALLOW.CONCATENTATIONS.FOR.W.VS.PW.ANALYSIS <- TRUE

# Remove participants performing < 50% correct
REMOVE.BAD.SUBJ <- TRUE
# Remove City participants having only one (segmented or continuous) condition
# Testable participants have only one
REMOVE.INCOMPLETE.SUBJ <- FALSE

ANALYZED.DATA.SETS <- c(CITY = TRUE,
                        TESTABLE = TRUE)

# Filter participants for whom the vocalization cannot be analyzed (computer 
# ran for several days), but whose production did not ressemble the words anyhow
L.BAD.SUBJ.CPUTIME <- list (
    tstbl = list (
        c (subj = "399612_200413_124119_M056106.csv",
           response = "dalonigtbdophophi dalobdakabdarobigopachu"),
        
        c (subj = "399612_210517_101428_M070415.csv",
           response = "be cu di tu dara pe gala du dopa,be cu di pe gala,be cu di pe gala bu dopa ,be cu di bu dopa"),
        
        # not an english speaker either
        c (subj = "399612_210517_100654_M038010.csv",
           response = "takahsakakakaratatataikokokokotatakatakatakatakatakatakataka"),
        
        c (subj = " 399612_210517_101201_M048600.csv",
           response = "dabroobitalooki,bkuti2,golab"),
        
        c (subj = "399612_210524_062929_M059506.csv",
           response = "matikulatatitulapapitularimatitulaatitula"),
        
        c (subj = "399612_210524_115845_M067482.csv",
           response = "tu kalla ti palla tuti kulla papi pu tu kalla ti palla tuti kulla papi pu"),
        
        c (subj = "399612_210524_120014_M099076.csv",
           response = "tutopitulakatutopitoolaka"),
        
        c (subj = "399612_210524_120523_M003515.csv",
           response = "papikuchi,butalapapikuchi,kukala,pikala,budharapikuchi,chupapikachubudarapi")
        
        )
)


```

In the analyses below, we use the following parameters: 
```{r recall-list-parameters}
knitr::kable(
    do.call (rbind, 
             lapply (ls(),
                     function (X) 
                         data.frame(Name = X, Value = as.character(get (X)))
             )
    )
)
```

```{r recall-load-libraries, include = FALSE, message = TRUE, warning = TRUE}

# check this
#http://www.ats.ucla.edu/stat/r/dae/melogit.htm
#http://www.sagepub.com/upm-data/38503_Chapter6.pdf
# Read in a random collection of custom functions
# Read in a random collection of custom functions
if (Sys.info()[["user"]] %in% c("ansgar", "endress")){
    source ("/Users/endress/R.ansgar/ansgarlib/R/tt.R")
    source ("/Users/endress/R.ansgar/ansgarlib/R/null.R")
    #source ("helper_functions.R")
} else {
    # Note that these will probably not be the latest versions
    source("http://endress.org/progs/tt.R")
    source("http://endress.org/progs/null.R")
}

library ("knitr")
library (kableExtra)
library (stringr)
library (rlang)
```

```{r recall-helper-functions-string-manipulation, include = FALSE}

count.sylls <- function (items){
    
    sapply (items,
            function (X) nchar (X) /2 )
    
}

get.unique.str.length <- function (items){
    
    item.len <- unique (stringr::str_length(items))
    
    if (length (item.len) > 1)
        stop ("Inconsistent item length")
    
    return (item.len)
}


get.substrings.of.length <- function (items, substr.len = 2, allow.overlap = FALSE, overlap.offset = NULL, simplify = TRUE){
    
    if (allow.overlap) {
        if (is.null (overlap.offset)){
            substr.offset <- substr.len   
        } else {
            substr.offset <- overlap.offset
        }
    } else {
        substr.offset <- substr.len
    }
    
    sapply (items,
            function (X) {
                if (simplify){
                    substring (X,
                               seq(1, nchar(X)-substr.len + 1, substr.offset),
                               seq(substr.len, nchar(X), substr.offset))
                } else{
                    ifelse (nchar (X) < substr.len,
                            list (NULL),
                            list (substring (X,
                                             seq(1, nchar(X)-substr.len + 1, substr.offset),
                                             seq(substr.len, nchar(X), substr.offset)))) %>%
                        unlist
                }
            },
            simplify = simplify)
}

reverse.items <- function (items, syll.len = 2){
    
    items.as.vectors <- get.substrings.of.length(items,
                                                 substr.len = syll.len,
                                                 simplify = FALSE) 
    
    items.rev <- lapply (items.as.vectors,
                         function (X) paste (rev (X), collapse = "")) %>%
        unlist
    
    return (items.rev)
}

```
```{r recall-helper-functions-segmentation, include = FALSE}

new_candidate <- function (candidate, n.changes = 0, surface = candidate){
    
    if (is.null (candidate))
        return (NULL)
    
    structure (list(underlying = candidate,
                    surface = surface,
                    n.changes = n.changes,
                    closest.match = NA,
                    closest.match.length = NA,
                    closest.match.pos = NA,
                    stream = NA),
               class = "candidate")
}


replace_phoneme <- function (candidate.list = ., phoneme1, phoneme2){
    
    if (class (candidate.list) == "candidate")
        candidate.list <- list (candidate.list)
    
    
    if (class (candidate.list) != "list")
        stop ("candidate.list must be of type candidate or list.\n")
    
    new.candidate.list <- list ()
    for (candidate in candidate.list){
        
        new.candidate.list <- c (new.candidate.list,
                                 list (candidate))
        
        if (!grepl (phoneme1, candidate$underlying, ignore.case = TRUE))
            next
        
        for (ppos in stringi::stri_locate_all(candidate$underlying, 
                                              regex = phoneme1)[[1]][,"start"]){
            # Replace each of the matches.
            # Then recursively replace the other matches 
            new.candidate <- new_candidate (candidate$underlying, 
                                            candidate$n.changes + 1,
                                            candidate$surface)
            substr (new.candidate$underlying, 
                    ppos, ppos) <- phoneme2
            
            new.candidate.list <- c (new.candidate.list,
                                     replace_phoneme (new.candidate,
                                                      phoneme1, phoneme2))
            
        }
        
    }
    
    return (new.candidate.list)
}

remove.geminates <- function (items){
    
    # https://stackoverflow.com/questions/29438282/find-repeated-pattern-in-a-string-of-characters-using-r
    geminate.regexp <- "(\\S+?)\\1(\\S)"
    
    if (is.list (items)){
        new.items <- list ()
    } else {
        new.items <- c()
    }
    
    for (current.item in items){
        
        while (grepl (geminate.regexp, current.item, perl = TRUE)){
            current.item <- gsub (geminate.regexp, "\\1\\2", 
                                  current.item, perl = TRUE)
        }
        
        new.items <- c(new.items, current.item)
        
    }
    
    new.items
}

get.syllables.from.words <- function (words = ., sort.sylls = TRUE, remove.duplicates = TRUE){
    
    get.substrings.of.length (words, simplify = FALSE) %>%
        unlist %>% 
        unname %>% 
        {if (sort.sylls) sort (.) else .} %>% 
        {if (remove.duplicates) unique (.) else .}
}

find_syllable_match <- function (candidate.list = ., syllable.list){
    
    if (class (candidate.list) == "candidate")
        candidate.list <- list (candidate.list)
    
    new.candidate.list <- list ()
    
    for (candidate in candidate.list){
        closest.match <- candidate$underlying
        
        closest.match.sylls <- c(get.substrings.of.length(closest.match))
        
        closest.match.sylls[!(closest.match.sylls %in% syllable.list)] <- "XX"
        
        closest.match <- paste (closest.match.sylls, collapse="")
        
        candidate$closest.match <- closest.match
        
        new.candidate.list <- c (new.candidate.list,
                                 list (candidate))    
    }
    
    new.candidate.list    
    
}

segment.extra.spaces <- function (utterance){
    
    # If any of the segmented items contains just a single contigent vowel
    #   Remove the spaces and keep the resulting item
    # else
    #   Keep separate entries of each item enclosed by a space
    # end							
    
    
    if (!grepl ("\\s", utterance))
        return (utterance)
    
    # Temporarily split utterance
    utterance.split <- strsplit(utterance, "\\s+")[[1]]
    
    # Count the number of vowels    
    n.vowels <- sapply (utterance.split,
                        str_count, "[aeiou]+")
    
    if (any (n.vowels == 1)){
        # One of the items is a single syllable, remove the spaces
        return (list (gsub ("\\s+", "", utterance)))
    } else {
        return (utterance.split)   
    }
}

segment.utterance <- function (utterance){
    if (grepl ("[;,]", utterance)){
        
        # First pass segmentation based on characters
        utterance.split <- strsplit(utterance, "[;,]+")[[1]]
        
        if (grepl("\\s", utterance)){
            # Deal with additional spaces
            
            utterance.split <- lapply (utterance.split,
                                       segment.extra.spaces) %>%
                unlist
        }
        
    } else {
        
        utterance.split <- strsplit(utterance, "\\s+")
    }
    
    return (utterance.split)
}


get.non.repeating.word.sequences <- function (words, max_length, return.df = FALSE){
    
    if ((length (words) == 1) & is.numeric(words))
        words <- 1:words
    
    word.repetition.filter <-  paste (
        paste ("(Var", 1:(max_length-1), sep =""),
        paste ("Var", 2:max_length, ")", sep =""),
        sep = "!=",
        collapse = "&")
    
    word.seq <- expand.grid(lapply (1:max_length, 
                                    function (X) words)) %>%
        filter (!!!parse_exprs(word.repetition.filter)) 
    
    if (return.df)
        return (word.seq)
    
    word.seq <- word.seq %>%
        apply (1, paste, collapse = "")
    
    return (word.seq)
}

find.longest.match <- function (target, lang, word.sequences) {
    
    # Find a match in any of the word.sequences with the full length 
    # of the word
    #
    # If no match is found, generate all subsequence of the target 
    # with on syllable less and recursively call the function again
    # until a match is found (or return NULL)
    
    if (nchar (target) < 2 )
        return (NULL)
    
    # Needs to be multiplied by 2 as the matches are segment based
    # The maximal distance is 0, except if one of the segments is XX, in which
    # case it's ignored
    max.dist <- 2 * str_count (target, "XX")
    
    for (ws in 1:length(word.sequences)){
        
        # 1. Search from the word onset
        current.word.list <- substr(word.sequences[[ws]][[lang]], 1, 
                                    nchar (target)) %>% 
            unique
        current.dist <- stringdist::stringdist (target,
                                                current.word.list,
                                                "hamming")
        
        if (any (current.dist == max.dist)) {
            
            return (list (match = target,
                          length = nchar (target),
                          pos = 1,
                          stream = names(word.sequences)[ws]))
        }
        
        # 2. Search from the word offset
        current.word.list.word.length <- get.unique.str.length (
            word.sequences[[ws]][[lang]])
        current.word.list <- substr(word.sequences[[ws]][[lang]], 
                                    current.word.list.word.length - nchar (target) + 1,
                                    current.word.list.word.length) %>% 
            unique
        
        current.dist <- stringdist::stringdist (target,
                                                current.word.list,
                                                "hamming")
        
        if (any (current.dist == max.dist)) {
            
            return (list (match = target,
                          length = nchar (target),
                          pos = 1,
                          stream = names(word.sequences)[ws]))
        }
        
        
        
    }
    
    # We haven't found a match yet
    target.fragement.length <- nchar(target)-2
    target.fragments <- get.substrings.of.length (target, 
                                                  substr.len = target.fragement.length, 
                                                  allow.overlap = TRUE, 
                                                  overlap.offset = 2)
    
    fragment.match.list <- list()
    for (tf in 1:length(target.fragments)){
        
        current.match <- find.longest.match (target.fragments[tf],
                                             lang,
                                             word.sequences)
        
        if (!is.null (current.match)) {
            current.match$pos <- (tf - 1) + current.match$pos  
            
            if (current.match$length == target.fragement.length){
                return (current.match)
            } else {
                fragment.match.list <- c(fragment.match.list, 
                                         list (current.match))
            }
            #return (current.match)
        }
        
    }
    
    if (length (fragment.match.list) > 0){
        
        longest.match.ind <- which.max (
            map_dbl (fragment.match.list, "length")
        )
        
        return (fragment.match.list[[longest.match.ind]])
    }
    
    return (NULL)
}

find.longest.matches <- function (targets, lang, word.sequences){
    
    lapply (targets, 
            find.longest.match,
            lang,
            word.sequences)
    
}

find.unique.candidates <- function (candidates = .){
    
    underlying.derivation.length <- lapply (candidates,
                                            function (X) {
                                                cbind.data.frame (underlying = X$underlying,
                                                                  n.changes = X$n.changes) 
                                            }) %>% 
        do.call (rbind, .) %>%  
        mutate (underlying = as.character (underlying)) %>% 
        group_by (underlying) %>% 
        summarize (n.changes = min(n.changes)) %>%
        remove_rownames() %>% 
        column_to_rownames("underlying")
    
    keep <- sapply (candidates, 
                    function (X) {
                        X$n.changes == 
                            underlying.derivation.length[X$underlying,"n.changes"]    
                    }) 
    
    candidates  <- candidates[keep]
    candidates <- candidates[!(sapply (candidates, 
                                       function (X) X$underlying) %>%  
                                   duplicated)]


    candidates
}

select.candidates.by.surface.form <- function (candidates, lang, syllables){
    
    # Remove candidates for which no match has been found 
    candidates.df <- candidates %>% 
        do.call (rbind.data.frame, .) %>% 
        filter (!is.na(closest.match)) %>% 
        mutate (n.attested.sylls = ifelse (
            is.na (underlying), 
            0,
            sapply (as.character (underlying),
                    function (U){
                        get.syllables.from.words (U) %>%
                            is.item.type (syllables[[lang]]) %>%
                            unlist %>%
                            sum (na.rm = TRUE)
                    }
            ))) %>%
        # Calculate by surface form, and filter 
        # underlying forms in this anking 
        # (1) maximum number of attested sylls.
        # (2) maximum length
        # (3) Number of changes
        group_by (surface, .drop = FALSE) %>% 
        filter (n.attested.sylls == max (n.attested.sylls)) %>% 
        filter (closest.match.length == max (closest.match.length)) %>% 
        filter (n.changes == min (n.changes)) %>% 
        ungroup
    if (nrow (candidates.df) == 0){
        candidates.df[1,names(candidates.df)] <- rep (NA, 
                                                      ncol (candidates.df)) %>% 
            t
    }
    
    
    return (candidates.df)
}

add.other.syllables.to.match <- function (underlying, lang, closest_match_just_match, ...) {
    # Take the closest match and add the remaining 
    # syllables in the underlying form
    
    if (is.na (closest_match_just_match))
        return (closest_match_just_match)
    
    if (underlying == closest_match_just_match)
        return (closest_match_just_match)
    
    closest.match.pos <- stringi::stri_locate_first(
        underlying, 
        fixed = closest_match_just_match)
    
    # Split underlying form into syllables
    underlying.sylls <- get.syllables.from.words(
        underlying, 
        sort.sylls = FALSE,
        remove.duplicates = FALSE)
    
    # Replace unattested syllables with XX
    underlying.sylls[!is.item.type (
        underlying.sylls, 
        syllables[[lang]])] <- "XX"
    
    closest.match2 <- paste(underlying.sylls, 
                            collapse = "")
    # Sanity check
    if (closest_match_just_match != substr (closest.match2,
                                            closest.match.pos[1,"start"],
                                            closest.match.pos[1,"end"]))
        warning ("Closest match ", closest_match_just_match, " does not match the underlying form ", underlying)
    
    return (closest.match2)
    
}

process.utterance <- function (utterance, lang, word.sequences, syllables){
    
    # 1. Apply pre-segmentation substitutions 
    
    utterance <- apply.substitution.rules.pre.segmentation(utterance)
    
    # 2. Segment into candidates
    
    utterance.split <- segment.utterance (utterance)
    
    # 3. Apply post-segmentation substitutions and make items unique
    
    utterance.split <- lapply (utterance.split,
                               remove.geminates) %>% 
        unlist %>% 
        unique 
    
    candidates <- lapply (unlist (utterance.split),
                          # Create candidate data structures for all candidates 
                          new_candidate) %>% 
        # And apply substitutions
        apply.substitution.rules.post.segmentation %>%
        find.unique.candidates
    
    # 4. Find longest match
    
    for (cand in 1:length(candidates)){
        # Create matches between the underlying forms 
        # of the candidate and the syllables in a language
        
        current.match <- find.longest.matches(
            candidates[[cand]]$underlying, 
            lang, 
            word.sequences) %>% 
            unlist (recursive = FALSE)
        
        if (!is.null (current.match)){
            
            candidates[[cand]]$closest.match <- current.match$match
            candidates[[cand]]$closest.match.length <- current.match$length
            candidates[[cand]]$closest.match.pos <- current.match$pos
            candidates[[cand]]$stream <- current.match$stream
            
        } else {
            
            candidates[[cand]]$closest.match <- NA
            candidates[[cand]]$closest.match.length <- 0
            candidates[[cand]]$closest.match.pos <- -1
        }
        
    }
    
    # 5. select longest match for each surface form
    
    candidates <- select.candidates.by.surface.form (candidates, lang, syllables)
    
    return (candidates)
}

```

```{r recall-helper-functions-string-analysis, include = FALSE}
is.item.type <- function (items = ., 
                          all.items.for.type){
    
    sapply (items, 
            function (X) X %in% all.items.for.type)
}

is.concatenation.of.item.type <- function (items,
                                           all.items.for.type){
    
    item.length <- sapply (all.items.for.type, nchar) %>% 
        unique
    
    if (length (item.length) > 1)
        stop ("Items do not have a consistent length")
    
    # Pad items to minimum length where required
    # Changed July 28th, 2020
    items <- ifelse (is.na (items),
                     rep ("x", item.length),
                     items)
    items <- sapply (items,
                     function (current.item) {
                         if (nchar (current.item) < item.length) {
                             paste (current.item,
                                    rep ("x", 
                                         item.length - nchar (current.item)), 
                                    collapse="") 
                         } else {
                             current.item
                         }
                     })
    # End change
    
    is.concatenation <- lapply (items,
                                get.substrings.of.length, item.length) %>%
        lapply (is.item.type,
                all.items.for.type) %>%
        lapply (all) %>% 
        unlist
    
    # Exclude single items
    is.concatenation[nchar (items) <= item.length] <- FALSE
    
    return (is.concatenation)
}

is.chunk.from.item.type <- function (items, 
                                     all.items.for.type,
                                     min.length = 4){
    
    # July 28th, 2020
    # Strip leading and trailing unattested syllables
    items <- gsub ("^x+", "", 
                   items, ignore.case = TRUE)
    items <- gsub ("x+$", "", 
                   items, ignore.case = TRUE)
    
    is.chunk <- sapply (items, 
                        function (X) {
                            ifelse (nchar(X) < min.length,
                                    FALSE,
                                    grepl (X, all.items.for.type) %>% 
                                        any())
                        })
    
    is.chunk[is.na(items)] <- FALSE
    
    return (is.chunk)
}

has.correct.initial.syll<- function (items, 
                                     all.items.for.type){
    
    sapply (items, 
            function (X) {
                grepl (paste ("^",
                              substr(X, 1, 2),
                              sep =""), 
                       all.items.for.type) %>%
                    any()
                
            })
}

has.correct.final.syll <- function (items, 
                                    all.items.for.type){
    
    sapply (items, 
            function (X) {
                grepl (paste (substr(X, nchar(X)-1, nchar(X)),
                              "$",
                              sep =""), 
                       all.items.for.type) %>%
                    any()
                
            })
}


get.part.words <- function (words, parts.word1 = c(3), parts.word2 = c(1, 2), allow.repeats = FALSE){
    
    words.as.sylls <- get.substrings.of.length (words) %>%
        as.data.frame(stringsAsFactors = FALSE) %>%
        as.list
    
    part.words <- c()
    for (first.word.ind in 1:length(words)){
        
        second.word.inds <- 1:length(words)
        if (!allow.repeats)
            second.word.inds <- second.word.inds[-first.word.ind]    
        
        part.words.current <- lapply (second.word.inds,
                                      function (X) paste (
                                          paste (words.as.sylls[[first.word.ind]][parts.word1], 
                                                 collapse=""),
                                          paste (words.as.sylls[[X]][parts.word2],
                                                 collapse=""),
                                          sep = "")) %>%
            unlist 
        
        part.words <- c(part.words,
                        part.words.current)
    }
    return (part.words)
}

#' calculate.average.tps.from.chunks
#'
#' @description
#' Calculate the average TPs in a set of items based 
#'   on the TPs in its constituent chunks. 
#'
#' @param items A vector of items whose TPs should be calculated
#' @param item.type.list List of lists with element chunks and tp
#'   containing chunks with their TPs 
#' @param chunk.length Length of the chunks (in characters) whose 
#'   TPs should be calculated
#'
#' @return Vector of the average TPs in \code{items}
#'
#' @examples 
#' calculate.average.tps.from.chunks (items, list (list(chunks=chunkVector, tp=3)), 4)
#' 
#' @details 
#' For each item in \code{items}, the function generates all substrings
#' of length \code{chunk.length}. For each substring, it loops through 
#' \code{item.type.list}, checks whether the substring is contained in 
#' the chunk elements, records the corresponding TP and averages the TP
#' across chunks
calculate.average.tps.from.chunks <- function (items,
                                               item.type.list,
                                               chunk.length = 4){
    mean.tps.in.items <- c()
    for (current.item in items){
        
        if (is.na (current.item) |
            is.null (current.item) |
            (nchar (current.item) < chunk.length)) {
            mean.tps.in.items <- c(mean.tps.in.items,
                                   NA)
            next
        }
        
        current.chunk.list <- get.substrings.of.length(current.item, 
                                                       chunk.length, 
                                                       allow.overlap = TRUE, 
                                                       overlap.offset = chunk.length/2,
                                                       simplify = FALSE)
        
        # Loop through the chunks for the current item
        tps.in.current.chunks <- c()
        for (current.chunk in unlist (current.chunk.list)){
            
            current.tp <- 0
            for (current.item.type in item.type.list){
                
                if (any (grepl (current.chunk, current.item.type$chunks))){
                    
                    current.tp <- current.tp + current.item.type$tp    
                    
                }
            }
            
            tps.in.current.chunks <- c(tps.in.current.chunks, 
                                       current.tp)
        }
        mean.tps.in.items <- c(mean.tps.in.items,
                               mean (tps.in.current.chunks))
    }
    
    return (mean.tps.in.items)
}

#' calculate.expected.tps.for.chunks
#'
#' @description
#' Calculate the expected TPs in a set of items if the items
#'   correctly reproduce the speech stream
#'
#' @param items A vector of items whose TPs should be calculated
#' @param words Vector of words used to determine the expected TP
#' @param high.tp Within-word TPs (Default: 1)
#' @param low.tp Across-word TPs (Default: 1/3)
#' @param syll.len Length (in characters) of syllables (Default: 2)
#'
#' @return Vector of the expected TPs in \code{items}
#'
#' @details
#' The function first generates two lists:
#'   * A list of syllables that can occur in each position of a word. 
#'   * A list of vectors of TPs expected for each syllable. For example, 
#'     an item starting on a word-final syllable has the expected TPs
#'     \code{c(low.tp, high.tp, low.tp, ...)}
#' 
#' For each item in \code{items}, then determines the starting position 
#' (or picks a random position if none can be determined), retrieves the 
#' vector of TPs expected for this starting position, trims the vector 
#' to the length expected by the length of the item and returns the 
#' average expected TP.
calculate.expected.tps.for.chunks <- function (items, words, high.tp = 1, low.tp = 1/3, syll.len = 2) {
    
    # use words to detect whether we start with an A, B or C syllable
    word.len <- get.unique.str.length(words)
    
    # Generate list of the list of syllables that can 
    # occur in each position 
    potential.starting.sylls <- lapply (    
        seq (1, word.len, syll.len),
        function (X) substr (words, X, X + syll.len-1))
    
    # Calculate expected TPs
    max.item.length <- max (nchar (items)) / syll.len
    expected.tps <- list (
        # Starting with an A syllable
        rep (c(high.tp, high.tp, low.tp), ceil (max.item.length / 3)),
        # Starting with a B syllable
        rep (c(high.tp, low.tp, high.tp), ceil (max.item.length / 3)),
        # Starting with a C syllable
        rep (c(low.tp, high.tp, high.tp), ceil (max.item.length / 3)))
    
    lapply (items,
            function (X) {
                
                if ((nchar(X) / syll.len) < 2)
                    return (NA)
                
                # Find current starting position within a word 
                # or pick a random position if none can be determined
                current.starting.pos <- which (
                    lapply (potential.starting.sylls,
                            function (PSS) any (gdata::startsWith (X, PSS))) %>%
                        unlist)
                
                if (length (current.starting.pos) == 0){
                    warning (paste("No starting position could be determined for item ", 
                                   X, 
                                   ". Picking random position instead.", sep =""))
                    current.starting.pos <- sample (length (expected.tps), 1)
                }
                
                # Retrieve vector of expected TPs based on 
                # this starting position
                current.expected.tps <- expected.tps[[current.starting.pos]]
                
                # Trim the vector of expected TPs 
                current.expected.tps <- current.expected.tps[1:((nchar(X)/syll.len)-1)]
                
                # Average the TPs
                return (mean (current.expected.tps))
            }) %>%
        unlist
    
}
```


```{r recall-helper-functions-general, include = FALSE}

#' Wrapper for Wilcoxon
wt <- function (x = ., y = 0) {

    
    if (length (y) == 1){
        
        wtest <- wilcox.test (x, mu = y)
    
    } else {
        
        wtest <- wilcox.test (x, y)
    }
    
    paste0 ("$V$ = ", wtest$statistic, ", $p$ = ", signif (wtest$p.value, 3))

}



wilcox.p.2sample <- function (x, cond, paired = FALSE) {

    # First check whether a variable has enough data
    n.finite.in.x <-
        # Create a list of boolean vectors for each streamType
        tapply (
            x,
            cond,
            is.finite) %>%
        # Create a sum of these booleans
        sapply (sum)
    
    if (all (n.finite.in.x > 2)) {
        wilcox.test(x ~ cond,
                    data.frame (x=x, cond = cond),
                    na.action = na.omit,
                    paired = paired)$p.value
    } else {
        NA
    }
}



t.test.p <- function (x, mu = 0)
{
    x <- as.numeric (x)
    if ((all %.% is.na) (x)) {
        return (NA)
    } else {
        return (signif (
            t.test (x, mu = mu)$p.value,
            getOption('digits')))
    }
}

replace_column_labels <- function (X)
{    
    # Uses pryr
    # Evalution from right to left
    compose (
        function (X) {gsub ("flanker.rt.d.median.split", 
                            "Flanker Group", X)},
        function (X) {gsub ("scs.median.split", 
                            "Self Control Group", X)},
        function (X) {gsub ("countCond", "*Secondary Task*", X)},    
        function (X) {gsub ("experimentID", "Experiment", X)},
        function (X) {gsub ("poolSize", "*Pool Size*", X)},
        function (X) {gsub ("nItems", "*Set Size*", X)},
        function (X) {gsub ("yPosCond", "*Sequential Position*", X)},
        function (X) {gsub ("locCond", "*Location Condition*", X)},
        
        function (X) {gsub ("countCond", "*Secondary Task*", X)},
        function (X) {gsub ("piCond", "*PI Condition*", X)},
        function (X) {gsub ("partial.eta.squared", "$\\\\eta_p^2$", X)},
        
        function (X) {gsub ("p<=.05", "$p \\\\leq .05$", X)},
        function (X) {gsub ("p.value", "$p$", X)},
        function (X) {gsub ("F.value", "*F*", X)},
        function (X) {gsub ("Cohen.d", "Cohen's *d*", X)},
        function (X) {gsub ("^CI$", "*CI*", X)},
        function (X) {gsub ("^P$", "$p$", X)},
        function (X) {gsub ("^p$", "$p$", X)},
        function (X) {gsub ("^t$", "$t$", X)},
        function (X) {gsub ("p.t.test", "$p_{t\\\\ test}$", X)},
        function (X) {gsub ("p.wilcox.test", "$p_{Wilcoxon}$", X)},
        function (X) {gsub ("^SE.log$", "*SE* (log)", X)},
        function (X) {gsub ("^SE$", "*SE*", X)},
        function (X) {gsub ("^SD.log$", "*SD* (log)", X)},
        function (X) {gsub ("^SD$", "*SD*", X)},
        function (X) {gsub ("^M.log$", "*M* (log)", X)},
        function (X) {gsub ("^M$", "*M*", X)},
        function (X) {gsub ("^effect$", "Effect", X)},
        function (X) {gsub ("^model.name$", "Experiment", X)},
        function (X) {gsub ("^Chisq$", "$\\\\chi^2$", X)},
        function (X) {gsub ("Chi Df", "Df", X)},
        function (X) {gsub ("Pr\\(>Chisq\\)", "$p$", X)},
        function (X) {gsub ("IV.removed", "Removed IV", X)}
    ) (X)
}

```


```{r recognition-helper-functions-general, include = FALSE}
global.df.to.plot.df <- function (dat = ., 
                                  filter.string, 
                                  value.col = "correct",
                                  filter.col = "experimentID",
                                  condition.options = c("L1", "L2"),
                                  condition.col = "lang"){
    lapply (
        condition.options, 
        function (COND) {
            dat %>% 
                filter (.data[[filter.col]] == filter.string) %>% 
                filter (.data[[condition.col]] == COND) %>% 
                as.data.frame
        }) %>% 
        make.matrix.for.plot (.,
                              value.col,
                              df=T) %>% 
        setNames(condition.options)
}



analyze.experiment.against.chance <- function (dat = ., 
                                filter.str,
                                filter.col = "experimentID",
                                value.col = "correct", 
                                chance.level = 50,
                                scale.to.percent = TRUE){
    
    scaling.factor <- 1
    if (scale.to.percent){
        if (max (dat[,value.col]) <= 1){
            scaling.factor <- 100
        }
    }
    
    dat.restricted <- 
        dat %>% 
        filter (.data[[filter.col]] == filter.str) %>% 
        mutate (!!value.col := scaling.factor * .data[[value.col]]) 
        

    list (llr = lik.ratio (dat.restricted[[value.col]], 
                           chance.level),
              tt = tt4 (dat.restricted[[value.col]], 
                        chance.level, print.results = FALSE),
              wt = wt (dat.restricted[[value.col]], 
                       chance.level))

    
}

```

`r clearpage()`


# Introduction 
Associative learning is widespread and exists in many species and domains [@Aslin1998; @Chen2015; Conway2005a; @Fiser2002; @Hauser2001; @Saffran-Science; @Toro2005-backward; @Turk-Browne2005; @Turk-Browne-reversal]. This led to the conclusion that the computations for which it is critical might be similarly widespread, a view that has been particularly prominent in language acquisition [@Aslin2012; @Seidenberg2002; @Thiessen2017]. 

However, associative learning is also remarkably modular [@Endress-duplications]. For example, humans have independent associative learning abilities in superficially similar domains, including the learning of associations of objects with landmarks vs. boundaries [@Doeller2008; @Doeller2008a], associations among social vs. non-social objects [@Tompson2019] and associations among consonants vs. vowels [@Bonatti2005]. Further, associative learning abilities are generally not correlated across domains [@Siegelman2015]. 

Preferential associations between specific classes of stimuli abound [@Seligman1970], and can evolve in just 40 generations in fruit flies [@Dunlap2014]. For example, rats readily associate tastes with  sickness and external events such as sounds or light flashes with pain, but cannot (or only with great difficulty) associate taste with pain or external events with sickness [@Garcia1974; @Garcia1976]. This pattern of associations reflects the likely ecological sources of sickness vs. pain (i.e., food vs. external events). Critically, the formation of taste-sickness associations (but not of other forms of associations) is blocked in a suckling context for rats who have not been exposed to solid food [@Martin1979; @Alberts1984], presumably because avoidance of the only food source would be detrimental; in contrast, minimal exposure to solid food re-establish taste-sickness associations [@Gubernick1984]. 

While such results suggest that, over evolutionary times, the adaptiveness of associative learning is regulated by increasing or decreasing its availability for specific classes of stimuli, it is less clear if associative learning is specialized for specific computational functions, or whether it is essentially a side effect of local neural processing [a "spandrel" in biological terms; @Gould1979], that is sometimes adaptive, somtimes neutral and sometimes detrimentl. Here, we address this issue in a domain where the importance of associative learning has long been recognized: word learning. We suggest that associative learning is critical for predicting speech material under conditions where it can be predicted, but that it does not lead to memory. 

One of the most prominent cases for associative learning in language acquisition is word segmentation. Speech is thought to be a continuous signal, and before learners can commit any words to memory, they need to learn where words start and where they end, respectively. One strategy to find word boundaries relies on Transitional Probabilities (TPs) among items, that is, the conditional probability of a syllable $\sigma_{i+1}$ given a preceeding syllable $\sigma_{i+1}$, $P(\sigma_{i}\sigma_{i+1})/P(\sigma_{i})$. Early on, Shannon [@Shannon1951] showed that human adults are sensitive to such distributional information. Subsequent work conclusively demonstrated that infants and non-human animals share this ability [XXX], and suggested that TPs can be extract using simple associative mechanisms such as Hebbian learning [@Endress-TP-Model].

However, a sensitivity to distributional information does not imply that learners extract chunks that can be stored in memory. In fact, such as sensitive is typically tested by comparing participants' preference for high-TP items over low-TP items. It turns out that participants sometimes prefer high-TP items thet have never seen or heard (and thus could not have memorize) over low-TP items they have heard [@Endress-Phantoms-Vision], suggesting that associative learning and memory for specific chunks may be dissociable. In fact, the types of representations created by associative learning might well be different from those used for linguistic stimuli [@Endress-Phantoms-Vision; @Fischer-Baum2011], while associative knowledge might be critical for predictive processing that is critical for both language [@Levy2008; @Trueswell1999] and other cognitive processes (XXX).

Here, we test this idea, focusing on the conditions under which associative learning operates and on the function it might have. To elucidate the conditions under which associative learning operates, we note that speech does not come as an  continuous signal but rather as a sequence of smaller units due to its prosodic organization [@Beckman1986; @Cutler1997; @Nespor1986; @Selkirk1986; @Shattuck-Hufnagel1996]. This prosodic organization is perceived in unfamiliar languages [@Brentari2011; @Endress-cross-seg; @Fenlon2008; @Pilon1981], by infants [@Hirsh-Pasek1987; Christophe1994; @Gout2004] and even by newborns [@Christophe2001]. Further, associative learning operates primarily *within* rather than across major prosodic boundaries [@Shukla2007; @Shukla2011]. As result, the a learner's segmentation task is not so much to integrate distributional information over long stretches of continuous speech, but rather to decide whether the the correct grouping in prosodic groups such as "*thebaby*" is "*theba + by*" or "*the + baby*". We thus ask to whether associative learning operates in such smaller groups, or only in longer stretches of continuous sound. 

To elucidate the function of associative learning, we ask adult participants what they recall after being exposed to the speech stream from  [@Saffran-Science], again with a continuous speech stream or a sequence of pre-segmented syllable sequences. 


[@Bortfeld2005; @Shi2008] Frequent words
[@Ngon2013]: statistics within




`r clearpage()`

# Methods
## Recognition experiment (London)

### Participants
```{r stats-london-demographics-load}
dat.stats.london.demographics <- 
    read.table ('data/oversegmentation_city/demographics.txt', header=T) %>%
    dplyr::rename (experimentID = dir) %>% 
    filter (!grepl ("pros", experimentID))

if (!(dat.stats.london.demographics %>% 
    mutate (matching.Ns = (N.L1 == N.L2)) %>% 
    pull (matching.Ns) %>% 
    all)) {
    warning ("Some language conditions don't have equal N's in the London versions of the oversegmentation experiment, check demographics file.")    
}
                

dat.stats.london.demographics <- dat.stats.london.demographics %>% 
    dplyr::select (-c(N.L1, N.L2)) %>% 
    mutate (experimentID = revalue (
        experimentID,
        c(
            "./res.stats.e1" = "stats.1x.en.segm",
            "./res.stats.e1c.3x" = "stats.3x.en.segm",
            "./res.stats.e1c.3x.us3" = "stats.3x.us.segm",
            "./res.stats.e1b.cont" = "stats.3x.en.cont",
            "./res.stats.e1b2.cont.us3" = "stats.3x.us.cont",
            "./res.stats.e1b3.cont.us3" ="stats.3x.us.cont2"
        )))



```


```{r stats-london-demographics-print}
dat.stats.london.demographics %>% 
    filter (experimentID %in% 
            c("stats.3x.us.segm",
              "stats.3x.us.cont",
              "stats.3x.us.cont2")) %>% 
    mutate (experimentID = factor (experimentID,
                                   levels = c("stats.3x.us.segm",
                                            "stats.3x.us.cont",
                                            "stats.3x.us.cont2"))) %>% 
    mutate (experimentID = revalue (
        experimentID,
        c("stats.3x.us.segm" = "Pre-segmented",
          "stats.3x.us.cont" = "Continuous (1)",
          "stats.3x.us.cont2" = "Continuous (2)"))) %>% 
    # For sorting below
    group_by (experimentID) %>% 
    arrange (.by_group = TRUE) %>% 
    setNames (replace_column_labels(names(.))) %>%
    knitr::kable(caption = 'Demographics for Experiment 1.',
                 col.names = c("Familiarization Condition",
                               "N", "Females", "Males", 
                               "Age (*M*)", "Age (range)"),
                 booktabs = TRUE, escape = FALSE) %>%
    kable_classic() #%>%
    # kable_styling(latex_options =
    #                   c("scale_down"))

```
Participants were recruited from the City, University London participant pool and received course credit or monetary compensation for their time. We targeted 30 participants per experiment (15 per language). The final demographic information is given in Table \ref{tab:stats-london-demographics-print}. An additional six participants took part in the experiment but were not retained for analysis because they had taken part in a prior version of this experiment ($N = 4$), were much older than the rest of our sample ($N = 2$), or used their phone during the experiment or were visibly inattentive ($N = 2$).

### Design (London)
Participants were familiarized with a sequence of tri-syllabic words. In Language 1, both the TPs and the chunk frequency was higher in the bigram formed by the first two syllables than in the bigram formed by the last two syllables; as a result, an associative learner should a triplet like *ABC* into an initial *AB* chunk followed by a singleton *C* syllable (hereafter *AB+C* pattern). In Language 2, both the TPs and the chunk frequency favored an *A+BC* pattern). The basic structure of the words is shown in Table \ref{tab:stats-london-print-language-structure}

```{r stats-london-print-language-structure}
data.frame (L1.structure = 
                c("ABC", "ABD", "ABE",
                   "FGC", "FGD", "FGE",
                   "HJC", "HJD", "HJE"),
            L2.structure = 
                c("ABC", "FBC", "HBC",
                  "AGD", "FGD", "HGD",
                  "AJE", "FJE", "HJE"),
            L1.items = 
                c("AB", "FG", "HJ", rep("", 6)),
            L2.items = 
                c("BC", "GD", "JE", rep ("", 6)),
            L1.words = c(
                "w3:-le-gu:", "w3:-le-vOI", "w3:-le-nA:",
                "faI-zO:-gu:", "faI-zO:-vOI", "faI-zO:-nA:",
                "rV-b{-gu:", "rV-b{-vOI", "rV-b{-nA:"),
            L2.words = c(
                "w3:-le-gu:", "faI-le-gu:", "rV-le-gu:",
                "w3:-zO:-vOI", "faI-zO:-vOI", "rV-zO:-vOI",
                "w3:-b{-nA:", "faI-b{-nA:", "rV-b{-nA:")
            ) %>% 
    knitr::kable (caption = "Design of Experiment 1. (Left) Language structure. (Middle) Structure of test items. Correct items for Language 1 are foils for Language 2 and vice versa. (Right) Actual items in SAMPA format; dashes indicate syllable boundaries",
                  col.names = paste0 ("Language ", rep(1:2, 3)),
                  booktabs = TRUE, escape = TRUE) %>%
    kableExtra::add_header_above(c("Word structure for" = 2, 
                                   "Test item structure for" = 2, 
                                   "Actual words for" = 2),
                                 line = FALSE) %>%
    #kableExtra::kable_styling() %>%
    kableExtra::kable_classic(full_width = FALSE) 

    
```    

As result, in Language 1, the first bigram has a (forward and backward) TP of 1.0, while the second bigram has a (forward and backward) TP of .333. In contrast, in Language 2, the first bigram has a forward TP of .33, while the second bigram has a forward TP of 1.0. Likewise, the initial bigrams were three times as frequent as the final ones for Language 1, while the opposite holds for Language 2. 

We asked whether participants would extract initial bigrams or final bigrams. The test items are given in Table \ref{tab:stats-london-print-language-structure}. 


### Stimuli
Stimuli were synthesized using the mbrola system [@mbrola], using the *us3* (American English male) diphone base. (We also used the *en1* (British English male) diphone base; however, as discussed below, this diphone based turned out be of relatively low quality and introduced confounds in the data.)

We chose a constant segment duration of 60 ms (syllable duration 120 ms) with a constant $F_0$ of 120 Hz. These values were choosen to match recordings of natural speech that were intended to be used in an investigation of prosodic cues to word segmentation. 

For continuous streams, a single file with 45 repetitions of each word was synthesized for each language (2 min 26 s duration). It was faded in and out for 5 s using sox (http://sox.sourceforge.net/) and then compressed to an mp3 file using ffmpeg (https://ffmpeg.org/). The stream was then presented 3 times to a participant (total familiarization duration 7 min 17 s).

For segmented streams, words were individually synthesized using mbrola. We then used a custom-made perl script to randomize the words for each participant and concatenate them into a familiarization file using sox. The order of words was then randomized for each participant and concatenated into a single aiff file using sox (http://sox.sourceforge.net/). The silence among words was 540 ms (1.5 word durations). The total stream duration was 6 min 12s. The stream was then presented 3 times to a participant (total familiarization duration 18 min 14 s).

### Apparatus
The experiment was run using Psyscope X (http://psy.ck.sissa.it). Stimuli were presented over headphones in a quiet room. Responses were collected from pre-marked keys on the keybaord. 

### Procedure
Participants were informed that they would listen to a monologue by a talkative Martian, and instructed to try to remember the Martian words. Following this, they listend to three repetitions of the familiarization stream described above, for a total familiarization duration of 7 min 17 s (continuous stream) or 18 min 14 s (segmented stream).

Following this familiarization, participants were presented with pairs of items with an interstimulus interval of 500 ms, and had to choose which items was more like what they heard during familiarization. One item was comprised the first two syllables of a word, and was thus a correct choice for Language 1. The other items comprised the last two syllables of a word, and was thus a correct choice for Language 2. There were three items of each kind. They were combined into 9 test pairs. The test pairs were presented twice, with different item orders, for a total of 18 test trials.  

## Recall experiment
### Materials
We resynthesized the languages used in @Saffran-Science Experiment 2. The four words in each language are given in Table \ref{tab:recall-languages}. Stimuli were synthesized using the us3 (male American English) voice of the mbrola synthesizer [@mbrola], at with a constant F0 of 120 at a rate of 216 ms per syllable (108 ms per phoneme). 

```{r recall-recall-specificy-languages, include = FALSE}
words.fw <- list (L1 = c("pAbiku", "tibudO", "dArOpi", "gOLAtu"),
                  L2 = c("bikuti", "pigOLA", "tudArO", "budOpA"))
words.fw <- lapply (words.fw,
                    tolower)

words.bw <- lapply (words.fw,
                    reverse.items)

part.words.fw <- rbind.data.frame(
    # BCA
    lapply (words.fw,
            get.part.words,
            parts.word1 = c(2:3), 
            parts.word2 = c(1)),
    # CAB
    lapply (words.fw,
            get.part.words,
            parts.word1 = c(3), 
            parts.word2 = c(1:2)),
    stringsAsFactors = FALSE) %>%
    as.list 

part.words.bw <- rbind.data.frame(
    # BCA
    lapply (words.bw,
            get.part.words,
            parts.word1 = c(2:3), 
            parts.word2 = c(1)),
    # CAB
    lapply (words.bw,
            get.part.words,
            parts.word1 = c(3), 
            parts.word2 = c(1:2)),
    stringsAsFactors = FALSE) %>%
    as.list 

class.words.fw <- rbind.data.frame(
    # AiBiCj
    lapply (words.fw,
            get.part.words,
            parts.word1 = c(1:2), 
            parts.word2 = c(3)),
    # AiBjCj
    lapply (words.fw,
            get.part.words,
            parts.word1 = c(1), 
            parts.word2 = c(2:3)),
    stringsAsFactors = FALSE) %>%
    as.list 

low.tp.chunk.fw <- lapply (words.fw,
                           get.part.words,
                           parts.word1 = c(3), 
                           parts.word2 = c(1))

low.tp.chunk.bw <- lapply (words.bw,
                           get.part.words,
                           parts.word1 = c(3), 
                           parts.word2 = c(1))


syllables <- lapply (words.fw,
                     get.syllables.from.words)

# Generate list of concatenated words, bca part-words and cab part-words
word.sequences <- list (abc = lapply (words.fw,
                                      get.non.repeating.word.sequences,
                                      # Hopefully there will be no utterance longer than 
                                      # 10 * 3 = 30 syllables
                                      10)) 
word.sequences$bca <- lapply (word.sequences$abc,
                              substring, 3)
word.sequences$cab <- lapply (word.sequences$abc,
                              substring, 5)
```

```{r recall-print-languages}
words.fw %>%
    data.frame %>%
    #knitr::kable("latex", booktabs = T, caption = '\\label{tab:languages}Words used in the recall experiment.') %>%
    knitr::kable (caption = "\\label{tab:recall-languages}Languages used in the recall experiment.", booktabs = TRUE) %>%
    kable_styling(bootstrap_options = "striped")

```


During familiarization, words were presented 45 times each. For each participant, we generated a random concatenation of 45 repetitions of the 4 words, with the constraint that a words could not occur in immediate reptition. Each randomization was then (i) synthesized into a continuous speech stream using mbrola and then converted to mp3 using ffmpeg (https://ffmpeg.org/) (ii) used to concatenate words that had been synthesized in isolation, separated by silences of 222 ms into a segmented speech stream, which was then converted to mp3. Streams were faded in and out for 5 s using sox (http://sox.sourceforge.net/). For continuous streams, this yielded a stream duration of 1 min 57 s; for segmented streams, the duration was 2 min 37.

We created 20 versions of each streams with different random orders of words.

`r clearpage()`

### Procedure
#### Familiarization
Participants were informed that they would be listening to an unknown language and that they should try to learn the words from that language. Following, the familiarization stream was presented twice, leading to a total familiarization duration of 3 min 53 for the continuous streams and 5 min 13 for the segmented streams. They could proceed to the next presentation of the stream by pressing a button. 

For the online experiments, participants watched video with no clear objects during the familiarization (panning of the Carina nebula, obtained from https://esahubble.org/videos/heic0707g/). The video was combined with the speech stream using the the muxmovie utility.

Following the familiarization, the was a 30 s retention interval. Participants were instructed to count backwards from 99 in time with a metronome beat at 3s / beat. Performance was not monitored. 

(Note to self: This was the case for both psyscope and testable.)

#### Recall test
Following the retention interval, participants completed the recall test. During the lab-based experiments, participants had 45 s to repeat back the words they remembered; their vocalizations were recorded using ffmpeg and saved in mp3 format. During the web-based experiments, participants had 60 s to type their answer into a comment field, during which they viewed a progress bar. 

#### Recognition test
Following the recall test, participant completed a recognition test during which we pitted words against part-words. The (correct) test words for Language 1 (and part-words for Language 2) were /pAbiku/ and /tibudO/; the (correct) test words for Language 2 (and part-words for Language 1) were /tudArO/ and /pigOlA/.These items were combined into 4 test pairs

# Analysis

```{r stats-london-load-data}
dat.stats.london <- bind_rows (
    # Stream played 1x, segmented, en, unused
    read.table ('data/oversegmentation_city/res.stats.e1/res.tab', header=T) %>% 
        mutate (experimentID = "stats.1x.en.segm",
                experimentID.old = "e1",
                nStreams = 1,
                segm = "segmented",
                voice = "en",
                used = FALSE),
    
    # Stream played 3x, continuous, en, unused due to item bias
    read.table ('data/oversegmentation_city/res.stats.e1b.cont/res.tab', header=T) %>% 
                mutate (experimentID = "stats.3x.en.cont",
                experimentID.old = "e1b.cont",
                nStreams = 3,
                segm = "continuous",
                voice = "en",
                used = FALSE),

    # Stream played 3x, continuous, us3
    read.table ('data/oversegmentation_city/res.stats.e1b2.cont.us3/res.tab', header=T) %>% 
        mutate (experimentID = "stats.3x.us.cont",
                experimentID.old = "e1b2.cont.us3",
                nStreams = 3,
                segm = "continuous",
                voice = "us",
                used = TRUE),

    # Stream played 3x, continuous, us3
    # This is just a replication of the experiment above
    read.table ('data/oversegmentation_city/res.stats.e1b3.cont.us3/res.tab', header=T) %>% 
        mutate (experimentID = "stats.3x.us.cont2",
                experimentID.old = "e1b3.cont.us3",
                nStreams = 3,
                segm = "continuous",
                voice = "us",
                used = TRUE),
    
    # Stream played 3x, segmented, en
    read.table ('data/oversegmentation_city/res.stats.e1c.3x/res.tab', header=T) %>% 
        mutate (experimentID = "stats.3x.en.segm",
                experimentID.old = "e1c.3x",
                nStreams = 3,
                segm = "segmented",
                voice = "en",
                used = FALSE),
    
    # Stream played 3x, segmented, us3
    read.table ('data/oversegmentation_city/res.stats.e1c.3x.us3/res.tab', header=T) %>% 
            mutate (experimentID = "stats.3x.us.segm",
                experimentID.old = "e1c.3x.us3",
                nStreams = 3,
                segm = "segmented",
                voice = "us",
                used = TRUE) %>% 
        mutate (rt = as.numeric (as.character (rt))) 

) %>% 
    # Change factors to character, change back later
    mutate(across(where(is.factor), as.character)) %>% 
    # make subjects unique
    mutate (subj = paste0(experimentID, 
                          ".", 
                          subj)) %>% 
    mutate (correctItem = ifelse (correctPos == 1,
                                  item1, 
                                  item2),
            foil = ifelse (correctPos == 2,
                                  item1, 
                                  item2)) %>% 
    mutate(across(where(is.character), factor))
```

```{r recall-load-data, include = FALSE}

# Data from BSc at City
if (ANALYZED.DATA.SETS["CITY"]){
dat.recognition.city <- rbind(read.table ("data/recall_city/recall.i.e3.cont.tab", 
                               header=T, sep="\t", comment.char = "%"),
                     read.table ("data/recall_city/recall.i.e4.segm.tab",
                                 header=T, sep="\t", comment.char = "%")) %>% 
    mutate (subj = factor (tolower(as.character(subj))))



    dat.recall.city <- gdata::read.xls(
        "data/recall_city/segmentation_recall_transcriptions.xlsx", 
                                       sheet="Sheet2-ade",
                                       stringsAsFactors = FALSE,
                                       header=TRUE) %>%
        mutate (subj = paste (subjNum, subjInitials, sep = ".")) %>%
        mutate (closest_match = tolower(closest_match)) %>%
        # dplyr::distinct (.keep_all = TRUE)
        dplyr::distinct (subjNum, subjInitials, streamType, lang, closest_match, .keep_all = TRUE) %>%
        filter (!is.na (subjNum))
}

if (ANALYZED.DATA.SETS["TESTABLE"]){
    # Data from testable
    dat.recall.tstbl <-
        read.testable.results("data/recall_testable/399612_results", 
                              comment.char = "",
                              ignore.col.prefixes = IGNORE.COL.PREFIXES,
                              stringsAsFactors = FALSE)  %>%
        filter (myPhase != "sound_test") %>% 
        setNames (gsub ("myLang", "lang", names (.)))
    
    
}
```

```{r recall-check-that-city-participants-have-both-stream-types}

if (ANALYZED.DATA.SETS["CITY"]){
    dat.subj.with.one.streamType.city <- dat.recall.city %>%
        distinct(subj, streamType) %>%
        #    xtabs(formula = ~ subj + streamType) %>%
        xtabs(formula = ~ subj ) %>%
        as.data.frame() %>% 
        filter (Freq != 2)
    
    if (nrow(dat.subj.with.one.streamType.city))
        warning ("Some City participants have productions in only one stream type, exiting.")
}

# Testable subjects have just one stream anyhow
```

```{r recall-find-bad-subjects}
if (ANALYZED.DATA.SETS["CITY"]){
    bad.subj.city <- dat.recall.city %>% 
        filter (streamType == "continuous") %>% 
        distinct(subj, subjNum, subjInitials, correct_segm) %>%
        filter (correct_segm < .5) %>%
        pull ("subj")
    
    if (REMOVE.INCOMPLETE.SUBJ){
        bad.subj.city <- c(bad.subj.city,
                           dat.subj.with.one.streamType.city %>% 
                               pull (subj) %>% 
                               levels2) %>% 
            unique
    }
}

if (ANALYZED.DATA.SETS["TESTABLE"]){
    bad.subj.tstbl <- dat.recall.tstbl %>% 
        filter (myPhase == "test_recognition") %>%
        distinct(filename, correct) %>%
        group_by (filename) %>% 
        summarize(correct_segm = mean (correct)) %>% 
        filter (correct_segm < .5) %>%
        pull ("filename")
    
}
```

## Recognition experiment
Accuracy was averaged for each participant, and the scores were tested against the chance level of 50% using Wilcoxon tests. Performance differences across the languages (Language 1 vs. 2) and, when applicable, familiarization conditions (pre-segmented vs. continuous) were assessed using using a generalized linear model for the trial-by-trial data with the fixed factors language and, where applicable, familiarization condition, as well as random slopes for participants, correct items and foils. Following [@Baayen2008], random factors were removed from the model when they did not contribute to the model likelihood.

We use likelihood ratios to provide evidence for the null hypothesis that performance did not differ from the chance level of 50%. Following [@Glover2004], we fit the participant average to a linear model comprising only an intercept and the null model fixing the intercept to 50%, and evaluate the likelihood of these models after correcting for the difference in the number of parameters using the Bayesian Information Criterion. 

## Recall experiment
### Recognition test
```{r recall-city-check-recognition-consistency, warnings = FALSE}
# Check whether the recognition accuracy in the xlsx file matches 
# that automatically computed.

if (ANALYZED.DATA.SETS["CITY"]){
    dat.recognition.city.consistency <- full_join(
        dat.recognition.city %>% 
            dplyr::rename (c("lang"="language",
                             "streamType"="condDir")) %>% 
            mutate (streamType = ifelse(grepl("cont$", streamType),
                                        "continuous",
                                        "segmented")) %>% 
            group_by(subj, streamType, lang) %>% 
            summarize (correct.raw = mean (correct)),
        
        
        dat.recall.city %>% 
            group_by(subj, streamType, lang) %>% 
            summarize (correct.xls = unique (correct_segm))
    )
    
    if ((dat.recognition.city.consistency %>% 
         filter (correct.raw != correct.xls) %>% 
         nrow) > 0){
        error ("Recognition test for City data is not correctly recorded.")
    }
}    

```

```{r recall-combine-recognition-data}

dat.recognition.combined <- data.frame (
    data.set = factor (),
    subj = factor (),
    mySegmentationCond = factor (),
    lang = factor (),
    correct = numeric ()
)

if (ANALYZED.DATA.SETS["CITY"]){
    dat.recognition.combined <- bind_rows(
        dat.recognition.combined,
        dat.recognition.city %>% 
            dplyr::rename (c("lang"="language",
                             "mySegmentationCond"="condDir")) %>% 
            mutate (mySegmentationCond = ifelse(grepl("cont$", 
                                                      mySegmentationCond),
                                                "continuous",
                                                "segmented"))  %>% 
            dplyr::select(subj, mySegmentationCond, lang, correct) %>% 
            add_column(data.set = "city", .before = 1))
}


if (ANALYZED.DATA.SETS["TESTABLE"]){
    dat.recognition.combined <- bind_rows(
        dat.recognition.combined,
        dat.recall.tstbl %>%
            filter (myPhase == "test_recognition") %>%
            dplyr::rename (c("subj"="filename")) %>% 
            dplyr::select(subj, mySegmentationCond, lang, correct) %>% 
            add_column(data.set = "tstbl", .before = 1))
}
        

```

```{r recall-recognition-descriptives}


dat.recognition.combined.m <- bind_rows(
    dat.recognition.combined %>% 
        group_by(subj, data.set, mySegmentationCond, lang) %>%
        summarize (correct = mean (correct)) %>% 
        group_by(data.set, mySegmentationCond, lang) %>% 
        summarize (N = n (),
                   M = mean (correct),
                   SE = se (correct),
                   p.wilcox = wilcox.p(correct, .5)) %>% 
        add_column(filter = "all", .before = 1),
    dat.recognition.combined %>% 
        filter (!(subj %in% c(bad.subj.city, bad.subj.tstbl))) %>% 
        group_by(subj, data.set, mySegmentationCond, lang) %>%
        summarize (correct = mean (correct)) %>% 
        group_by(data.set, mySegmentationCond, lang) %>% 
        summarize (N = n (),
                   M = mean (correct),
                   SE = se (correct),
                   p.wilcox = wilcox.p(correct, .5)) %>% 
        add_column(filter = ">= 50%", .before = 1)
) 

dat.recognition.combined.m %>% 
    dplyr::select (-c("filter")) %>% 
    kable (caption = "Descriptives for the recognition test", booktabs = TRUE, escape = FALSE) %>%
    #     kable_styling() %>%
    pack_rows(index = make.pack.index (dat.recognition.combined.m$filter))
        

```


```{r recall-recognition-lmer, eval = FALSE}
# This doesn't converge

# Use glmer to test for significance
# * Test if intercept is > 0 
# * This is equivalent to P > .5
# * Since logit (x) = 1 / (1 + exp(-x))


lmer.recognition.city.all.results <- glmer (
    correct ~ mySegmentationCond +  
        (1|subj) + (1 | lang), 
    control=glmerControl(optimizer="bobyqa"),
    family="binomial",
    data = dat.recognition.combined %>% 
        mutate (across (where (is.character), factor)) %>%  
        filter (data.set == "city") 
) %>% 
    extract.results.from.model(.)

lmer.recognition.city.filtered.results <- glmer (
    correct ~ mySegmentationCond +  
        (1|subj) + (1 | lang), 
    control=glmerControl(optimizer="bobyqa"),
    family="binomial",
    data = dat.recognition.combined %>% 
        mutate (across (where (is.character), factor)) %>%  
        filter (data.set == "city") %>% 
        filter (!(subj %in% bad.subj.city))
) %>% 
    extract.results.from.model(.)

lmer.recognition.tstbl.all.results <- glmer (
    correct ~ mySegmentationCond +  
        (1|subj) + (1 | lang), 
    control=glmerControl(optimizer="bobyqa"),
    family="binomial",
    data = dat.recognition.combined %>% 
        mutate (across (where (is.character), factor)) %>%  
        filter (data.set == "tstbl")
) %>% 
    extract.results.from.model(.)

lmer.recognition.tstbl.filtered.results <- glmer (
    correct ~ mySegmentationCond +  
        (1|subj) + (1 | lang), 
    control=glmerControl(optimizer="bobyqa"),
    family="binomial",
    data = dat.recognition.combined %>% 
        mutate (across (where (is.character), factor)) %>%  
        filter (data.set == "tstbl") %>% 
        filter (!(subj %in% bad.subj.tstbl))
) %>% 
    extract.results.from.model(.)


bind_rows(
    lmer.recognition.city.all.results %>% 
        process.glmm.table, 
    lmer.recognition.city.filtered.results %>% 
        process.glmm.table, 
    lmer.recognition.tstbl.all.results %>% 
        process.glmm.table, 
    lmer.recognition.tstbl.filtered.results %>% 
        process.glmm.table
    ) %>% 
     kable (caption = "Results of a generalized linear model for the segmentation test. The model included random intercepts for participants and  languages. We didn't test whether any of these random effects contributed to the model likelihood. Note that a positive intercept indicates above chance performance. ", booktabs = TRUE, escape = FALSE) %>% 
    pack_rows(index = c(
        "City, all participants" = 2,
        "City, filtered" = 2,
        "Testable, all participants" = 2,
        "Testable, filtered" = 2))
    
    
```



### Recall test (for the lab-based experiment or in general?)
The substitution rules employed in the current experiment are shown in Table \ref{tab:recall-print-substitution-rules}.

#### Substitution rules compensating for potential misperceptions

* O might be perceived as A (but probably not vice versa)
* Voiced and unvoiced consonants can be confused:
  	 - g and k
	 - d and t
	 - b and p

* b might be perceived as v

In some cases, these rules give you several possible matches. For example, in line 64, rapidala might be rOpidAlA or rOpidOlA

In such case, we apply the following criteria to decide which match to choose (in this order).

1.  If one option provides more or longer existing chunks, choose it. For example, rOpidAlA has the chunk rOpi (pidA isn't possible in the stream), while rOpidOlA contains rOpi, so in this case the rule doesn't discrminate between the two :)

2. If one option requires fewer changes with respect to the original transcription, choose that.

I would apply the rules in this order, but I can see why you might want to use the opposite order as well.





```{r specify-substitution-rules-pre-segmentation}

# Pre-segmentation subsitution rules
# These rules are not taken into consideration for the transformation count

substitution.rules.pre.segmentation <- list (
    list ("-", "", FALSE),
    list ("2", "tu", FALSE),
    list ("two", "tu", FALSE),
    # Some participants perceived "rock"
    list ("([aeou])ck", "\\1k", FALSE),
    list ("ar([,\\s+])", "a\\1", TRUE),
    # Some participant perceived "dollar"
    list ("ar$", "a", TRUE),
    # The next one most likely reflects a typo
    list ("tyu", "tu", FALSE),
    list ("ph", "f", FALSE),
    list ("th", "t", FALSE),
    list ("qu", "k", FALSE),
    list ("ea", "i", FALSE),
    list ("ou", "u", FALSE),
    list ("aw", "a", FALSE),
    list ("ai", "a", FALSE),
    list ("ie", "i", FALSE),
    list ("ee", "i", FALSE),
    list ("oo", "u", FALSE),
    list ("e", "i", FALSE),
    list ("c", "k", FALSE),
    list ("w", "v", FALSE),
    list ("y", "i", FALSE),
    list ("h", "", FALSE)) %>%
    rename_list_items (c("pattern", "replacement", "perl"))


apply.substitution.rules.pre.segmentation <- function (utterance = .){
    
    for (s.rule in substitution.rules.pre.segmentation){
        
        utterance <- gsub (s.rule$pattern,
                           s.rule$replacement, 
                           utterance, 
                           perl = s.rule$perl)
    }
    
    return (utterance)
    
    # July 27, 2020: We now loop through substitution
    # rules so they can be printed more easily
    
    # compose (
    #     function (X) {gsub ("h", "", X)},
    #     function (X) {gsub ("y", "i", X)},
    #     function (X) {gsub ("w", "v", X)},
    #     function (X) {gsub ("c", "k", X)},
    #     function (X) {gsub ("e", "i", X)},
    #     function (X) {gsub ("oo", "u", X)},
    #     function (X) {gsub ("ee", "i", X)},
    #     function (X) {gsub ("ie", "i", X)},
    #     function (X) {gsub ("ai", "a", X)},
    #     function (X) {gsub ("aw", "a", X)},
    #     function (X) {gsub ("ou", "u", X)},
    #     function (X) {gsub ("ea", "i", X)},
    #     function (X) {gsub ("qu", "k", X)},
    #     function (X) {gsub ("th", "t", X)},
    #     function (X) {gsub ("ph", "f", X)},
    #     
    #     # The next one most likely reflects a typo
    #     function (X) {gsub ("tyu", "tu", X)},
    #     # Some participant perceived "dollar"
    #     function (X) {gsub ("ar$", "a", X, 
    #                         perl = TRUE)},
    #     function (X) {gsub ("ar([,\\s+])", "a\\1", X,
    #                         perl = TRUE)},
    #     # Some participants perceived "rock"
    #     function (X) {gsub ("([aeou])ck", "\\1k", X)},
    #     function (X) {gsub ("two", "tu", X)},
    #     function (X) {gsub ("2", "tu", X)},
    #     function (X) {gsub ("-", "", X)}
    #     
    # ) (utterance)
    # 
}
```

```{r recall-specify-substitution-rules-post-segmentation}
# Post-segmentation subsitution rules
# These rules are  taken into consideration for the transformation count


substitution.rules.post.segmentation <- list (
    list ("u", "o"),
    list ("v", "b"),
    list ("p", "b"),
    list ("b", "p"),
    list ("t", "d"),
    list ("d", "t"),
    list ("k", "g"),
    list ("g", "k"),
    list ("a", "o")
) %>% 
    rename_list_items (c("pattern", "replacement"))

apply.substitution.rules.post.segmentation <- function (candidate = .){
    
    for (s.rule in substitution.rules.post.segmentation){
        
        candidate <- replace_phoneme (
            candidate,
            s.rule$pattern,
            s.rule$replacement)
    }
    
    return (candidate)
    
    # July 27, 2020: We now loop through substitution
    # rules so they can be printed more easily
    
    # compose (
    #     function (X) {replace_phoneme (X, "a", "o")},
    #     function (X) {replace_phoneme (X, "g", "k")},
    #     function (X) {replace_phoneme (X, "k", "g")},
    #     function (X) {replace_phoneme (X, "d", "t")},
    #     function (X) {replace_phoneme (X, "t", "d")},
    #     function (X) {replace_phoneme (X, "b", "p")},
    #     function (X) {replace_phoneme (X, "p", "b")},
    #     function (X) {replace_phoneme (X, "v", "b")},
    #     function (X) {replace_phoneme (X, "u", "o")}
    # ) (candidate)
}
```

```{r recall-print-substitution-rules}
full_join (
    substitution.rules.pre.segmentation %>% 
        lapply (., unlist) %>% 
        do.call (rbind, .) %>% 
        as.data.frame (stringsAsFactors = FALSE)%>% 
        select (-c("perl")) %>% 
        mutate (line.number = row_number()),
    substitution.rules.post.segmentation %>% 
        lapply (., unlist) %>% 
        do.call (rbind, .) %>% 
        as.data.frame (stringsAsFactors = FALSE) %>% 
        mutate (line.number = row_number()),
    by = "line.number",
    keep = FALSE
) %>% 
    select (-c("line.number")) %>% 
    mutate_each(funs(replace(., which(is.na(.)), ""))) %>% 
    kable (caption = "Substitution rules applied to the participants vocalizations before and after the input was segmented into chunks. The patterns are given as regular expressions",
           col.names = rep (c("Pattern", "Replacement"), 2),
           booktabs = TRUE) %>%
    #     kable_styling() %>%
        add_header_above(c("Before segmentation" = 2,
                          "After segmentation" = 2)) %>% 
    kableExtra::kable_classic()

```




```{r recall-remove-bad-subjects}
if (REMOVE.BAD.SUBJ){
    if (ANALYZED.DATA.SETS["CITY"]){
        
        dat.recall.city <- dat.recall.city %>%
            remove.bad.subj(bad.subj.city,
                            subj.var = "subj")
        
        
    }
    
    if (ANALYZED.DATA.SETS["TESTABLE"]){

        dat.recall.tstbl <- dat.recall.tstbl %>%
            remove.bad.subj(bad.subj.tstbl,
                            subj.var = "filename")
        
    }
    
}
```

#### Identify closest matches (for testable)
```{r recall-extract-recognition-performance}
if (ANALYZED.DATA.SETS["TESTABLE"]){
    dat.recall.tstbl.recognition.m <-dat.recall.tstbl %>% 
        filter (myPhase == "test_recognition") %>%
        group_by(filename, lang, mySegmentationCond) %>%
        summarize (N = n(),
                   correct_segm = mean (correct))
    
}
```
```{r recall-extract-recall-items}
if (ANALYZED.DATA.SETS["TESTABLE"]){
    if (RESEGMENT.RESPONSES){
        dat.all.recall.items.tstbl <- dat.recall.tstbl %>% 
            filter (myPhase == "test_recall") %>%
            filter (responseType == "comment") %>%
            dplyr::select(filename, subjectGroup, age, sex, Native.language.s., 
                          mySegmentationCond, lang, response) %>% 
            mutate (response = gsub (";1$", "", response)) %>%
            mutate (response = gsub (";timeout$", "", response)) %>%
            mutate (response = sub ("^\\s+", "", response)) %>%
            mutate (response = gsub ("\\s+$", "", response)) %>% 
            mutate (response = gsub ("\\\\n", ",", response)) %>%
            mutate (response = gsub (", ", ",", response)) %>%
            mutate (response = gsub (",,", ",", response)) %>%
            mutate (response = gsub (",$", "", response)) %>%
            mutate (response = gsub ("\\d\\.\\s*", "", response)) %>%
            mutate (response = tolower(response)) %>%
            arrange (desc(mySegmentationCond))
        
        # Add recognition performance
        dat.all.recall.items.tstbl <- left_join(
            dat.all.recall.items.tstbl,
            dat.recall.tstbl.recognition.m %>% 
                ungroup %>% 
                select (filename, correct_segm),
            by = "filename")
        
        
        # save.data.frame(all.recall.items,
        #                 row.names = FALSE)    
        
        # dat <- read.delim ("all.recall.items.txt", 
        #                    sep = "\t", 
        #                    stringsAsFactors = FALSE)
        
        dat.all.recall.items.tstbl <- dat.all.recall.items.tstbl %>% 
            # Filter participants for whom the vocalization 
            # cannot be analyzed (computer ran for several 
            # days), 
            filter (!(filename %in% get.from.list(L.BAD.SUBJ.CPUTIME$tstbl, "subj")))
        
    }
}
```

```{r recall-find-closest-matches-to-recall-items}

if (ANALYZED.DATA.SETS["TESTABLE"]){
    if (RESEGMENT.RESPONSES){        
        i <- 1
        dat.all.recall.items.tstbl.with.candidates <- dat.all.recall.items.tstbl %>% 
            slice (i) %>% 
            #group_by_all (.drop = FALSE) %>%
            group_by_all %>%
            do (process.utterance(.$response,
                                  .$lang,
                                  word.sequences,
                                  syllables)) 
        
        for (i in 2:nrow(dat.all.recall.items.tstbl)){
            cat (sprintf ("Processing row %d of %d (%s)\n%s\n", 
                            i, 
                            nrow(dat.all.recall.items.tstbl),
                            dat.all.recall.items.tstbl$filename[i] %>% 
                                as.character,
                            dat.all.recall.items.tstbl$response[i]
                            ))
            
            dat.all.recall.items.tstbl.with.candidates <- dat.all.recall.items.tstbl.with.candidates %>% 
                bind_rows(dat.all.recall.items.tstbl %>% 
                              slice (i) %>% 
                              #                          group_by_all (.drop = FALSE) %>%
                              group_by_all%>%
                              do (process.utterance(.$response,
                                                    .$lang,
                                                    word.sequences,
                                                    syllables)))
        }
        
        
        # candidates <- process.utterance(
        #     utterance = dat.all.recall.items.tstbl$response[38],
        #     lang = dat.all.recall.items.tstbl$lang[38],
        #     word.sequences)
        
        dat.all.recall.items.tstbl.with.candidates  <- 
            dat.all.recall.items.tstbl.with.candidates %>%
            as.data.frame %>%
            mutate (across (c("underlying",
                              "surface",
                              "closest.match"),
                            as.character)) %>% 
            filter ((closest.match.length %% 2) == 0) %>% 
            setNames(gsub ("closest.match$", "closest_match_just_match", names (.))) %>% 
            mutate (closest_match =
                        pmap_chr (., add.other.syllables.to.match))
        
        
        save.data.frame(dat.all.recall.items.tstbl.with.candidates,
                        .path = "output",
                        row.names = FALSE)
        
    } else {
        
        dat.all.recall.items.tstbl.with.candidates <- read.delim("output/dat.all.recall.items.tstbl.with.candidates.txt", sep = "\t", header = TRUE, stringsAsFactors = FALSE)
    }
}
```

Each recall response was analyzed in five steps. First, we applied pre-segmentation substitution rules to make the transcriptions more consistent (see Table \ref{tab:recall-print-substitution-rules}). For example, *ea* (presumably as in *tea*) was replaced with *i*. 

Second, responses were segmented into their underlying units. If a response contained a semicolon (;) or comma character (,), these were used to delineate units. For each of the resulting units, we verified if they contained additional spaces. If they did, these spaces were removed if further subdividing resulted in any single-syllable response (operationalized as a string with a single vowel); otherwise, the units were further sub-divided based on the spaces. The rationale for this algorithm is that responses such as *bee coo tee,two da ra,bout too pa* were like to reflect the words *bikuti*, *tudaro* and *budopa*.

Finally, if the response did not contain any commata or semicolons, it was segmented based on its spaces (if any).

Third, we removed geminate consonants and applied another set of substitution rules that to take into account possible misperceptions (see \ref{tab:recall-print-substitution-rules})). For example, we treated the voiced and unvoiced variety of stop consonants as interchangeable. Specifically, for each "surface" form produced by the participants, we generated candidate "underlying" forms by recursively applying all substitutions rules and keeping track of the number of substitution rules that were applied to derive an underlying form from a surface form. For each unique candidate underlying form, we kept the shortest derivation. 

Fourth, for each candidate underlying form, we identified the longest matching string in the familiarization stream. The algorithm first verified if a form was contained in a speech stream starting with an *A*, *B* or *C* syllable; if the underlying form contained unattested syllable, one syllable change was allowed with respect to the speech streams. If no matches were found, two substrings were created by clipping the first or the last syllable from the underlying form, and the search was repeated recursively for each of these substrings until a match was found. We then selected the longest match for all substrings. 

Fifth, for each surface form, we selected the underlying form using the criteria (in this order) that, among the candidate underlying form of each surface form, the selected underlying form had (i)  had the maximal number of attested syllables, (ii) the maximal length, and (iii) the shortest derivation. 

#### Change columns to categorize transcriptions
```{r recall-categorize-transcriptions-define, include = FALSE}
#categorize.matches %<a-% {
categorize.matches <- function (dat = ., current.lang) {
    dat %>% 
        mutate (n_syllables = count.sylls (closest_match)) %>%
        mutate (is_word = is.item.type (closest_match, 
                                        words.fw[[current.lang]])) %>%
        mutate (is_multiple_words = is.concatenation.of.item.type (closest_match,
                                                                   words.fw[[current.lang]])) %>%
        mutate (is_single_or_multiple_words = is_word | is_multiple_words) %>% 
        mutate (is_part_word = is.item.type (closest_match, 
                                             part.words.fw[[current.lang]])) %>%
        mutate (is_multiple_part_words = is.concatenation.of.item.type (closest_match,
                                                                        part.words.fw[[current.lang]])) %>%
        mutate (is_single_or_multiple_part_words = is_part_word | is_multiple_part_words) %>%
        mutate (is_class_word = is.item.type(closest_match,
                                             class.words.fw[[current.lang]])) %>%
        mutate (is_high_tp_chunk = is.chunk.from.item.type (closest_match,
                                                            words.fw[[current.lang]])) %>%
        mutate (is_low_tp_chunk = is.chunk.from.item.type (closest_match,
                                                           low.tp.chunk.fw[[current.lang]])) %>%
        mutate (has_correct_initial_syllable = has.correct.initial.syll (closest_match,
                                                                         words.fw[[current.lang]])) %>%
        mutate (has_correct_final_syllable = has.correct.final.syll (closest_match,
                                                                     words.fw[[current.lang]])) %>%
        mutate (is_part_of_stream = ifelse (n_syllables == 1,
                                            closest_match %in% 
                                                get.substrings.of.length(words.fw[[current.lang]], 2) %>% 
                                                paste,
                                            is_word |
                                                is_multiple_words |
                                                is_part_word |
                                                is_multiple_part_words |
                                                is_high_tp_chunk | 
                                                is_low_tp_chunk)) %>%
        mutate (is_part_of_stream = ifelse (is.na (is_part_of_stream),
                                            FALSE,
                                            is_part_of_stream)) %>% 
        mutate (is_part_of_stream = as.logical(is_part_of_stream)) %>%
        mutate (is_bw_word = is.item.type (closest_match, 
                                           words.bw[[current.lang]])) %>%
        mutate (is_multiple_bw_words = is.concatenation.of.item.type (closest_match,
                                                                      words.bw[[current.lang]])) %>%
        mutate (is_single_or_multiple_bw_words = is_bw_word | is_multiple_bw_words) %>%
        mutate (is_bw_part_word = is.item.type (closest_match, 
                                                part.words.bw[[current.lang]])) %>%
        mutate (is_multiple_bw_part_words = is.concatenation.of.item.type (closest_match,
                                                                           part.words.bw[[current.lang]])) %>%
        mutate (is_single_or_multiple_bw_part_words = is_bw_part_word | is_multiple_bw_part_words) %>%
        mutate (is_high_tp_bw_chunk = is.chunk.from.item.type (closest_match,
                                                               words.bw[[current.lang]])) %>%
        mutate (is_low_tp_bw_chunk = is.chunk.from.item.type (closest_match,
                                                              low.tp.chunk.bw[[current.lang]])) %>%
        mutate (average_fw_tp = calculate.average.tps.from.chunks (closest_match,
                                                                   list (list (chunks = words.fw[[current.lang]],
                                                                               tp = 1),
                                                                         list (chunks = low.tp.chunk.fw[[current.lang]],
                                                                               tp = 1/3)),
                                                                   chunk.length = 4)) %>% 
        mutate (expected_fw_tp = calculate.expected.tps.for.chunks (closest_match, words.fw[[current.lang]])) %>%
        mutate (average_bw_tp = calculate.average.tps.from.chunks (reverse.items (closest_match),
                                                                   list (list (chunks = words.bw[[current.lang]],
                                                                               tp = 1),
                                                                         list (chunks = low.tp.chunk.bw[[current.lang]],
                                                                               tp = 1/3)),
                                                                   chunk.length = 4)) 
    
}
```


```{r recall-categorize-transcriptions-do, include = FALSE}

if (ANALYZED.DATA.SETS["CITY"]){
    dat.recall.city <- lapply (c(L1 = "L1", L2 = "L2"),
                               function (current.lang) {
                                   dat.recall.city %>% 
                                       filter (lang == current.lang) %>%
                                       categorize.matches (current.lang)
                               }) %>% 
        do.call (rbind, .) %>% 
        arrange(subjNum, subjInitials, streamType)
}

if (ANALYZED.DATA.SETS["TESTABLE"]){
    dat.all.recall.items.tstbl.with.candidates <- lapply (c(L1 = "L1", L2 = "L2"),
                                                          function (current.lang) {
                                                              dat.all.recall.items.tstbl.with.candidates %>%
                                                                  filter (lang == current.lang) %>%
                                                                  categorize.matches (current.lang)
                                                          }) %>% 
        do.call (rbind, .) %>% 
        arrange(filename, mySegmentationCond)
    
}
```

We then computed various properties for each underlying form, given the "target" language the participant had been exposed to. Specifically, we calculated: (1) the number of syllables, (2) whether it was a word from the target language, (3) whether it was a concatentation of words from the target language, (4) whether it was a single word or a concatenation of words from the target language (i.e., the disjunction of (2) and (3)), (5) whether it was a part-words from the target language, (6) whether it was a *complete* concatenation of part-words from the target language (i.e., the number of syllables of the item had to be a multiple of three, without any unattested syllables), (7) whether it was a single part-word or a concatenation of part-words from the target language, (8) whether it was a "class-word" with the two initial syllables coming from one word and the final syllables from another word or vice versa (i.e., class-words had the form *A\_iB\_iC\_j* or *A\_iB\_jC\_j*, (9) whether it was high-TP chunk (i.e., a word or a word with the first or the last syllable missing, after removing any leading or trailing unattested syllables), (10) whether it was a low-TP chunk (i.e., a chunk of the form *C\_iA\_j*, after removing lead or trailing unattested syllables, (11) whether it had a "correct" initial syllable, (12) whether it had a "correct" final syllable, (13) whether it is part of the speech stream (i.e., the disjunction of being an attested syllable, being a word or a concatenation thereof, being a part-word or a concatenation thereof, being a high-TP chunk or a low-TP chunk), (14) whether it was a backward word from the target language (i.e., a word with the syllable order reversed), (15) whether it was a concatenation of backward words, (16) whether it was a backward word or a concatenation thereof (i.e., the disjunction of (14) and (15)), (17) whether it was a backward part-word, (18) whether it was a concatenation of backward part-words, (19) whether it was a backward word or a concatenation thereof (i.e., the disjunction of (17) and (18)), (19) whether it was high-*backward*-TP chunk (i.e., a backward word or a backward word with the first or the last syllable missing, after removing any leading or trailing unattested syllables), (20) whether it was a low-*backward*-TP chunk (i.e., a chunk of the form *A\_iC\_j*, after removing lead or trailing unattested syllables, (21) the average forward TP of the transitions in the form, (22) the *expected* forward TP of the form if form is attested in the speech stream (see below for the calculation), and (23) the average backward TP of the transitions in the form.

#### Expected TPs 
For items that are *correctly* reproduced from the speech stream, the expected TPs depend on the starting position. For example, the expected TPs for items of at least 2 syllables starting on an initial syllable are c(1, 1, 1/3, 1, 1, 1/3, 1, 1, 1/3, ...); if the item starts on a word-medial syllable, these TPs are c(1, 1/3, 1, 1, 1/3, 1, 1, 1/3, 1, ...).

In contrast, the expected TPs for a random concatenation of syllables are the TPs in a random bigram. For an *A* or a *B* syllable, the random TP is 1 $\times$ 1 / 12, as there is only 1 (out of 12) non-zero TP continuations. For a C syllable, the random TP is 3 $\times$ 1/3 / 12, as there are 3 possible concatenations. On average, the random TP is thus $(1/12 + 1/12 + 1/12)/ 3 = 1/12 \approx .083$. 

```{r recall-print-number-of-unattested-items}

dat.recall.unattested.m <- list ()

if (ANALYZED.DATA.SETS["CITY"]){
    
    dat.recall.unattested.m.city <- dat.recall.city %>%
        group_by (subj, streamType) %>% 
        summarize (N.total = n (), 
                   N.unattested = sum (!is_part_of_stream)) %>% 
        group_by (streamType) %>% 
        summarize (N.total.M = mean (N.total),
                   N.total.min = min (N.total),
                   N.total.max = max (N.total),
                   N.unattested.M = mean (N.unattested),
                   N.unattested.min = min (N.unattested),
                   N.unattested.max = max (N.unattested)) %>% 
        add_column (
            data.set = "city", 
            .before = 1)
    
    dat.recall.unattested.m <- c (dat.recall.unattested.m,
                                  list (city = dat.recall.unattested.m.city))
    
}

if (ANALYZED.DATA.SETS["TESTABLE"]){
    dat.recall.unattested.m.tstbl <- dat.all.recall.items.tstbl.with.candidates %>%
        group_by (filename, mySegmentationCond) %>% 
        summarize (N.total = n (), 
                   N.unattested = sum (!is_part_of_stream)) %>% 
        group_by (mySegmentationCond) %>% 
        summarize (N.total.M = mean (N.total),
                   N.total.min = min (N.total),
                   N.total.max = max (N.total),
                   N.unattested.M = mean (N.unattested),
                   N.unattested.min = min (N.unattested),
                   N.unattested.max = max (N.unattested)) %>% 
        add_column (
            data.set = "testable", 
            .before = 1) %>% 
        # for compability with city data set
        rename(streamType = mySegmentationCond)
    
    dat.recall.unattested.m <- c (dat.recall.unattested.m,
                                  list (testable = dat.recall.unattested.m.tstbl))
}

bind_rows(dat.recall.unattested.m) %>% 
    kable(caption="Number of unattested items",
          booktabs = TRUE) %>%
kable_styling(latex_options =
                              c("hold_position", 
                                "scale_down"))

```

```{r recall-save-unattested-items}
xlsx::write.xlsx (bind_rows(dat.recall.unattested.m) %>% 
                      data.frame,
                  file="output/segmentation_recall_unattested.xlsx",
                  row.names=FALSE,
                  sheetName="Ns",
                  append=FALSE)

if (ANALYZED.DATA.SETS["CITY"]){
    
    dat.recall.city %>%
        filter (!is_part_of_stream) %>% 
        xlsx::write.xlsx (.,
                  file="output/segmentation_recall_unattested.xlsx",
                  row.names=FALSE,
                  sheetName="city",
                  append=TRUE)

}

if (ANALYZED.DATA.SETS["TESTABLE"]){
    dat.all.recall.items.tstbl.with.candidates %>%
                filter (!is_part_of_stream) %>% 
        xlsx::write.xlsx (.,
                  file="output/segmentation_recall_unattested.xlsx",
                  row.names=FALSE,
                  sheetName="tstbl",
                  append=TRUE)

}


```


As shown in Table \ref{tab:recall-print-number-of-unattested-items}, there was a considerable number of recall responses containing unattested syllables. The complete list of unattested items is in `segmentation_recall_unattested.xlsx`. Unattested items are items that are not word, part-words (or concatenations thereof), high- or low-TP chunks, or a single syllable. However, it is unclear if these unattested syllables reflect misperceptions not caught by our substitution rules, typos, memory failures or creative responses. This makes it difficult to analyze these responses. For example, the TPs from and to an unattested syllable are zero. However, if the unattested syllable reflects a misperception or a typo, the true TP would be positive, and our estimates would underestimate the participant's statistical learning ability. 

We thus decided to restrict ourselves to responses that can be clearly interpreted and removed all items containing unattested syllables. Here, `FILTER.UNATTESTED.ITEMS` was set to `r FILTER.UNATTESTED.ITEMS`, while `FILTER.SINGLE.SYLLABLES` was set to `r FILTER.SINGLE.SYLLABLES`.


```{r recall-filter-unattested-items, include = FALSE}
if (FILTER.UNATTESTED.ITEMS){
    
    if (ANALYZED.DATA.SETS["CITY"]){
        dat.recall.city <- dat.recall.city %>%
            filter (is_part_of_stream)
    }
    
    if (ANALYZED.DATA.SETS["TESTABLE"]){
        dat.all.recall.items.tstbl.with.candidates <- dat.all.recall.items.tstbl.with.candidates %>%
            filter (is_part_of_stream)
    }
}
```

We also decided to remove single syllable responses, as it is not clear if participants volunteered such responses because they thought that individual syllables reflected the underlying units in the speech streams or because they misunderstood what they were ask to do.

```{r recall-filter-single-syllable-responses, include = FALSE}
if (FILTER.SINGLE.SYLLABLES){
    
    if (ANALYZED.DATA.SETS["CITY"]){
        dat.recall.city <- dat.recall.city %>%
            filter (n_syllables > 1)
    }
    
    if (ANALYZED.DATA.SETS["TESTABLE"]){
        dat.all.recall.items.tstbl.with.candidates <- dat.all.recall.items.tstbl.with.candidates %>%
            filter (n_syllables > 1)
    }
}
```

### Demographics and missing subjects
```{r recall-final-demographics-calculate}

if (ANALYZED.DATA.SETS["CITY"]){
    dat.recall.demographics.city <- dat.recall.city  %>% 
        #filter (streamType == "continuous") %>%
        #distinct (subj, Gender, Age, lang) %>%
        # most participants at city did both languages with different streamType
        distinct (streamType, subj, Gender, Age) %>%
        mutate (Gender = tolower (Gender)) %>%
        mutate (Gender = ifelse (startsWith(Gender, "f"), "female", "male")) %>%
        group_by(streamType) %>%
        summarize (N = n(), 
                   Females = sum (gdata::startsWith(Gender, "f", ignore.case = TRUE)),
                   Males = sum (gdata::startsWith(Gender, "m", ignore.case = TRUE)),
                   Age.m = round (mean (Age), 1),
                   Age.range = paste (range(Age), collapse = "-")) %>% 
        add_column(data.set = "city", .before = 1) %>% 
        add_column(lang = "both", .after = "streamType")
    
} 

if (ANALYZED.DATA.SETS["TESTABLE"]){
    dat.recall.demographics.tstbl <- 
    dat.all.recall.items.tstbl.with.candidates %>%
        distinct (filename, sex, age,
                  mySegmentationCond, lang) %>% 
        rename(subj = filename, Gender = sex, Age = age,
               streamType = mySegmentationCond) %>% 
        group_by(streamType, lang) %>% 
        summarize (N = n(),
                   Females = sum (Gender == "female"),
                   Males = sum (Gender == "male"),
                                      Age.m = round (mean (Age), 1),
                   Age.range = paste (range(Age), collapse = "-")) %>% 
        add_column(data.set = "tstbl", .before = 1) 

}

dat.recall.demographics.combined <- 
    data.frame (data.set = character (),
                streamType = character (),
                lang = character (),
                N = integer (),
                Females = integer (),
                Males = integer(),
                Age.m = numeric (),
                Age.range = character ()) 
                
if (ANALYZED.DATA.SETS["CITY"]) {
    dat.recall.demographics.combined <- bind_rows(
        dat.recall.demographics.combined,
        dat.recall.demographics.city
    )                
}

if (ANALYZED.DATA.SETS["CITY"]) {
    dat.recall.demographics.combined <- bind_rows(
        dat.recall.demographics.combined,
        dat.recall.demographics.tstbl
    )                
}

```

To reduce performance differences between the pre-segmented and the continuous familiarization conditions, participants were excluded from analysis if their accuracy in the recognition test was below 50% ($N = $, `r length (bad.subj.tstbl) + length (bad.subj.city)`).  Another `r length(L.BAD.SUBJ.CPUTIME$tstbl)` participants were excluded because parsing their productions took an excessive amount of computing time, though their productions did not seem to ressemble the familiarization items in the first place. The responses were `r paste (get.from.list(L.BAD.SUBJ.CPUTIME$tstbl, "response"), collapse = " // ")`.

The final demographic information is given in Table \ref{tab:recall-final-demographics-print}. 

```{r recall-final-demographics-print}
dat.recall.demographics.combined %>%
    setNames (replace_column_labels(names(.))) %>%
    knitr::kable(caption = 'Demographics of the final sample. Note that the City participants completed both segmentation conditions.', booktabs = TRUE, escape = FALSE) %>%
    kableExtra::pack_rows(index = make.pack.index(dat.recall.demographics.combined$data.set)) %>% 
    kableExtra::kable_classic()


```

#### Save categorized data 
```{r recall-save-data1, include = FALSE}
# save.data.frame(dat, row.names = FALSE)

if (ANALYZED.DATA.SETS["CITY"]){
    xlsx::write.xlsx (dat.recall.city,
                      file="output/recall.city.populated.xlsx",
                      row.names=FALSE,
                      sheetName="Sheet1",
                      append=FALSE)
} 

if (ANALYZED.DATA.SETS["TESTABLE"]){
    xlsx::write.xlsx (dat.all.recall.items.tstbl.with.candidates,
                      file="output/recall.tstbl.populated.xlsx",
                      row.names=FALSE,
                      sheetName="Sheet1",
                      append=FALSE)
}

```

#### Measures for productions in the recall phase
We will use the following measures to analyze the participants' productions in the recall phase. Some analyses below rely on within-participant averages [A], within-participant sums [S] or other measures [O]. 

* General measures
    - [S] Number of items produced. To be compared across segmentation conditions, and against zero. 
    - [A] Average length of items produced. To be compared across segmentation conditions.
    - [S,A] Number and proportion (among productions) of words (and concatenations thereof)
    - [S,A] Number and proportion (among productions) of part-words (and concatenations thereof)
    - [O] Performance in the two alternative forced-choice test. To be compared across segmentation conditions.
* TP-based analyses (raw TPs)
    - [A] Average forward TP in items
        + Compare across segmentation conditions
        + Compare to expected TPs for a random string. The expected TPs for a random concatenation are the TPs in a random bigram. For an A or a B syllable, the random TP is 1 $\times$ 1 / 12, as there is only 1 (out of 12) non-zero TP continuations. For a C syllable, the random TP is 3 $\times$ 1/3 / 12, as there are 3 possible concatenations. On average, the random TP is thus $(1/12 + 1/12 + 1/12)/ 3 = 1/12 \approx .083$. 
        + Calculate difference *expected* TPs for correctly reproduced items, given the item's initial position. The expected TPs for items of at least 2 syllables starting on an initial syllable are c(1, 1/3, 1, 1, 1/3, 1, 1, 1/3, ...). The difference between the actual and the expected TP needs to be compared to zero, as the expected TP differs across items.
    - [A] Average backward TP in items
* TP-based analyses (chunks). In addition to th raw TPs above, we also counted high- and low-TP *chunks*. As mentioned above, high-TP chunks are words or words with the first or the last syllable missing, after removing any leading or trailing unattested syllables, while low-TP chunks are chunks of the form $C_iA_j$, after removing lead or trailing unattested syllables. 
    + [S,A] Number and proportion of high TP-chunks.
    + [S,A] Number and proportion of low TP-chunks.
    + [0] Proportion of high-TP chunks among high and low-TP chunks.
* Positional analyses    
    - [A] Proportion of items with syllables in correct postions
        a. Items with correct initial syllables. Chance level: 4/12
        b. Items with correct final syllables. Chance level: 4/12
        c. Disjunction of *a* and *b*. Chance level: $2 \times 4/12 - 4/12 \times 4/12 = 5/9$

`r clearpage()`

# Results

## Recognition experiments (Results with the us3 diphone base; the en1 results are in the appendix)
        
```{r stats-london-make-averages}
dat.stats.london.m <- dat.stats.london %>% 
    group_by (experimentID, nStreams, segm, voice, used, lang, subj) %>% 
    summarize (correct = mean (correct))
```

```{r stats-london-descriptives}
# demographics can be gotten from the age.sex files
dat.stats.london.m %>%
    filter (experimentID != "stats.1x.en.segm") %>% 
    group_by (voice, experimentID,segm, lang) %>%
    summarize (N = n (),
               M = mean (correct),
               SE = se (correct)) %>% 
    kable (caption = "Descriptives. Check exclusion files",
           booktabs = TRUE)

```



### Can people recover words from pre-segmented prosodic units?
We first asked if learners can split utterances that likely result from prosodic presegmentation into its underlying components. 


```{r stats-london-stats.3x.us.segm.ana}

ana.stats.3x.us.segm <- dat.stats.london.m %>% 
    analyze.experiment.against.chance ("stats.3x.us.segm")
    
lmer.stats.3x.us.segm.1 <- glmer (correct ~ lang + 
                                         (1|subj) + (1|correctItem) + (1|foil),
                                  control=glmerControl(optimizer="bobyqa"),
                                  family="binomial",
                                  data =  dat.stats.london %>% 
                                      filter (experimentID == "stats.3x.us.segm")
                                  )    

lmer.stats.3x.us.segm.2 <- update (
    lmer.stats.3x.us.segm.1,
    ~ . - (1|foil))

lmer.stats.3x.us.segm.3 <- update (
    lmer.stats.3x.us.segm.2,
    ~ . - (1|correctItem))

# anova (lmer.stats.3x.us.segm.1,
#        lmer.stats.3x.us.segm.2,
#        lmer.stats.3x.us.segm.3)

lmer.stats.3x.us.segm.1.results <- 
    extract.results.from.model (lmer.stats.3x.us.segm.1)


```

```{r stats-london-stats.3x.us.segm.plot, fig.cap="Results for a segmented presentation of the stream (540 ms silences) with three repetition of the stream (45 repetitions per word). The diphone based was *us2*."}

if (PRINT.INDIVIDUAL.FIGURES){
    #current.plot.name <- "stats.3x.us.segm"
    #prepare.graphics
    
    
    
    dat.stats.3x.us.segm.for.plot <- dat.stats.london.m %>% 
        global.df.to.plot.df ("stats.3x.us.segm")
    
    
    
    strip4c (100*dat.stats.3x.us.segm.for.plot,  
             x=1:2,
             ylab="% Correct",
             xlab_big=names (dat.stats.3x.us.segm.for.plot),
             xlab_big_at=c(1:2),
             main="Segmented - 3 presentation of stream")
    #show.graphics
}
```

As shown in Figure \ref{fig:stats-london-stats.3x.us.segm.cont.plot}, the average performance did not differ significantly from the chance level of 50%, `r ana.stats.3x.us.segm$tt`, `r ana.stats.3x.us.segm$wt`. Likelihood ratio analysis favored the null hypothesis by a factor of `r ana.stats.3x.us.segm$llr` after correction with the Bayesian Information Criterion. Further, as shown in Table \ref{tab:stats-london-stats.us.lang.glmm.print}, performance did not depend on the language condition. As shown in Appendix XXX, the failure to use statistical learning was replicated using a second diphone base. 

Further, the failure to use statistical learning to split pre-segmented units was replicated in a pilot experiment with Spanish/Catalan speakers using chunk frequency and backwards TPs as the primary cues (see Supplementary Information XXX).


### Can people recover words from a continuous stream? (1)
While observers failed to split presegmented words into their underlying units, we now ask if they can do so when the words are embedded into a continuous speech stream.

```{r stats-london-stats.3x.us.cont.ana}

ana.stats.3x.us.cont <- dat.stats.london.m %>% 
    analyze.experiment.against.chance ("stats.3x.us.cont")
    
lmer.stats.3x.us.cont.1 <- glmer (correct ~ lang + 
                                         (1|subj) + (1|correctItem) + (1|foil),
                                  control=glmerControl(optimizer="bobyqa"),
                                  family="binomial",
                                  data =  dat.stats.london %>% 
                                      filter (experimentID == "stats.3x.us.cont")
                                  )    

lmer.stats.3x.us.cont.2 <- update (
    lmer.stats.3x.us.cont.1,
    ~ . - (1|foil))

lmer.stats.3x.us.cont.3 <- update (
    lmer.stats.3x.us.cont.2,
    ~ . - (1|correctItem))

# anova (lmer.stats.3x.us.cont.1,
#        lmer.stats.3x.us.cont.2,
#        lmer.stats.3x.us.cont.3)

lmer.stats.3x.us.cont.1.results <- 
    extract.results.from.model (lmer.stats.3x.us.cont.1)


```

```{r stats-london-stats.3x.us.cont.plot, fig.cap="Results for a continuous presentation of the stream (540 ms silences) with three repetition of the stream (45 repetitions per word). The diphone based was *us2*."}

if (PRINT.INDIVIDUAL.FIGURES){
    
    #current.plot.name <- "stats.3x.us.cont"
    #prepare.graphics
    
    dat.stats.3x.us.cont.for.plot <- dat.stats.london.m %>% 
        global.df.to.plot.df ("stats.3x.us.cont")
    
    
    
    strip4c (100*dat.stats.3x.us.cont.for.plot,  
             x=1:2,
             ylab="% Correct",
             xlab_big=names (dat.stats.3x.us.cont.for.plot),
             xlab_big_at=c(1:2),
             main="Continuous - 3 presentation of stream")
    #show.graphics
}
```

As shown in Figure \ref{fig:stats-london-stats.3x.us.segm.cont.plot}, the average performance differed significantly from the chance level of 50%, `r ana.stats.3x.us.cont$tt`, `r ana.stats.3x.us.cont$wt`. As shown in Table \ref{tab:stats-london-stats.us.lang.glmm.print}, performance did not depend on the language condition. As shown in Table \ref{tab:stats-london-stats.us.lang.glmm.print}, performance was significantly better compared to when the stream was pre-segmented. 


### Can people recover words from a continuous stream? (2) (Replication)
We replicated the sucessful tracking of statistical information using a new sample of participants.

```{r stats-london-stats.3x.us.cont2.ana}

ana.stats.3x.us.cont2 <- dat.stats.london.m %>% 
    analyze.experiment.against.chance ("stats.3x.us.cont2")
    
lmer.stats.3x.us.cont2.1 <- glmer (correct ~ lang + 
                                         (1|subj) + (1|correctItem) + (1|foil),
                                  control=glmerControl(optimizer="bobyqa"),
                                  family="binomial",
                                  data =  dat.stats.london %>% 
                                      filter (experimentID == "stats.3x.us.cont2")
                                  )    

lmer.stats.3x.us.cont2.2 <- update (
    lmer.stats.3x.us.cont2.1,
    ~ . - (1|foil))

lmer.stats.3x.us.cont2.3 <- update (
    lmer.stats.3x.us.cont2.2,
    ~ . - (1|correctItem))

# anova (lmer.stats.3x.us.cont2.1,
#        lmer.stats.3x.us.cont2.2,
#        lmer.stats.3x.us.cont2.3)

lmer.stats.3x.us.cont2.1.results <- 
    extract.results.from.model (lmer.stats.3x.us.cont2.1)


```

```{r stats-london-stats.3x.us.cont2.plot, fig.cap="Results for a continuous presentation of the stream (540 ms silences) with three repetition of the stream (45 repetitions per word). The diphone based was *us2*."}


if (PRINT.INDIVIDUAL.FIGURES){
    #current.plot.name <- "stats.3x.us.cont2"
    #prepare.graphics
    
    
    
    dat.stats.3x.us.cont2.for.plot <- dat.stats.london.m %>% 
        global.df.to.plot.df ("stats.3x.us.cont2")
    
    
    
    strip4c (100*dat.stats.3x.us.cont2.for.plot,  
             x=1:2,
             ylab="% Correct",
             xlab_big=names (dat.stats.3x.us.cont2.for.plot),
             xlab_big_at=c(1:2),
             main="Continuous (2) - 3 presentation of stream")
    #show.graphics
    
}
```

As shown in Figure \ref{fig:stats-london-stats.3x.us.segm.cont.plot}, the average performance differed significantly from the chance level of 50%, `r ana.stats.3x.us.cont2$tt`, `r ana.stats.3x.us.cont2$wt`. As shown in Table \ref{tab:stats-london-stats.us.lang.glmm.print}, performance did not depend on the language condition. As shown in Table \ref{tab:stats-london-stats.us.lang.glmm.print}, performance was significantly better compared to when the stream was pre-segmented. 

However, as shown in Appendix XXX, when trying to replicate these results using a different diphone base, we obtained a preference for specific items due to the poor quality of the diphone base.

```{r stats-london-stats.3x.us.segm.cont.glmm}

# Model including segmented and original continuous condition
lmer.stats.3x.us.segm.cont1.1 <- glmer (correct ~ lang*segm + 
                                           (1|subj) + (1|correctItem) + (1|foil),
                                       control=glmerControl(optimizer="bobyqa"),
                                       family="binomial",
                                       data =  dat.stats.london %>% 
                                           filter (experimentID %in%
                                                       c("stats.3x.us.segm",
                                                         "stats.3x.us.cont")))    
lmer.stats.3x.us.segm.cont1.2 <- update (
    lmer.stats.3x.us.segm.cont1.1,
    ~ . - lang:segm)

lmer.stats.3x.us.segm.cont1.3 <- update (
    lmer.stats.3x.us.segm.cont1.2,
    ~ . - (1|foil))

lmer.stats.3x.us.segm.cont1.4 <- update (
    lmer.stats.3x.us.segm.cont1.3,
    ~ . - (1|correctItem))

# anova (
#     lmer.stats.3x.us.segm.cont1.1,
#     lmer.stats.3x.us.segm.cont1.2,
#     lmer.stats.3x.us.segm.cont1.3,
#     lmer.stats.3x.us.segm.cont1.4
# )


lmer.stats.3x.us.segm.cont1.2.results <- 
    extract.results.from.model(lmer.stats.3x.us.segm.cont1.2)



# Model including segmented and replicated continuous condition
lmer.stats.3x.us.segm.cont2.1 <- glmer (correct ~ lang*segm + 
                                           (1|subj) + (1|correctItem) + (1|foil),
                                       control=glmerControl(optimizer="bobyqa"),
                                       family="binomial",
                                       data =  dat.stats.london %>% 
                                           filter (experimentID %in%
                                                       c("stats.3x.us.segm",
                                                         "stats.3x.us.cont2")))    
lmer.stats.3x.us.segm.cont2.2 <- update (
    lmer.stats.3x.us.segm.cont2.1,
    ~ . - lang:segm)

lmer.stats.3x.us.segm.cont2.3 <- update (
    lmer.stats.3x.us.segm.cont2.2,
    ~ . - (1|foil))

lmer.stats.3x.us.segm.cont2.4 <- update (
    lmer.stats.3x.us.segm.cont2.3,
    ~ . - (1|correctItem))

# anova (
#     lmer.stats.3x.us.segm.cont2.1,
#     lmer.stats.3x.us.segm.cont2.2,
#     lmer.stats.3x.us.segm.cont2.3,
#     lmer.stats.3x.us.segm.cont2.4
# )


lmer.stats.3x.us.segm.cont2.2.results <- 
    extract.results.from.model(lmer.stats.3x.us.segm.cont2.2)



```

```{r stats-london-stats.3x.us.segm.cont.plot, fig.cap="Results of Experiment 1. Each dot represents a participants. The central red dot is the sample mean; error bars represent standard errors from the mean. The results show the percentage of correct choices in the recognition test after familiarization with (left) a pre-segmented familiarization stream or (middle, right) a continuous familiarization stream. The two continuous conditions are replictions of one another."}

dat.stats.london.m %>% 
    filter (experimentID %in%
                c("stats.3x.us.segm",
                  "stats.3x.us.cont",
                  "stats.3x.us.cont2")) %>% 
    mutate (experimentID = gsub ("stats.3x.us.", "", experimentID)) %>% 
    mutate (experimentID = factor (experimentID, 
                                   levels = c("segm", "cont", "cont2"))) %>% 
    ggplot (aes (x = experimentID,
                 y = 100 * correct)) +
    theme_light(14) +
    theme (axis.title.x = element_blank()) + 
    ylab ("% Correct") + 
    geom_violin() +
    geom_dotplot(binaxis='y', stackdir='center', dotsize=1) +
    stat_summary(fun.data=mean_se, 
                 geom="pointrange", color="red") +
    geom_hline(yintercept=50, linetype = "dotted") +
    scale_x_discrete(labels = c("Pre-segmented", "Continuous (1)", "Continuous (2)"))
    

```

```{r stats-london-stats.us.lang.glmm.print}

pack.index <- c(
    "Pre-segmented familiarization" = 
        nrow (lmer.stats.3x.us.segm.1.results),
    "Continuous familiarization (1)" = 
        nrow (lmer.stats.3x.us.cont.1.results),
    "Continuous familiarization (2)" = 
        nrow (lmer.stats.3x.us.cont2.1.results),
    "Pre-segmented vs. continuous familiarization (1)" = 
        nrow (lmer.stats.3x.us.segm.cont1.2.results),
    "Pre-segmented vs. continuous familiarization (2)" = 
        nrow (lmer.stats.3x.us.segm.cont2.2.results))

pack.index <- pack.index - 1

bind_rows(
    lmer.stats.3x.us.segm.1.results %>% 
        process.glmm.table,
    lmer.stats.3x.us.cont.1.results %>% 
        process.glmm.table,
    lmer.stats.3x.us.cont2.1.results %>% 
        process.glmm.table,
    lmer.stats.3x.us.segm.cont1.2.results %>% 
        process.glmm.table,
    lmer.stats.3x.us.segm.cont2.2.results %>% 
        process.glmm.table
) %>% 
    filter (!grepl ("Intercept", Effect)) %>% 
    kable (caption = "Performance differences across familiarization conditions. The differences were assessed using a generalized linear model for the trial-by-trial data, using participants, correct items and foils as random factors. Random factors were removed from the model when they did not contribute to the model likelihood.",
           booktabs = TRUE, escape = FALSE) %>%
    kableExtra::kable_classic() %>% 
    kableExtra::pack_rows (index = pack.index) 
    # kableExtra::kable_styling(latex_options =
    #                               c("scale_down",
    #                               "hold_position"))


```


`r clearpage()`

## Recall experiment
In the analyses below, we removed all items that contained syllables not attested in the speech stream as it is unclear how these items should be analyzed. As a result, we also removed participants who did not produce any items that contained attested syllables only. 

```{r recall-averages-across-items-calculate, include = FALSE}

if (ANALYZED.DATA.SETS["CITY"]){
 
    dat.recall.city.m <- dat.recall.city %>% 
        mutate (across (starts_with("is_") | starts_with("has_"), as.logical)) %>% 
        # compared to testable below, this doesn't group by lang as everybody has 
        # two languages
        group_by(subj, Age, Gender, streamType, correct_segm) %>%
                summarize (
            n.items = n(),
            n.syll = mean (n_syllables),
            
            # Number and proportion (among all responses) of words
            n.words = sum (is_word),
            p.words = mean (is_word),
            n.words.or.multiple = sum (is_word | is_multiple_words),
            p.words.or.multiple = mean (is_word | is_multiple_words),

            # Number and proportion (among all responses) of part-words
            n.part.words = sum (is_part_word),
            p.part.words = mean (is_part_word),
            n.part.words.or.multiple = sum (is_part_word | is_multiple_part_words),
            p.part.words.or.multiple = mean (is_part_word | is_multiple_part_words),
            
            # Proportion of Words among Words and Part-Words (or multiples thereof)
            p.words.part.words = sum (is_word) / sum (is_word | is_part_word),
            p.words.part.words.or.multiple = sum (is_word | is_multiple_words) / 
                sum (is_word | is_multiple_words | is_part_word | is_multiple_part_words),

            # Number and proportion (among all responses) of high and low TP chunk
            n.high.tp.chunk = sum (is_high_tp_chunk),
            p.high.tp.chunk = mean (is_high_tp_chunk),
            
            n.low.tp.chunk = sum (is_low_tp_chunk),
            p.low.tp.chunk = mean (is_low_tp_chunk),
                        
            # Proportion of high-TP chunks among high and low-TP chunks
            p.high.tp.chunk.low.tp.chunk = sum (is_high_tp_chunk) / 
                sum (is_high_tp_chunk | is_low_tp_chunk),

            # Average forward TPs and difference from expected TP
            average_fw_tp = mean (average_fw_tp, na.rm = TRUE),
            average_fw_tp_d_actual_expected = mean (average_fw_tp - expected_fw_tp, na.rm = TRUE),
            
            average_bw_tp = mean (average_bw_tp, na.rm = TRUE),
            
            # Proportion of items with syllables in correct postions
            p.correct.initial.syll = mean (has_correct_initial_syllable),
            p.correct.final.syll = mean (has_correct_final_syllable),
            p.correct.initial.or.final.syll = mean (has_correct_initial_syllable | has_correct_final_syllable)
        )

    # These are counts that are significantly above zero
    dat.recall.city.m.selected.vars.by.wilcox.df <- 
        dat.recall.city.m %>% 
        group_by(streamType) %>%
        summarize_at (vars(starts_with("n.")), function (X) wilcox.test (X, alternative = "greater")$p.value) %>% 
        remove_rownames %>% 
        column_to_rownames("streamType") %>% 
        t %>% 
        as.data.frame (row.names = row.names(.)) %>% 
        rownames_to_column("var") %>% 
        mutate (use = (continuous <= .05) | (segmented <= .05)) 
    
    dat.recall.city.m.selected.vars.by.wilcox <- dat.recall.city.m.selected.vars.by.wilcox.df %>% 
        filter (use) %>% 
        pull ("var")
    
} 

if (ANALYZED.DATA.SETS["TESTABLE"]){
    dat.recall.tstbl.m <- 
        dat.all.recall.items.tstbl.with.candidates %>% 
        #mutate_at (vars(starts_with("is_"),starts_with("has_")), as.logical) %>%
        mutate (across (starts_with("is_") | starts_with("has_"), as.logical)) %>% 
        # For compatibility with the city data set
        rename (subj = filename,
                Age = age,
                Gender = sex,
                streamType = mySegmentationCond) %>% 
        # We have the following column in the city data set
        group_by(subj, Age, Gender, lang, streamType, correct_segm) %>%
#        summarize_at (vars(n_syllables:average_bw_tp), mean, na.rm = TRUE)
        summarize (
            n.items = n(),
            n.syll = mean (n_syllables),
            
            # Number and proportion (among all responses) of words
            n.words = sum (is_word),
            p.words = mean (is_word),
            n.words.or.multiple = sum (is_word | is_multiple_words),
            p.words.or.multiple = mean (is_word | is_multiple_words),

            # Number and proportion (among all responses) of part-words
            n.part.words = sum (is_part_word),
            p.part.words = mean (is_part_word),
            n.part.words.or.multiple = sum (is_part_word | is_multiple_part_words),
            p.part.words.or.multiple = mean (is_part_word | is_multiple_part_words),
            
            # Proportion of Words among Words and Part-Words (or multiples thereof)
            p.words.part.words = sum (is_word) / sum (is_word | is_part_word),
            p.words.part.words.or.multiple = sum (is_word | is_multiple_words) / 
                sum (is_word | is_multiple_words | is_part_word | is_multiple_part_words),

            # Number and proportion (among all responses) of high and low TP chunk
            n.high.tp.chunk = sum (is_high_tp_chunk),
            p.high.tp.chunk = mean (is_high_tp_chunk),
            
            n.low.tp.chunk = sum (is_low_tp_chunk),
            p.low.tp.chunk = mean (is_low_tp_chunk),
                        
            # Proportion of high-TP chunks among high and low-TP chunks
            p.high.tp.chunk.low.tp.chunk = sum (is_high_tp_chunk) / 
                sum (is_high_tp_chunk | is_low_tp_chunk),

            # Average forward TPs and difference from expected TP
            average_fw_tp = mean (average_fw_tp, na.rm = TRUE),
            average_fw_tp_d_actual_expected = mean (average_fw_tp - expected_fw_tp, na.rm = TRUE),
            
            average_bw_tp = mean (average_bw_tp, na.rm = TRUE),
            
            # Proportion of items with syllables in correct postions
            p.correct.initial.syll = mean (has_correct_initial_syllable),
            p.correct.final.syll = mean (has_correct_final_syllable),
            p.correct.initial.or.final.syll = mean (has_correct_initial_syllable | has_correct_final_syllable)
        )
            
    # These are counts that are significantly above zero
    dat.recall.tstbl.m.selected.vars.by.wilcox.df <- 
        dat.recall.tstbl.m %>% 
        group_by(streamType) %>%
        summarize_at (vars(starts_with("n.")), function (X) wilcox.test (X, alternative = "greater")$p.value) %>% 
        remove_rownames %>% 
        column_to_rownames("streamType") %>% 
        t %>% 
        as.data.frame (row.names = row.names(.)) %>% 
        rownames_to_column("var") %>% 
        mutate (use = (continuous <= .05) | (segmented <= .05)) 
    
    dat.recall.tstbl.m.selected.vars.by.wilcox <- dat.recall.tstbl.m.selected.vars.by.wilcox.df %>% 
        filter (use) %>% 
        pull ("var")    
    
}

```

```{r recall-load-and-assign-column-attributes}

dat.column.meaning <- read.csv("helper/column_meanings.csv",
                               header = TRUE,
                               stringsAsFactors = FALSE,
                               comment.char = "#") %>% 
    column_to_rownames("colName")


# Attributes can be assigned as follows
# data.frame (n.items	= 1:10,
#             n.syll	= 1:10,
#             n.words = 1:10) %>% 
#     add.column.attrib (dat.attrib) %>% 
#     print.column.attrib

```

```{r recall-combine-averages-across-data-sets}
#The testable df has a language column the city one doesn't have
# name.comp <- cbind.na (sort (names (dat.recall.city.m)),
#           sort (names (dat.recall.tstbl.m))) %>% 
#     data.frame 
# save.data.frame(name.comp)

dat.recall.combined.m <- list ()
dat.recall.combined.m.selected.vars.by.wilcox <- list ()

if (ANALYZED.DATA.SETS["CITY"]){
    dat.recall.combined.m <- c(dat.recall.combined.m,
                               list (dat.recall.city.m %>% 
                                         add_column (
                                             data.set = "city", 
                                             .before = 1) %>% 
                                         add_column(
                                             lang = NA,
                                             .after = "Gender")))    
    
    dat.recall.combined.m.selected.vars.by.wilcox <- c(
        dat.recall.combined.m.selected.vars.by.wilcox,
        list (city = dat.recall.city.m.selected.vars.by.wilcox)
    )
}

if (ANALYZED.DATA.SETS["TESTABLE"]){
    dat.recall.combined.m <- c(dat.recall.combined.m,
                               list (dat.recall.tstbl.m %>% 
                                         add_column(
                                             data.set = "testable",
                                             .before  = 1)))
    
    dat.recall.combined.m.selected.vars.by.wilcox <- c(
        dat.recall.combined.m.selected.vars.by.wilcox,
        list (testable = dat.recall.tstbl.m.selected.vars.by.wilcox))
    
}

dat.recall.combined.m <- bind_rows (dat.recall.combined.m) %>% 
    as.data.frame %>% 
    mutate (data.set = revalue (data.set,
                                c(city = "lab-based",
                                testable = "online"))) %>% 
    add.column.attrib (dat.column.meaning) 


```

```{r recall-print-used-column-attributes}
dat.recall.combined.m %>% 
    print.column.attrib %>% 
    filter (!is.na (meaning)) %>% 
    kable (caption="Analyses performed for the vocalizations", 
           booktabs = TRUE) %>% 
    kable_styling(latex_options =
                              c("hold_position", 
                                "scale_down"),
                  bootstrap_options = "striped") %>% 
    kableExtra::column_spec(2, width = "30em") %>% 
    kableExtra::kable_classic()

```

After computing these counts and averages, we asked which counts were significantly different from zero in a one-tailed Wilcoxon test, either for the continuous or the segmented condition. These counts were `r toString (dat.recall.tstbl.m.selected.vars.by.wilcox)`. (Note: These counts are currently restricted to the testable data set.)


As shown in Table \ref{tab:recall-all-results-print}, participants produced on average `r  dat.recall.combined.m %>% filter (streamType == "segmented") %>% pull (n.words) %>% mean` words in the segmented condition, and `r dat.recall.combined.m %>% filter (streamType == "continuous") %>% pull (n.words) %>% mean` in the continuous condition.


### General measures
We first calculate the number of items produced as well their average, and compare them  against the zero as well as across segmentation conditions. As shown in Table \ref{tab:recall-all-results-print} and Figures \ref{fig:recall-general-measures-tp-plot}a and b, participants produced positive number of items. Neither the number of items produced nor their number of syllables differed across the segmentation conditions.

```{r recall-streamTypeContrast-functions}
prepare.data.for.streamType.contrast <- function (dat = ., value.var, mu = 0) {
    
    dat %>%     
        mutate (paired = (data.set!="online")) %>% 
        reshape2::dcast (data.set + subj + paired ~ streamType, value.var = value.var) %>% 
        group_by (data.set) %>%  
        summarize (Continuous = wilcox.p(continuous, mu, TRUE),
                   Segmented = wilcox.p(segmented, mu, TRUE),
                   d = wilcox.test(continuous, segmented,
                                   mu = 0,
                                   paired = paired)$p.value %>% 
                       signif (3)
        )
    
}

prepare.plot.for.streamType.contrast <- function (dat = ., x, y) {
    
 dat %>% 
    ggplot (aes (x= {{x}}, y = {{y}})) + 
    # geom_boxplot (alpha=.5, fill="lightblue", outlier.shape = NA) + 
    geom_dotplot(binaxis = "y", stackdir = "center") +
    geom_violin(alpha = 0, 
                fill = "#5588CC", col="#5588CC") +
    stat_summary(fun.data=mean_sdl, 
                 fun.args = list (mult=1), 
                 geom="pointrange", color="#cc556f") + 
    facet_grid(data.set ~ ., scales = "free") +
    theme_light (16) +
    theme (axis.title.x = element_blank()) 
    
}


```

```{r recall-n-items-produced-calculate}
# Print later in common table with length
dat.recall.combined.m.n.items <- dat.recall.combined.m %>% 
    prepare.data.for.streamType.contrast("n.items")
    
    
# Plot later together with length
plot.recall.combined.m.n.items <- dat.recall.combined.m %>% 
    prepare.plot.for.streamType.contrast (streamType, n.items) +
    ylab ("# items")

    

```



```{r recall-length-items-produced-calculate}

# Print later in common table with number of items
dat.recall.combined.m.n.syll <- dat.recall.combined.m %>% 
    prepare.data.for.streamType.contrast("n.syll")
    
# Plot later together with number of items
plot.recall.combined.m.n.syll <- dat.recall.combined.m %>% 
    prepare.plot.for.streamType.contrast (streamType, n.syll) +
    ylab ("# syllables/item")

    


```



```{r recall-general-measures-plot, eval = FALSE, fig.cap="Number of items produced as well as their numbers of syllables."}
ggpubr::ggarrange (
    plot.recall.combined.m.n.items,
    plot.recall.combined.m.n.syll,
    nrow = 1, ncol = 2,
    #common.legend = TRUE, legend="bottom", 
    #legend.grob = plot.circle.combined.within.recency.legend,
    labels = "auto")

```


### TP-based analyses
We first computed the average forward TPs in the produced items, and separately for each segmentation condition, compared it to both the expected TPs for random strings and the expected TPs given the starting syllable.

* The expected TPs for items of at least 2 syllables starting on an initial syllable are c(1, 1/3, 1, 1, 1/3, 1, 1, 1/3, ...). The difference between the actual and the expected TP needs to be compared to zero, as the expected TP differs across items.
* The expected TPs for a random concatenation are the TPs in a random bigram. For an A or a B syllable, the random TP is 1 $\times$ 1 / 12, as there is only 1 (out of 12) non-zero TP continuations. For a C syllable, the random TP is 3 $\times$ 1/3 / 12, as there are 3 possible concatenations. On average, the random TP is thus $(1/12 + 1/12 + 1/12)/ 3 = 1/12 \approx .083$.

We compared these measures across segmentation conditions. 


As shown in Table \ref{tab:recall-all-results-print} and Figures \ref{fig:recall-general-measures-tp-plot}c and d, forward and backward TPs were significantly greater than expected for a random string in both the continuous and the segmented condition, with greater TPs in the segmented conditions. However, they were significantly *lower* than the TPs expected if items recalled faithfully, given their starting position.

As shown in Figure \ref{fig:recall-w-pw-chunks-positions-plot}b, participants produced a positive number of high-TP chunks in both the segmented and the continuous condition, with a significantly greater number in the segmented condition. In contrast, they produced a positive number of low-TP chunks only in the continuous condition. Accordingly, the proportion of high-TP chunks among high- and low-TP chunks exceeded 50% only in the segmented condition. 


```{r recall-forward-tps-calculate}
dat.recall.combined.m.forward.tps <- dat.recall.combined.m %>% 
    # This is the TP for a random string
    prepare.data.for.streamType.contrast("average_fw_tp", 1/12)

dat.recall.combined.m.forward.tps.actual.vs.expected <- dat.recall.combined.m %>% 
    prepare.data.for.streamType.contrast("average_fw_tp_d_actual_expected", 0)



plot.recall.combined.m.forward.tps <- dat.recall.combined.m %>% 
    prepare.plot.for.streamType.contrast (streamType, average_fw_tp) +
    ylab ("Forward TPs") +
    geom_hline (yintercept = 1/12, lty = 3)
    
plot.recall.combined.m.forward.vs.expected.tps <- dat.recall.combined.m %>% 
    prepare.plot.for.streamType.contrast (streamType, average_fw_tp_d_actual_expected) +
    ylab ("Actual - Expected Forward TPs")
    


```


```{r recall-backward-tps-calculate}
dat.recall.combined.m.backward.tps <- dat.recall.combined.m %>% 
    # This is the TP for a random string
    prepare.data.for.streamType.contrast("average_bw_tp", 1/12)


plot.recall.combined.m.backward.tps <- dat.recall.combined.m %>% 
    prepare.plot.for.streamType.contrast (streamType, average_bw_tp) +
    ylab ("Backward TPs") +
    geom_hline (yintercept = 1/12, lty = 3)
    
```


```{r recall-tps-plot, eval = FALSE, fig.height=11, fig.cap="Plot of TP comparisons."}
ggpubr::ggarrange (
plot.recall.combined.m.forward.tps,
plot.recall.combined.m.forward.tps,
plot.recall.combined.m.backward.tps,
    nrow = 3, ncol = 1,
    #common.legend = TRUE, legend="bottom", 
    #legend.grob = plot.circle.combined.within.recency.legend,
    labels = "auto")

```



```{r recall-tp-chunks-calculate}

dat.recall.combined.m.n.high.tp.chunk <- dat.recall.combined.m %>% 
    prepare.data.for.streamType.contrast("n.high.tp.chunk", 0)
dat.recall.combined.m.p.high.tp.chunk <- dat.recall.combined.m %>% 
    prepare.data.for.streamType.contrast("p.high.tp.chunk", 0)

dat.recall.combined.m.n.low.tp.chunk <- dat.recall.combined.m %>% 
    prepare.data.for.streamType.contrast("n.low.tp.chunk", 0)
dat.recall.combined.m.p.low.tp.chunk <- dat.recall.combined.m %>% 
    prepare.data.for.streamType.contrast("p.low.tp.chunk", 0)

dat.recall.combined.m.p.high.tp.chunk.low.tp.chunk <- dat.recall.combined.m %>% 
    prepare.data.for.streamType.contrast("p.high.tp.chunk.low.tp.chunk", c(0.5, 2/3))


plot.recall.combined.m.n.high.tp.chunk <- dat.recall.combined.m %>% 
    prepare.plot.for.streamType.contrast (streamType, n.high.tp.chunk) +
    ylab ("# High TP Chunks")
plot.recall.combined.m.p.high.tp.chunk <- dat.recall.combined.m %>% 
    prepare.plot.for.streamType.contrast (streamType, p.high.tp.chunk) +
    ylab (TeX("\\frac{High TP chunks}{Productions}$"))

plot.recall.combined.m.n.low.tp.chunk <- dat.recall.combined.m %>% 
    prepare.plot.for.streamType.contrast (streamType, n.low.tp.chunk) +
    ylab ("# Low TP Chunks")
plot.recall.combined.m.p.low.tp.chunk <- dat.recall.combined.m %>% 
    prepare.plot.for.streamType.contrast (streamType, p.low.tp.chunk) +
    ylab (TeX("\\frac{Low TP chunks}{Productions}$"))

plot.recall.combined.m.p.high.tp.chunk.low.tp.chunk <- dat.recall.combined.m %>% 
    prepare.plot.for.streamType.contrast (streamType, p.high.tp.chunk.low.tp.chunk) +
    ylab (TeX("\\frac{High TP chunks}{High + Low TP chunks}$")) +
    geom_hline (yintercept = 1/2, lty = 3) +
    geom_hline (yintercept = 2/3, lty = 5)


```

```{r recall-tp-chunks-plot, eval = FALSE, fig.height=11, fig.cap="Plot of High and Low TP chunks."}
ggpubr::ggarrange (
plot.recall.combined.m.n.high.tp.chunk,
plot.recall.combined.m.p.high.tp.chunk,
plot.recall.combined.m.n.low.tp.chunk,
plot.recall.combined.m.p.low.tp.chunk,
plot.recall.combined.m.p.high.tp.chunk.low.tp.chunk,
    nrow = 3, ncol = 2,
    #common.legend = TRUE, legend="bottom", 
    #legend.grob = plot.circle.combined.within.recency.legend,
    labels = "auto")

```


### Word vs. part-word analysis
We next calculate the number and proportion of among (productions) of words and part-words respectively; we also accept concatenations of words and part-words. The proportions will be compared across stream types as well as to zero.

Finally, we calculate the proportion of words among the word and part-word productions. This proportion will be compared across segmentation types, as well as to the chance level of 50%.

As shown in Table \ref{tab:recall-all-results-print} and in Figure \ref{fig:recall-w-pw-chunks-positions-plot}a, participants produced a positive number of words only in the segmented condition, but not in the continuous condition. In contrast, they produced a positive number of part-words only in the continuous condition, but not in the segmented condition. Accordingly, the proportion of words among words and part-words was significantly greater than 50% in the segmented condition, but numerically (though not significantly) smaller than 50% in the continuous condition. The latter result is consistent with participants randomly picking a syllable to start their vocalizations; if so, part-words should be 2 times as likely as words.

```{r recall-p-n-words-calculate}
dat.recall.combined.m.n.words <- dat.recall.combined.m %>% 
        prepare.data.for.streamType.contrast("n.words.or.multiple")
dat.recall.combined.m.p.words <- dat.recall.combined.m %>% 
        prepare.data.for.streamType.contrast("p.words.or.multiple")

plot.recall.combined.m.n.words <- dat.recall.combined.m %>% 
        prepare.plot.for.streamType.contrast (streamType, n.words.or.multiple) +
        ylab ("# Words")

plot.recall.combined.m.p.words <- dat.recall.combined.m %>% 
    prepare.plot.for.streamType.contrast (streamType, p.words.or.multiple) +
    ylab (TeX("\\frac{Words}{Productions}$"))
```

```{r recall-p-n-part-words-calculate}
dat.recall.combined.m.n.part.words <- dat.recall.combined.m %>% 
        prepare.data.for.streamType.contrast("n.part.words.or.multiple")
dat.recall.combined.m.p.part.words <- dat.recall.combined.m %>% 
        prepare.data.for.streamType.contrast("p.part.words.or.multiple")

plot.recall.combined.m.n.part.words <- dat.recall.combined.m %>% 
        prepare.plot.for.streamType.contrast (streamType, n.part.words.or.multiple) +
        ylab ("# Part-Words")

plot.recall.combined.m.p.part.words <- dat.recall.combined.m %>% 
    prepare.plot.for.streamType.contrast (streamType, p.part.words.or.multiple) +
    ylab (TeX("\\frac{Part-Words}{Productions}$"))
```

```{r recall-words-part-words-calculate}
dat.recall.combined.m.p.words.part.words <- dat.recall.combined.m %>% 
        prepare.data.for.streamType.contrast("p.words.part.words.or.multiple", c(.5, 1/3))


plot.recall.combined.m.p.word.part.words <- dat.recall.combined.m %>% 
    prepare.plot.for.streamType.contrast (streamType, p.words.part.words.or.multiple) +
    ylab (TeX("\\frac{Words}{Words + Part-Words}$")) + 
    geom_hline (yintercept = 1/2, lty = 3) + 
    geom_hline (yintercept = 1/3, lty = 5)

    
```



```{r recall-words-part-words-plot, eval = FALSE, fig.height=11, fig.cap="Plot of various comparisons between words and part-words."}
ggpubr::ggarrange (
    plot.recall.combined.m.n.words,
    plot.recall.combined.m.p.words,
    plot.recall.combined.m.n.part.words,
    plot.recall.combined.m.p.part.words,
    plot.recall.combined.m.p.word.part.words,
    nrow = 3, ncol = 2,
    #common.legend = TRUE, legend="bottom", 
    #legend.grob = plot.circle.combined.within.recency.legend,
    labels = "auto")

```


### Positional analyses
Finally, we analyze the productions in terms of correct initial final positions. As there are four initial and final positions, respectively, 4/12 of the productions should have "correct" initial positions, 4/12 should have correct final positions, while $2 \times 4/12 - (4/12)^2 = 5/9$  should have either correct initial or final positions.

As shown in Table \ref{tab:recall-all-results-print} and Figure \ref{fig:recall-w-pw-chunks-positions-plot}c and d, participants produced items with correct initial or final positions at great than chance level only in the segmented condition, but not the continuous condition.

```{r recall-positions-calculate}
dat.recall.combined.m.p.correct.initial.syll <- dat.recall.combined.m %>% 
    prepare.data.for.streamType.contrast("p.correct.initial.syll", 4/12)
dat.recall.combined.m.p.correct.final.syll <- dat.recall.combined.m %>% 
    prepare.data.for.streamType.contrast("p.correct.final.syll", 4/12)
dat.recall.combined.m.p.correct.initial.or.final.syll <- dat.recall.combined.m %>% 
    prepare.data.for.streamType.contrast("p.correct.initial.or.final.syll", 5/9)




plot.recall.combined.m.p.correct.initial.syll <- dat.recall.combined.m %>% 
    prepare.plot.for.streamType.contrast (streamType, p.correct.initial.syll) +
    ylab (TeX("\\frac{# Correct Initial Syllables}{Productions}$")) + 
    geom_hline (yintercept = 4/12, lty = 3)

plot.recall.combined.m.p.correct.final.syll <- dat.recall.combined.m %>% 
    prepare.plot.for.streamType.contrast (streamType, p.correct.final.syll) +
    ylab (TeX("\\frac{# Correct Final Syllables}{Productions}$")) +
    geom_hline (yintercept = 4/12, lty = 3)


plot.recall.combined.m.p.correct.initial.or.final.syll <- dat.recall.combined.m %>% 
    prepare.plot.for.streamType.contrast (streamType, p.correct.initial.or.final.syll) +
    ylab (TeX("\\frac{# Correct Initial Final Syllables}{Productions}$")) + 
    geom_hline (yintercept = 5/9, lty = 3)



```

```{r recall-positions-plot, eval = FALSE, fig.height=11, fig.cap="Plot of chunks with correct initial or final positions."}
ggpubr::ggarrange (
plot.recall.combined.m.p.correct.initial.syll,
plot.recall.combined.m.p.correct.final.syll,
plot.recall.combined.m.p.correct.initial.or.final.syll,
    nrow = 3, ncol = 1,
    #common.legend = TRUE, legend="bottom", 
    #legend.grob = plot.circle.combined.within.recency.legend,
    labels = "auto")

```


```{r recall-all-results-prepare}
# Analyses that are commented out are given in the Appendix

dat.recall.combined.m.all.results.main <-
    bind_rows(
        dat.recall.combined.m %>% 
            prepare.data.for.streamType.contrast("correct_segm", .5) %>% 
            mutate (filter = "Recognition accuracy"),
        
        dat.recall.combined.m.n.items %>% 
            mutate (filter = "Number of items"),
        dat.recall.combined.m.n.syll %>% 
            mutate (filter = "Number of syllables/item"),
        
        # dat.recall.combined.m.n.words %>% 
        #     mutate (filter = "Number of words"),
        # dat.recall.combined.m.n.words %>% 
        #     mutate (filter = "Proportion of words among productions"),
        # dat.recall.combined.m.n.part.words %>% 
        #     mutate (filter = "Number of part-words"),
        # dat.recall.combined.m.n.part.words %>% 
        #     mutate (filter = "Proportion of part-words among productions"),
        dat.recall.combined.m.p.words.part.words %>% 
            mutate (filter = "Proportion of words among words and part-words (or concatenations thereof)" ),
        
        dat.recall.combined.m.forward.tps %>% 
            mutate (filter = "Forward TPs"),
        # dat.recall.combined.m.forward.tps.actual.vs.expected %>% 
        #     mutate (filter = "Actual vs. expected forward TPs"),
        dat.recall.combined.m.backward.tps %>% 
            mutate (filter = "Backward TPs"),
        
        # dat.recall.combined.m.n.high.tp.chunk %>% 
        #     mutate (filter = "Number of High-TP chunks"),
        # dat.recall.combined.m.p.high.tp.chunk %>% 
        #     mutate (filter = "Proportion of High-TP chunks among productions"),
        # dat.recall.combined.m.n.low.tp.chunk %>% 
        #     mutate (filter = "Number of Low-TP chunks"),
        # dat.recall.combined.m.p.low.tp.chunk %>% 
        #     mutate (filter = "Number of Low-TP chunks among productions"),
        
        dat.recall.combined.m.p.high.tp.chunk.low.tp.chunk %>% 
            mutate (filter = "Proportion of High-TP chunks among High- and Low-TP chunks"),
        
        
        dat.recall.combined.m.p.correct.initial.syll %>% 
            mutate (filter = "Proportion of items with correct initial syllables"),
        dat.recall.combined.m.p.correct.final.syll %>% 
            mutate (filter = "Proportion of items with correct final syllables")
        # dat.recall.combined.m.p.correct.initial.or.final.syll %>% 
        #     mutate (filter = "Proportion of items with correct initial or final syllables")
   
    )


```

```{r recall-all-results-print}

dat.recall.combined.m.all.results.main %>% 
    dplyr::select (-c("filter")) %>% 
    kable (caption = "Various analyses pertaining to the productions as well as test against their chances levels. Number of items produced, their numbers of syllables, number of words, number of part-words (chance level: 0), proportion  of words among productions, proportion of part-words among productions, proportion of words among words and part-words (chance level 50%), average forward TPs (chance level: 1/12), difference between positionally expected and actual TPs, average backward TPS. CHUNKS ", 
           col.names = c("", "Continuous", "Segmented", "*p* (Continuous vs. Segmented)"),
           booktabs = TRUE, escape = FALSE) %>%
    kable_styling(latex_options =
                      c("scale_down")) %>%
    kableExtra::column_spec(2:3, width = "30em") %>%
    kableExtra::column_spec(4, width = "10em") %>% 
    kableExtra::kable_classic() %>% 
    pack_rows(index = make.pack.index (dat.recall.combined.m.all.results.main$filter))

```



```{r recall-general-measures-tp-plot, fig.height=8, fig.cap="Number of items produced, number of syllables per item and forward and backward TPs. The dotted line represents the chance level for a randomly ordered syllable sequence."}
ggpubr::ggarrange (
    plot.recall.combined.m.n.items,
    plot.recall.combined.m.n.syll,
    plot.recall.combined.m.forward.tps,
    plot.recall.combined.m.backward.tps,
    nrow = 2, ncol = 2,
    #common.legend = TRUE, legend="bottom", 
    #legend.grob = plot.circle.combined.within.recency.legend,
    labels = "auto")

```

```{r recall-w-pw-chunks-positions-plot, fig.height=8, fig.cap="Analyses of the participants' productions. (a) Proportion of words among words and part-words. The dotted line represents the chance level of 50% in a two-alternative forced-choice task, while the dashed line represents the chance level of 33% that an attested 3 syllable-chunk is a word rather than a part-word. (b) Proportion of high-TP chunks among high- and low-TP chunks. The dashed line represents the chance level of 66% that an attested 2 syllable-chunk is a high-TP rather than a low-TP chunk. (c) proportion of productions with correct initial syllables and (d) with correct final syllables. The dotted line represents the chance level of 33%."}
ggpubr::ggarrange (
    plot.recall.combined.m.p.word.part.words,
    plot.recall.combined.m.p.high.tp.chunk.low.tp.chunk,
    plot.recall.combined.m.p.correct.initial.syll,
    plot.recall.combined.m.p.correct.final.syll,
    nrow = 2, ncol = 2,
    #common.legend = TRUE, legend="bottom", 
    #legend.grob = plot.circle.combined.within.recency.legend,
    labels = "auto")


```

```{r recall-save-data2}
xlsx::write.xlsx (dat.recall.city,
                  file="output/segmentation_recall_transcriptions_output.xlsx",
                  row.names=FALSE,
                  sheetName="complete (city)",
                  append=FALSE)

xlsx::write.xlsx (dat.recall.tstbl,
                  file="output/segmentation_recall_transcriptions_output.xlsx",
                  row.names=FALSE,
                  sheetName="complete (tstbl)",
                  append=TRUE)


xlsx::write.xlsx (dat.recall.combined.m,
                  file="output/segmentation_recall_transcriptions_output.xlsx",
                  row.names=FALSE,
                  sheetName="means",
                  append=TRUE)



```

`r clearpage()`

# Appendix
## Additional results for the recall experiments

```{r recall-extra-results-prepare}
# Results that are commented out are in the main text

dat.recall.combined.m.all.results.extra <-
    bind_rows(
        # dat.recall.combined.m %>% 
        #     prepare.data.for.streamType.contrast("correct_segm", .5) %>% 
        #     mutate (filter = "Recognition accuracy"),
        # 
        # dat.recall.combined.m.n.items %>% 
        #     mutate (filter = "Number of items"),
        # dat.recall.combined.m.n.syll %>% 
        #     mutate (filter = "Number of syllables/item"),
        
        dat.recall.combined.m.n.words %>%
            mutate (filter = "Number of words"),
        dat.recall.combined.m.n.words %>%
            mutate (filter = "Proportion of words among productions"),
        dat.recall.combined.m.n.part.words %>%
            mutate (filter = "Number of part-words"),
        dat.recall.combined.m.n.part.words %>%
            mutate (filter = "Proportion of part-words among productions"),
        # dat.recall.combined.m.p.words.part.words %>% 
        #     mutate (filter = "Proportion of words among words and part-words (or concatenations thereof)" ),
        # 
        # dat.recall.combined.m.forward.tps %>% 
        #     mutate (filter = "Forward TPs"),
        dat.recall.combined.m.forward.tps.actual.vs.expected %>%
            mutate (filter = "Actual vs. expected forward TPs"),
        # dat.recall.combined.m.backward.tps %>% 
        #     mutate (filter = "Backward TPs"),
        
        dat.recall.combined.m.n.high.tp.chunk %>%
            mutate (filter = "Number of High-TP chunks"),
        dat.recall.combined.m.p.high.tp.chunk %>%
            mutate (filter = "Proportion of High-TP chunks among productions"),
        dat.recall.combined.m.n.low.tp.chunk %>%
            mutate (filter = "Number of Low-TP chunks"),
        dat.recall.combined.m.p.low.tp.chunk %>%
            mutate (filter = "Number of Low-TP chunks among productions")
        
        # dat.recall.combined.m.p.high.tp.chunk.low.tp.chunk %>% 
        #     mutate (filter = "Proportion of High-TP chunks among High- and Low-TP chunks"),
        
        
        # dat.recall.combined.m.p.correct.initial.syll %>% 
        #     mutate (filter = "Proportion of items with correct initial syllables"),
        # dat.recall.combined.m.p.correct.final.syll %>% 
        #     mutate (filter = "Proportion of items with correct final syllables")
        # # dat.recall.combined.m.p.correct.initial.or.final.syll %>% 
        # #     mutate (filter = "Proportion of items with correct initial or final syllables")
   
    )


```

```{r recall-extra-results-print}

dat.recall.combined.m.all.results.extra %>% 
    dplyr::select (-c("filter")) %>% 
    kable (caption = "Various supplementary analyses pertaining to the productions as well as test against their chances levels. Number of items produced, their numbers of syllables, number of words, number of part-words (chance level: 0), proportion  of words among productions, proportion of part-words among productions, proportion of words among words and part-words (chance level 50%), average forward TPs (chance level: 1/12), difference between positionally expected and actual TPs, average backward TPS. CHUNKS ", 
           col.names = c("", "Continuous", "Segmented", "*p* (Continuous vs. Segmented)"),
           booktabs = TRUE, escape = FALSE) %>%
    kable_styling(latex_options =
                      c("scale_down")) %>%
    kableExtra::column_spec(2:3, width = "30em") %>%
    kableExtra::column_spec(4, width = "10em") %>% 
    kableExtra::kable_classic() %>% 
    pack_rows(index = make.pack.index (dat.recall.combined.m.all.results.extra$filter))

```



```{r recall-words-part-words-raw-plot, fig.height=8, fig.cap="Number and proportion (among vocalizations) of words and part-words."}
ggpubr::ggarrange (
    plot.recall.combined.m.n.words,
    plot.recall.combined.m.p.words,
    plot.recall.combined.m.n.part.words,
    plot.recall.combined.m.p.part.words,
    nrow = 3, ncol = 2,
    #common.legend = TRUE, legend="bottom", 
    #legend.grob = plot.circle.combined.within.recency.legend,
    labels = "auto")

```



```{r recall-tp-chunks-raw-plot, fig.height=8, fig.cap="Plot of High and Low TP chunks."}
ggpubr::ggarrange (
plot.recall.combined.m.n.high.tp.chunk,
plot.recall.combined.m.p.high.tp.chunk,
plot.recall.combined.m.n.low.tp.chunk,
plot.recall.combined.m.p.low.tp.chunk,
    nrow = 3, ncol = 2,
    #common.legend = TRUE, legend="bottom", 
    #legend.grob = plot.circle.combined.within.recency.legend,
    labels = "auto")

```

`r clearpage()`


## Experiments with the *en1* diphone base
### Segmented stream, 3 repetitions of the stream, en1 diphone based

```{r stats-london-stats.3x.en.segm.ana}

ana.stats.3x.en.segm <- dat.stats.london.m %>% 
    analyze.experiment.against.chance ("stats.3x.en.segm")
    
lmer.stats.3x.en.segm.1 <- glmer (correct ~ lang + 
                                         (1|subj) + (1|correctItem) + (1|foil),
                                  control=glmerControl(optimizer="bobyqa"),
                                  family="binomial",
                                  data =  dat.stats.london %>% 
                                      filter (experimentID == "stats.3x.en.segm")
                                  )    

lmer.stats.3x.en.segm.2 <- update (
    lmer.stats.3x.en.segm.1,
    ~ . - (1|foil))

lmer.stats.3x.en.segm.3 <- update (
    lmer.stats.3x.en.segm.2,
    ~ . - (1|correctItem))

# anova (lmer.stats.3x.en.segm.1,
#        lmer.stats.3x.en.segm.2,
#        lmer.stats.3x.en.segm.3)


lmer.stats.3x.en.segm.3.results <- extract.results.from.model (lmer.stats.3x.en.segm.3)

# T test and likelhood ratio
ana.stats.3x.en.segm <- dat.stats.london.m %>% 
    filter (experimentID == "stats.3x.en.segm") %>% 
    mutate (correct = 100 * correct) %>% 
    pull (correct) %>% 
    list (llr = lik.ratio (., 50),
          tt = tt4 (., 50, print.results = FALSE))

```


```{r stats-london-stats.3x.en.segm.plot-old, eval = FALSE, fig.cap="Results for a segmented presentation of the stream (540 ms silences) with three repetition of the stream (45 repetitions per word). The diphone based was *en1*."}



#current.plot.name <- "stats.3x.en.segm"
#prepare.graphics


dat.stats.3x.en.segm.for.plot <- dat.stats.london.m %>% 
     global.df.to.plot.df ("stats.3x.en.segm")
    

strip4c (100*dat.stats.3x.en.segm.for.plot,  
         x=1:2,
         ylab="% Correct",
         xlab_big=names (dat.stats.3x.en.segm.for.plot),
         xlab_big_at=c(1:2),
         main="Segmented - 3 presentation of stream \nen1 [e1c] ")
#show.graphics

```

```{r stats-london-stats.3x.en.segm-cont.plot, fig.cap="Results for a pre-segmented presentation of the stream (540 ms silences, left) and continuous presentation of the stream (right). Each word was repeated 45 times. The diphone based was *en1*."}

dat.stats.london.m %>% 
    filter (experimentID %in% c("stats.3x.en.segm", "stats.3x.en.cont")) %>% 
    mutate (segm = factor (segm, 
                           levels = segm %>% 
                               levels %>% 
                               rev)) %>%
    ggplot (aes (x = lang,
                 y = 100 * correct)) +
    theme_light(14) +
    theme (axis.title.x = element_blank()) +
    ylab ("% Correct") +
    ggtitle ("Experiments with the en1 diphone base") +
    geom_violin() +
    geom_dotplot(binaxis='y', stackdir='center', dotsize=1) +
    stat_summary(fun.data=mean_se,
                 geom="pointrange", color="red") +
    geom_hline(yintercept=50, linetype="dashed") +
    facet_grid(. ~ segm, labeller = labeller (segm = stringi::stri_trans_totitle))
    
```

As shown in Figure \ref{fig:stats-london-stats.3x.en.segm-cont.plot}, the average performance did not differ significantly from the chance level of 50%, `r ana.stats.3x.en.segm$tt`, `r ana.stats.3x.en.segm$wt`. Likelihood ratio analysis favored the null hypothesis by a factor of `r ana.stats.3x.en.segm$llr` after correction with the Bayesian Information Criterion. Further, as shown in Table \ref{tab:stats.en.lang.glmm}, performance did not depend on the language condition.

### Continuous stream, 3 repetitions of the stream, en1 diphone based
```{r stats-london-stats.3x.en.cont.ana}

ana.stats.3x.en.cont <- dat.stats.london.m %>% 
    analyze.experiment.against.chance ("stats.3x.en.cont")
    
lmer.stats.3x.en.cont.1 <- glmer (correct ~ lang + 
                                         (1|subj) + (1|correctItem) + (1|foil),
                                  control=glmerControl(optimizer="bobyqa"),
                                  family="binomial",
                                  data =  dat.stats.london %>% 
                                      filter (experimentID == "stats.3x.en.cont")
                                  )    

lmer.stats.3x.en.cont.2 <- update (
    lmer.stats.3x.en.cont.1,
    ~ . - (1|foil))

lmer.stats.3x.en.cont.3 <- update (
    lmer.stats.3x.en.cont.2,
    ~ . - (1|correctItem))

# anova (lmer.stats.3x.en.cont.1,
#        lmer.stats.3x.en.cont.2,
#        lmer.stats.3x.en.cont.3)

lmer.stats.3x.en.cont.1.results <- 
    extract.results.from.model (lmer.stats.3x.en.cont.1)


```

```{r stats-london-stats.3x.en.cont.plot-old, eval = FALSE, fig.cap="Results for a continous presentation of the stream (540 ms silences) with three repetition of the stream (45 repetitions per word). The diphone based was *en1*."}



#current.plot.name <- "stats.3x.en.cont"
#prepare.graphics



dat.stats.3x.en.cont.for.plot <- dat.stats.london.m %>% 
    global.df.to.plot.df ("stats.3x.en.cont")
    
    

strip4c (100*dat.stats.3x.en.cont.for.plot,  
         x=1:2,
         ylab="% Correct",
         xlab_big=names (dat.stats.3x.en.cont.for.plot),
         xlab_big_at=c(1:2),
         main="Continuous - 3 presentation of stream")
#show.graphics

```

As shown in Figure \ref{fig:stats-london-stats.3x.en.segm-cont.plot}, the average performance did not differ significantly from the chance level of 50%, `r ana.stats.3x.en.cont$tt`, `r ana.stats.3x.en.cont$wt`. Likelihood analyses revealed that the null hypothesis was `r ana.stats.3x.en.cont$llr` than the alternative hypothesis after a correction with the Bayesian Information Criterion. However, as shown in Table \ref{tab:stats.en.lang.glmm}, performance was much better for Language 1 than for Language 2, presumably due to some click-like sounds the synthesizer produced for some stops and fricatives (notably /f/ and /g/). These sound might have prevent participants from using statistical learning. 


```{r stats-london-stats.en.lang.glmm.print}

pack.index <- c(
    stats.3x.en.segm = nrow (lmer.stats.3x.en.segm.3.results),
    stats.3x.en.cont = nrow (lmer.stats.3x.en.cont.1.results)
    )

pack.index <- pack.index - 1
    
bind_rows(
    lmer.stats.3x.en.segm.3.results %>% 
        process.glmm.table,
    lmer.stats.3x.en.cont.1.results %>% 
        process.glmm.table
) %>% 
    filter (!grepl ("Intercept", Effect)) %>% 
    kable (caption = "\\label{tab:stats.en.lang.glmm}Performance differences across language conditions. The differences were assessed using a generalized linear model for the trial-by-trial data, using participants, correct items and foils as random factors. Random factors were removed from the model when they did not contribute to the model likelihood",
           booktabs = TRUE, escape = FALSE) %>% 
    kableExtra::pack_rows (index = pack.index) 
    # kableExtra::kable_styling(latex_options =
    #                               c("scale_down",
    #                               "hold_position"))


```

## Pilot recognition experiment testing the use of chunk frequency
In Pilot Experiment 1, we asked if participants could break up tri-syllabic items by using the chunk frequency of sub-chunks. The artificial languages were designed such that, in a trisyllabic item such as *ABC*, chunk frequency (and backwards TPs) favor in the initial *AB* chunk for half of the participants, and the final *BC* chunk for the other participants. 

Across participants, we also varied the exposure to the languages, with 3, 15 or 30 repetitions per word, respectively. 

### Methods 

```{r bcn-helper-functions}

# Libraries to load the experyment data
source ("~/R.ansgar/expyriment_data_ade.R")


# Some random helper functions

f1 <- function (x){
    with (x,
          tt4(x$cor, .5))
}

f2 <- function (x){
    
    with(x,
         reportAOV(summary (aov (cor~lang))))
}


make.figure.from.mean <- function (df, lang.col="lang", data.col="correct"){

    languages <- levels (df[,lang.col])
    exp.name <- deparse (substitute(df))
    
    tmp <- make.matrix.for.plot (list (df[df[,lang.col]==languages[1],],
                                       df[df[,lang.col]==languages[2],]),
                                 data.col,
                                 df=T)
    names (tmp) <- languages


    pdf (paste (exp.name, ".pdf", sep=""))
    strip4c (100*tmp, x=c(1:2),
             ylab="% Correct",
             xlab_big=names (tmp),
             xlab_big_at=c(1:2),
             main=exp.name)
    dev.off ()
}
```

```{r bcn-load-data-stats, include = FALSE}

#system ("./preProcessAll.sh res.exp1")
dat.bcn.exp1.3x <- read.files.in.dir ("data/oversegmentation_bcn/res.exp1",
                                      ".res$", "\t", comment.char="%")
dat.bcn.exp1.3x <- dat.bcn.exp1.3x[dat.bcn.exp1.3x$Condition=="TEST",]


# These are the python experiments
dat.bcn.exp1.15x <- read.expyriment.data ("data/oversegmentation_bcn/res.exp1.15x", 
                                          "\\.xpd$", remove.space=TRUE, add.subj.col=TRUE)
dat.bcn.exp1.15x <- dat.bcn.exp1.15x[dat.bcn.exp1.15x$TrialType=="test",]

dat.bcn.exp1.30x <- read.expyriment.data ("data/oversegmentation_bcn/res.exp1.30x", 
                                          "\\.xpd$", remove.space=TRUE, add.subj.col=TRUE)
dat.bcn.exp1.30x <- dat.bcn.exp1.30x[dat.bcn.exp1.30x$TrialType=="test",]

dat.bcn.exp1.combined <- bind_rows(
    dat.bcn.exp1.3x %>% 
        select (subj, age, lang, correct) %>% 
        setNames (c("subj", "age", "lang", "cor")) %>% 
        add_column (n.rep.word = 3, .before = 1) %>% 
        add_column(experimentID = "bcn.exp1", .before = 1) %>% 
        mutate (age = as.numeric(as.character(age))),
    
    dat.bcn.exp1.15x %>% 
        select (Tst_Exp_1, subj, Age, Tst_Lang_1, ChoiceMatch)  %>% 
        setNames (c("exp", "subj", "age", "lang", "cor")) %>% 
        add_column (n.rep.word = 15, .after = "exp") %>% 
        rename (experimentID = exp) %>% 
        mutate (experimentID = "bcn.exp1") %>% 
        mutate (age = as.numeric(as.character(age))),
    
    dat.bcn.exp1.30x %>% 
        select (Tst_Exp_1, subj, Age, Tst_Lang_1, ChoiceMatch)  %>% 
        setNames (c("exp", "subj", "age", "lang", "cor")) %>% 
        add_column (n.rep.word = 30, .after = "exp") %>% 
        rename (experimentID = exp) %>% 
        mutate (experimentID = "bcn.exp1") %>% 
        mutate (age = as.numeric(as.character(age)))
)
    
```



```{r bcn-make-averages-stats}
dat.bcn.exp1.3x.m <- with (dat.bcn.exp1.3x,
                           aggregate (correct, 
                                      list (subj, age, lang), mean)) %>% 
    setNames (c("subj", "age", "lang", "cor")) %>% 
    add_column (n.rep.word = 3, .before = 1) %>% 
    add_column(exp = "bcn.exp1", .before = 1) %>% 
    mutate (age = as.numeric(as.character(age)))

dat.bcn.exp1.15x.m <- with (dat.bcn.exp1.15x,
                aggregate (ChoiceMatch, 
                           list (Tst_Exp_1, subj, Age, Tst_Lang_1), mean)) %>% 
    setNames (c("exp", "subj", "age", "lang", "cor")) %>% 
    add_column (n.rep.word = 15, .after = "exp") %>% 
    mutate (exp = "bcn.exp1") %>% 
    mutate (age = as.numeric(as.character(age)))

dat.bcn.exp1.30x.m <- with (dat.bcn.exp1.30x,
                aggregate (ChoiceMatch, 
                           list (Tst_Exp_1, subj, Age, Tst_Lang_1), mean)) %>% 
    setNames (c("exp", "subj", "age", "lang", "cor")) %>% 
            add_column (n.rep.word = 30, .after = "exp") %>% 
        mutate (exp = "bcn.exp1") %>% 
    mutate (age = as.numeric(as.character(age)))

dat.bcn.exp1.combined.m <- dat.bcn.exp1.combined %>% 
    group_by (experimentID, n.rep.word, subj, age, lang) %>% 
    summarize (cor = mean (cor))

```

#### Participants
```{r bcn-demographics}
dat.bcn.exp1.combined.m %>% 
    group_by(n.rep.word) %>% 
    summarize (N = n (),
               Age.m = round (mean (age, na.rm = TRUE), 1),
               Age.range = paste (range(age, na.rm = TRUE), collapse = "-")) %>% 
    knitr::kable(caption = 'Demographics of Pilot Experiment 1.', 
                 col.names = c("# Repetitions/word", "*N*", "Age (*M*)", "Age (Range)"),
                 booktabs = TRUE, escape = TRUE) %>%
    kableExtra::kable_classic()
    # kableExtra::kable_styling(latex_options =
    #                   c("scale_down"))        


```

Demographic information of Pilot Experiment 1 is given in Table \ref{tab:bcn-demographics}. Participants were native speakers of Spanish and Catalan and were recruited from the Universitat Pompeu Fabra community. 

### Stimuli 
Stimuli transcriptions are given in Table \ref{tab:bcn-print-language-structure}. They were synthesized using the *es2* (Spanish male) diphone base of the mbrola [@mbrola] speech synthesized, using a segment duration of 225 ms and an fundamental frequency of 120 Hz.

#### Apparatus
Participants were test individually in a quiet room. Stimuli were presented over headphones. Responses were collected from pre-marked keys on the keyboard. The experiment with 3 repetitions per word (see below) were run using PsyScope X; the other experiments were run using Experyment (https://www.expyriment.org/).

#### Familiarization
The design of Pilot Experiment 1 is shown in Table \ref{tab:bcn-print-language-structure}. The languages comprise trisyllabic items. All foward TPs were 0.5. However, in Language 1 the chunk composed of the first two syllables (e.g., *AB* in *ABC*) were twice as frequent as the chunk composed of the last two syllables (e.g., *BC* in *ABC*); the backward TPs were twice as high as well. Language 2 favored the word-final chunk. 
Participants were informed that they would listen to a sequency of Martian words, and then listened to a sequence of the eight words in \ref{tab:stats-london-print-language-structure} with an ISI of 1000 ms and 3, 15 or 30 repetitions per word. Due to programming error, the familiarization items for 15 and 30 repetitions per word were sampled with replacement. 

```{r bcn-print-language-structure}
data.frame (L1.structure = 
                c("ABC", "DEF", "ABF", "DEC",
                  "AGJ", "AGK", "DHJ", "DHK"

                    
                    ),
            L2.structure = 
                c("ABC", "DEF", "DBC", "AEF",
                  "JBG", "KBG", "JEH", "KEH"),
            L1.items = 
                c("AB", "DE", rep("", 6)),
            L2.items = 
                c("BC", "EF", rep ("", 6)),
            L1.words = c("ka-lu-mo", "ne-fi-To", "ka-lu-To", "ne-fi-mo",
                         "ka-do-ri", "ka-do-tSo", "ne-pu-ri", "ne-pu-tSo"),
            L2.words = c("ka-lu-mo", "ne-fi-To", "ne-lu-mo", "ka-fi-To",
                         "ri-lu-do", "tSo-lu-do", "ri-fi-pu", "tSo-fi-pu")
            ) %>% 
    knitr::kable (caption = "Design of the Pilot Experiment 1. (Left) Language structure. (Middle) Structure of test items. Correct items for Language 1 are foils for Language 2 and vice versa. (Right) Actual items in SAMPA format; dashes indicate syllable boundaries",
                  col.names = paste0 ("Language ", rep(1:2, 3)),
                  booktabs = TRUE, escape = TRUE) %>%
    kableExtra::add_header_above(c("Word structure for" = 2, 
                                   "Test item structure for" = 2, 
                                   "Actual words for" = 2),
                                 line = FALSE) %>%
    #kableExtra::kable_styling() %>%
    kableExtra::kable_classic(full_width = FALSE) 

    
```    


#### Test
Following this familiarization, participants were informed that they would hear new items, and had to decide which of them was in Martian. Following this, they heard pairs of two syllabic items with an ISI of 1000 ms. One was a word-initial chunk and one a word-final chunk.

The test items shown in Table \ref{tab:stats-london-print-language-structure} were combined into four test pairs, which were presented twice with different item orders. A new trial started 100 ms after a participant response. 

### Results

```{r bcn-plot-stats-old-format, eval = FALSE}
make.figure.from.mean (dat.bcn.exp1.3x.m, data.col="cor")
make.figure.from.mean (dat.bcn.exp1.15x.m, data.col="cor")
make.figure.from.mean (dat.bcn.exp1.30x.m, data.col="cor")
```

```{r bcn-plot-stats, fig.cap="Results of Pilot Experiment 1. Each dot represents a participants. The central red dot is the sample mean; error bars represent standard errors from the mean. The results show the percentage of correct choices in the recognition test after familiarization with (left) 3, (middle) 15  or (right) 30 repetitions per word."}

dat.bcn.exp1.combined.m %>% 
    mutate (n.rep.word = factor (n.rep.word)) %>% 
    ggplot (aes (x = n.rep.word,
                 y = 100 * cor)) +
    theme_light(14) +
    #theme (axis.title.x = element_blank()) + 
    xlab ("# Repetitions/word") + 
    ylab ("% Correct") + 
    geom_violin() +
    geom_dotplot(binaxis='y', stackdir='center', dotsize=1) +
    stat_summary(fun.data=mean_se, 
                 geom="pointrange", color="red") +
    geom_hline(yintercept=50, linetype="dashed") #+
    #scale_x_discrete(labels = c("Pre-segmented", "Continuous (1)", "Continuous (2)"))
```


```{r bcn-glmm-calculate}

lmer.bcn.exp1.1 <- glmer (cor ~ lang * n.rep.word + 
                              (1|subj),
                          control=glmerControl(optimizer="bobyqa"),
                          family="binomial",
                          data =  dat.bcn.exp1.combined)
                          

lmer.bcn.exp1.1.results <- 
    extract.results.from.model(lmer.bcn.exp1.1)


```


```{r bcn-glmm-print}

lmer.bcn.exp1.1.results %>%
    process.glmm.table %>% 
    filter (!grepl ("Intercept", Effect)) %>% 
    kable (caption = "Performance in Pilot Experiment 1 for different amounts of exposure. The differences were assessed using a generalized linear model for the trial-by-trial data, using participants as a random factor.",
           booktabs = TRUE, escape = FALSE) %>%
    kableExtra::kable_classic() 
    # kableExtra::kable_styling(latex_options =
    #                               c("scale_down",
    #                               "hold_position"))


```

As shown Table \ref{tab:bcn-glmm-print}, a generalized linear model revealed that performance depended neither on the amount of familiarization nor on the familiarization language. As shown in Figure \ref{fig:bcn-plot-stats}, a Wilcoxon test did not detect any deviation from the chance level of 50%, neither for all amounts of familiarization combined, `r dat.bcn.exp1.combined.m %>% mutate (cor = 100*cor) %>% pull (cor) %>% wilcox.p (50, TRUE)`, nor for the individual familiarization conditions (3 repetitions per word: `r dat.bcn.exp1.combined.m %>% mutate (cor = 100*cor) %>% filter (n.rep.word == 3) %>% pull (cor) %>% wilcox.p (50, TRUE)`; 15 repetitions per word: `r dat.bcn.exp1.combined.m %>% mutate (cor = 100*cor) %>% filter (n.rep.word == 15) %>% pull (cor) %>% wilcox.p (50, TRUE)`; 30 repetitions per word: `r dat.bcn.exp1.combined.m %>% mutate (cor = 100*cor) %>% filter (n.rep.word == 30) %>% pull (cor) %>% wilcox.p (50, TRUE)`). Following @Glover2004, the null hypothesis was `r lik.ratio(dat.bcn.exp1.combined.m$cor, .5, "BIC")` times more likely than the alternative hypothesis after corrections with the Bayesian Information Criterion, and `r lik.ratio(dat.bcn.exp1.combined.m$cor, .5, "AIC")` more likely after correction with the Akaike Information Criterion.



`r clearpage()`


```{r recall-UPTOHERE}
knit_exit()

```    


# Unused stuff
## Statistical experiments London
### Segmented stream, 1 repetition of the stream, en1 diphone base
```{r stats-london-stats.1x.en.segm, fig.cap="Results for a segmented presentation of the stream (540 ms silences) with one repetition of the stream (45 repetitions per word). This limited exposure was not sufficient to trigger any learning."}
tmp <-  dat.stats.london.m %>% 
     global.df.to.plot.df ("stats.1x.en.segm")
    

#current.plot.name <- "stats.1x.en.segm.strip"
#prepare.graphics

strip4c (100*tmp, x=c(1:2),
         ylab="% Correct",
         xlab_big=names (tmp),
         xlab_big_at=c(1:2),
         main="Segmented (1 presentation of stream)\n[e1]")


llr.stats.1x.en.segm <- dat.stats.london.m %>% 
    filter (experimentID == "stats.1x.en.segm") %>% 
    pull (correct) %>% 
    lik.ratio (., .5)

#show.graphics

```
In an initial experiment, we presented a segmented stream (with 540 ms) with only once (45 repetitions per word). As shown in Figure \ref{fig:stats.1x.en.segm}, the likelihood ratio in favor of the null hypothesis was `r llr.stats.1x.en.segm`.

## Recall experiment
```{r recall-averages-across-subjects-calculate, include = FALSE, eval = FALSE}
dat.recall.combined.m2 <- dat.recall.combined.m %>% 
    group_by(data.set, streamType) %>%
    summarize_at (vars(correct_segm:p.correct.initial.or.final.syll),
    mean, na.rm = TRUE)

```

```{r recall-wilcox-across-stream-types-calculate, eval = FALSE}
# Check which outcomes differ across the strema types. Used unpaired
# test by default

dat.recall.combined.m.wilcox.by.streamType <-   dat.recall.combined.m %>% 
    ungroup %>% 
    mutate (streamType = factor (streamType)) %>%
    group_by (data.set) %>%
    summarize (across(correct_segm:p.correct.initial.or.final.syll,
                      ~ wilcox.p.2sample (.x, streamType)))

```

```{r recall-averages-print, eval= FALSE}

dat.recall.combined.m2 %>% 
    data.frame %>%
    rbind (., 
           cbind(streamType = "$p_{Wilcoxon}$", 
                 dat.recall.combined.m.wilcox.by.streamType)) %>% 
    remove_rownames %>% 
    arrange (data.set, desc(streamType)) %>% 
    mutate (rownames = paste0 (data.set, ".", streamType)) %>% 
    column_to_rownames("rownames") %>%
    t %>% 
    knitr::kable (caption = "All averages. The *p* value has been calculated from a paired Wilcoxon test across the familiarization conditions.", booktabs = T) %>%
    kable_styling(latex_options =
                      c("hold_position", 
                        "scale_down"),
                  bootstrap_options = "striped") %>% 
    kableExtra::kable_classic()

```



```{r recall-averages-plot-city-old, fig.cap="\\label{fig:recall_w_vs_pw-old}. Counts of words and part-words produced by the participants. The counts reflect only words and part-words, but not concatenations thereof.", eval = FALSE}

if (ANALYZED.DATA.SETS["CITY"]){
current.plot.name <- "recall_numbers"
prepare.graphics

dat.recall.combined.m %>% 
    ungroup %>%
    data.table::setDT(.) %>% 
    data.table::dcast (subj ~ streamType, 
                       value.var = c("p.words", "p.part.words")) %>%
    data.table::setDF(.) %>%
    dplyr::select (c(p.words_continuous, p.part.words_continuous, p.words_segmented, p.part.words_segmented)) %>%
    strip4c(.,
            main="",
            ylim=c(-0.5,4),  pch=21, mean.pch=17, x=c(1, 2, 4, 5),
            offset = .5,
            ylab="Number of items recalled",
            xlab_big=c("Continuous", "Segmented"), xlab_big_at=c(1.5, 4.5), xlab_big_line=1,
            xlab_exp=rep(c("W", "PWs"), 2), xlab_exp_at=c(1:2, 4:5), xlab_exp_line=-.5,
            margins=c(3.5,6.5,2.5,2.5), write.percent=FALSE, forced.digits=2,
            ref.line = NULL)

show.graphics
}
```

```{r recall-averages-plot-to-be-moved, fig.cap="\\label{fig:recall_w_vs_pw}. Counts of words and part-words (or produced by the participants. The counts both reflect only words and part-words, and concatenations thereof.", eval = FALSE}

dat.recall.combined.m %>% 
    gather (productionType, p, p.words.or.multiple, p.part.words.or.multiple) %>% 
    mutate (productionType = ifelse (productionType == "p.words.or.multiple",
                                     "Words",
                                     "Part-Words")) %>% 
    ggplot (aes (x=productionType, y = 100*p)) + 
    # geom_boxplot (alpha=.5, fill="lightblue", outlier.shape = NA) + 
    geom_dotplot(binaxis = "y", stackdir = "center") +
    geom_violin(alpha = 0, 
                fill = "#5588CC", col="#5588CC") +
    stat_summary(fun.data=mean_sdl, 
                 fun.args = list (mult=1/sqrt(55)), 
                 geom="pointrange", color="#cc556f") + 
    facet_grid(data.set ~ streamType, scales = "free") +
#                labeller = labeller (experimentID = experimentID_facet_labels)) + 
    theme_light (16) +
    theme (axis.title.x = element_blank()) +
    ylab ("% of vocalizations")

    
```

```{r recall-word-vs-pw-analysis-calculate-city, include = FALSE, eval = TRUE}
w.vs.pw <- dat.recall.city %>% 
    group_by(subjNum, subjInitials, streamType, correct_segm) %>%
    summarize_at (vars (is_single_or_multiple_words, is_single_or_multiple_part_words), sum, na.rm = TRUE) %>%
    mutate (p_word_vs_part_word = ifelse ((is_single_or_multiple_words == 0 ) &
                                              (is_single_or_multiple_part_words == 0),
                                          .5,
                                          is_single_or_multiple_words / (is_single_or_multiple_words + is_single_or_multiple_part_words)))

w.vs.pw.wide <- w.vs.pw %>%
    data.table::setDT(.) %>% 
    data.table::dcast (subjNum + subjInitials ~ streamType, 
                       value.var = c("correct_segm",
                                     "is_single_or_multiple_words",
                                     "is_single_or_multiple_part_words",
                                     "p_word_vs_part_word")) %>%
    data.table::setDF(.) %>% 
    mutate (d_segm = correct_segm_segmented - correct_segm_continuous) %>%
    mutate (d_p_word_vs_part_word = p_word_vs_part_word_segmented - p_word_vs_part_word_continuous) %>%
    mutate (d_segm_p_word = d_segm - d_p_word_vs_part_word)

w.vs.pw.long <- w.vs.pw %>% 
    gather (testType, 
            p.cor, 
            c(correct_segm, p_word_vs_part_word),
            factor_key = TRUE) 

# Long version of data frame for differences 
w.vs.pw.d.long <- w.vs.pw.wide %>% 
    gather (testType, 
            d, 
            c(d_segm, d_p_word_vs_part_word),
            factor_key = TRUE) 


```

```{r recall-word-vs-pw-analysis-print-city, eval = FALSE}
w.vs.pw.wide %>%
    dplyr::select(starts_with("p_word_vs_part_word")) %>%
    summarize_all (funs(n(), mean(., na.rm = TRUE))) %>%
    t %>%
    kable
```

```{r recall-word-vs-pw-analysis-plot-city, fig.cap="\\label{fig:recall_w_vs_pw}. Perentage of words among words and part-words. The percentage counted both words and part-words and concatenations thereof. The below-chance performance in the continuous condition is expected if participants start items on a random syllable, because they are twice as likely to produce an item starting with the second or the third syllable of a word than to start with a word-initial syllable.",  eval = FALSE}
current.plot.name <- "recall_w_vs_pw"
prepare.graphics

w.vs.pw.wide %>%
    dplyr::select(c(starts_with("correct_segm"),
                    starts_with("p_word_vs_part_word"))) %>%
    mutate_all (function (X) 100 * X) %>%
    strip4c(.,
            main="Words vs. Part-Words",
            ylim=c(0,100),  pch=21, mean.pch=17, x=c(1, 2, 4, 5),
            offset = .4,
            ylab=TeX("$100 \\times \\frac{Words}{Words + Part-Words}$"),
            xlab_sma=rep(c("Cont.", "Segm."), 2), xlab_sma_at=c(1, 2, 4, 5), xlab_sma_line=.2,
            xlab_big=c("Recognition", "Recall"), xlab_big_at=c(1.5, 4.5), xlab_big_line=2,
            margins=c(4.5,6.5,2.5,2.5), write.percent=TRUE, forced.digits=2,
            ref.line = 50)

show.graphics
```

```{r recall-sw-acc-calculate-city, include = FALSE}
w.vs.pw.sw <- 
    w.vs.pw.long %>% 
    calculate.shapiro.wilk.test.for.cells(.,
                                          c("testType",
                                            "streamType"),
                                          "p.cor",
                                          .return.msg = FALSE)

```


```{r recall-sw-acc-print-city, eval = FALSE}

if (any (w.vs.pw.sw$p.value <= .05)) {
    w.vs.pw.sw %>%
        filter (p.value <= .05) %>%
        setNames(replace_column_labels(names (.))) %>%
        #dplyr::select (-c(locCond)) %>%
        arrange (-row_number()) %>%
        knitr::kable (caption = "\\label{tab:sw_acc}Cells across experiments where a violation of normality was detected by a Shapiro-Wilk test when performance was measured in terms of accuracy.")
}

```


```{r recall-sw-d-calculate-city, include = FALSE, eval = FALSE}
w.vs.pw.d.sw <- 
    w.vs.pw.d.long %>% 
    calculate.shapiro.wilk.test.for.cells(.,
                                          c("testType"),
                                          "d",
                                          .return.msg = FALSE)
```


```{r recall-sw-d-print-city, eval = FALSE}

if (any (w.vs.pw.d.sw$p.value <= .05)) {
    w.vs.pw.d.sw %>%
        filter (p.value <= .05) %>%
        setNames(replace_column_labels(names (.))) %>%
        #dplyr::select (-c(locCond)) %>%
        arrange (-row_number()) %>%
        knitr::kable (caption = "\\label{tab:sw_acc}Cells across experiments where a violation of normality was detected by a Shapiro-Wilk test when performance was measured in terms of accuracy.")
}

```


```{r recall-will-be-anova-city, eval = FALSE}
lapply (grep ("^d_", names (w.vs.pw.wide), value = TRUE),
        function (X){
            cbind (d = X,   
                   P = w.vs.pw.wide %>%
                       pull (X) %>% 
                       wilcox.p(.))
        }) %>%
    do.call (rbind, .) %>%
    kable (caption = "\\label{tab:wilcox_d}Wilcoxon tests for various differences. No of them is normally distributed.")

```

```{r print-time-elapsed}
end.time <- Sys.time()

end.time - start.time
```
